2023-03-22 22:01:54,207:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-22 22:01:54,207:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-22 22:01:54,207:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-22 22:01:54,207:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-22 22:01:55,151:INFO:Soft dependency imported: prophet: 1.1.2
2023-03-22 22:01:58,474:INFO:PyCaret ClassificationExperiment
2023-03-22 22:01:58,474:INFO:Logging name: clf-default-name
2023-03-22 22:01:58,474:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-22 22:01:58,474:INFO:version 3.0.0
2023-03-22 22:01:58,474:INFO:Initializing setup()
2023-03-22 22:01:58,474:INFO:self.USI: 087b
2023-03-22 22:01:58,474:INFO:self._variable_keys: {'_available_plots', 'idx', 'fold_shuffle_param', 'target_param', 'y', 'data', 'is_multiclass', 'gpu_param', 'exp_id', 'X_test', 'html_param', 'fold_generator', 'y_train', 'pipeline', '_ml_usecase', 'gpu_n_jobs_param', 'memory', 'log_plots_param', 'X_train', 'X', 'fix_imbalance', 'logging_param', 'exp_name_log', 'USI', 'n_jobs_param', 'seed', 'fold_groups_param', 'y_test'}
2023-03-22 22:01:58,474:INFO:Checking environment
2023-03-22 22:01:58,474:INFO:python_version: 3.9.12
2023-03-22 22:01:58,474:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-22 22:01:58,474:INFO:machine: AMD64
2023-03-22 22:01:58,474:INFO:platform: Windows-10-10.0.22624-SP0
2023-03-22 22:01:58,474:INFO:Memory: svmem(total=8378363904, available=1293475840, percent=84.6, used=7084888064, free=1293475840)
2023-03-22 22:01:58,474:INFO:Physical Core: 4
2023-03-22 22:01:58,474:INFO:Logical Core: 8
2023-03-22 22:01:58,474:INFO:Checking libraries
2023-03-22 22:01:58,474:INFO:System:
2023-03-22 22:01:58,474:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-22 22:01:58,474:INFO:executable: C:\Users\Mujahid\anaconda3\python.exe
2023-03-22 22:01:58,474:INFO:   machine: Windows-10-10.0.22624-SP0
2023-03-22 22:01:58,474:INFO:PyCaret required dependencies:
2023-03-22 22:01:58,474:INFO:                 pip: 23.0.1
2023-03-22 22:01:58,474:INFO:          setuptools: 61.2.0
2023-03-22 22:01:58,474:INFO:             pycaret: 3.0.0
2023-03-22 22:01:58,474:INFO:             IPython: 8.2.0
2023-03-22 22:01:58,474:INFO:          ipywidgets: 7.6.5
2023-03-22 22:01:58,474:INFO:                tqdm: 4.64.0
2023-03-22 22:01:58,474:INFO:               numpy: 1.21.5
2023-03-22 22:01:58,474:INFO:              pandas: 1.4.2
2023-03-22 22:01:58,474:INFO:              jinja2: 3.1.2
2023-03-22 22:01:58,474:INFO:               scipy: 1.7.3
2023-03-22 22:01:58,474:INFO:              joblib: 1.2.0
2023-03-22 22:01:58,474:INFO:             sklearn: 1.0.2
2023-03-22 22:01:58,474:INFO:                pyod: 1.0.8
2023-03-22 22:01:58,474:INFO:            imblearn: 0.10.1
2023-03-22 22:01:58,474:INFO:   category_encoders: 2.6.0
2023-03-22 22:01:58,474:INFO:            lightgbm: 3.3.5
2023-03-22 22:01:58,474:INFO:               numba: 0.55.1
2023-03-22 22:01:58,474:INFO:            requests: 2.27.1
2023-03-22 22:01:58,474:INFO:          matplotlib: 3.5.1
2023-03-22 22:01:58,474:INFO:          scikitplot: 0.3.7
2023-03-22 22:01:58,474:INFO:         yellowbrick: 1.5
2023-03-22 22:01:58,474:INFO:              plotly: 5.6.0
2023-03-22 22:01:58,474:INFO:             kaleido: 0.2.1
2023-03-22 22:01:58,474:INFO:         statsmodels: 0.13.2
2023-03-22 22:01:58,474:INFO:              sktime: 0.16.1
2023-03-22 22:01:58,474:INFO:               tbats: 1.1.2
2023-03-22 22:01:58,474:INFO:            pmdarima: 2.0.3
2023-03-22 22:01:58,474:INFO:              psutil: 5.9.4
2023-03-22 22:01:58,474:INFO:PyCaret optional dependencies:
2023-03-22 22:01:58,725:INFO:                shap: 0.41.0
2023-03-22 22:01:58,725:INFO:           interpret: Not installed
2023-03-22 22:01:58,725:INFO:                umap: 0.5.3
2023-03-22 22:01:58,725:INFO:    pandas_profiling: 4.0.0
2023-03-22 22:01:58,725:INFO:  explainerdashboard: Not installed
2023-03-22 22:01:58,725:INFO:             autoviz: Not installed
2023-03-22 22:01:58,725:INFO:           fairlearn: Not installed
2023-03-22 22:01:58,725:INFO:             xgboost: 1.7.4
2023-03-22 22:01:58,725:INFO:            catboost: 1.1.1
2023-03-22 22:01:58,725:INFO:              kmodes: 0.12.2
2023-03-22 22:01:58,725:INFO:             mlxtend: 0.21.0
2023-03-22 22:01:58,725:INFO:       statsforecast: Not installed
2023-03-22 22:01:58,725:INFO:        tune_sklearn: Not installed
2023-03-22 22:01:58,725:INFO:                 ray: Not installed
2023-03-22 22:01:58,725:INFO:            hyperopt: Not installed
2023-03-22 22:01:58,725:INFO:              optuna: Not installed
2023-03-22 22:01:58,725:INFO:               skopt: Not installed
2023-03-22 22:01:58,725:INFO:              mlflow: 2.2.2
2023-03-22 22:01:58,725:INFO:              gradio: Not installed
2023-03-22 22:01:58,725:INFO:             fastapi: 0.95.0
2023-03-22 22:01:58,725:INFO:             uvicorn: 0.21.1
2023-03-22 22:01:58,725:INFO:              m2cgen: Not installed
2023-03-22 22:01:58,725:INFO:           evidently: Not installed
2023-03-22 22:01:58,725:INFO:               fugue: Not installed
2023-03-22 22:01:58,725:INFO:           streamlit: Not installed
2023-03-22 22:01:58,725:INFO:             prophet: 1.1.2
2023-03-22 22:01:58,725:INFO:None
2023-03-22 22:01:58,725:INFO:Set up data.
2023-03-22 22:01:58,757:INFO:Set up train/test split.
2023-03-22 22:01:58,835:INFO:Set up index.
2023-03-22 22:01:58,835:INFO:Set up folding strategy.
2023-03-22 22:01:58,835:INFO:Assigning column types.
2023-03-22 22:01:58,866:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-22 22:01:58,903:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 22:01:58,914:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-22 22:01:58,945:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:01:59,039:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:01:59,165:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 22:01:59,165:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-22 22:01:59,196:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:01:59,196:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:01:59,196:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-22 22:01:59,227:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-22 22:01:59,259:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:01:59,259:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:01:59,306:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-22 22:01:59,322:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:01:59,337:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:01:59,337:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-03-22 22:01:59,404:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:01:59,404:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:01:59,463:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:01:59,478:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:01:59,478:INFO:Preparing preprocessing pipeline...
2023-03-22 22:01:59,478:INFO:Set up simple imputation.
2023-03-22 22:01:59,494:INFO:Set up column name cleaning.
2023-03-22 22:01:59,604:INFO:Finished creating preprocessing pipeline.
2023-03-22 22:01:59,608:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Mujahid\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Type', 'Order Region',
                                             'Delivery Status',
                                             'Late_delivery_risk',
                                             'Customer Country', 'Order State',
                                             'Order City', 'Customer Segment',
                                             'Customer State',
                                             'Customer Zipcode',
                                             'Order Country', 'Order Zipcode',
                                             'shipping date (Dat...
                                                              verbose=0))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-03-22 22:01:59,608:INFO:Creating final display dataframe.
2023-03-22 22:01:59,955:INFO:Setup _display_container:                     Description             Value
0                    Session id              2134
1                        Target   SUSPECTED_FRAUD
2                   Target type            Binary
3           Original data shape      (180519, 15)
4        Transformed data shape      (180519, 15)
5   Transformed train set shape      (126363, 15)
6    Transformed test set shape       (54156, 15)
7              Numeric features                14
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              087b
2023-03-22 22:02:00,046:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:02:00,049:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:02:00,111:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:02:00,111:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:02:00,111:INFO:setup() successfully completed in 3.76s...............
2023-03-22 22:02:00,134:INFO:Initializing compare_models()
2023-03-22 22:02:00,134:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-03-22 22:02:00,134:INFO:Checking exceptions
2023-03-22 22:02:00,172:INFO:Preparing display monitor
2023-03-22 22:02:00,205:INFO:Initializing Logistic Regression
2023-03-22 22:02:00,205:INFO:Total runtime is 0.0 minutes
2023-03-22 22:02:00,211:INFO:SubProcess create_model() called ==================================
2023-03-22 22:02:00,211:INFO:Initializing create_model()
2023-03-22 22:02:00,212:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E209E0FE50>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:02:00,212:INFO:Checking exceptions
2023-03-22 22:02:00,212:INFO:Importing libraries
2023-03-22 22:02:00,212:INFO:Copying training dataset
2023-03-22 22:02:00,267:INFO:Defining folds
2023-03-22 22:02:00,267:INFO:Declaring metric variables
2023-03-22 22:02:00,270:INFO:Importing untrained model
2023-03-22 22:02:00,273:INFO:Logistic Regression Imported successfully
2023-03-22 22:02:00,280:INFO:Starting cross validation
2023-03-22 22:02:00,281:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:02:10,896:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:02:27,956:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-22 22:02:28,587:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-22 22:02:28,729:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-22 22:02:30,994:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-22 22:02:34,818:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-22 22:02:34,991:INFO:Calculating mean and std
2023-03-22 22:02:34,991:INFO:Creating metrics dataframe
2023-03-22 22:02:36,465:INFO:Uploading results into container
2023-03-22 22:02:36,465:INFO:Uploading model into container now
2023-03-22 22:02:36,465:INFO:_master_model_container: 1
2023-03-22 22:02:36,465:INFO:_display_container: 2
2023-03-22 22:02:36,465:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=2134, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-03-22 22:02:36,465:INFO:create_model() successfully completed......................................
2023-03-22 22:02:36,604:INFO:SubProcess create_model() end ==================================
2023-03-22 22:02:36,604:INFO:Creating metrics dataframe
2023-03-22 22:02:36,614:INFO:Initializing K Neighbors Classifier
2023-03-22 22:02:36,614:INFO:Total runtime is 0.6068252603212992 minutes
2023-03-22 22:02:36,619:INFO:SubProcess create_model() called ==================================
2023-03-22 22:02:36,619:INFO:Initializing create_model()
2023-03-22 22:02:36,619:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E209E0FE50>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:02:36,619:INFO:Checking exceptions
2023-03-22 22:02:36,619:INFO:Importing libraries
2023-03-22 22:02:36,619:INFO:Copying training dataset
2023-03-22 22:02:36,682:INFO:Defining folds
2023-03-22 22:02:36,682:INFO:Declaring metric variables
2023-03-22 22:02:36,686:INFO:Importing untrained model
2023-03-22 22:02:36,689:INFO:K Neighbors Classifier Imported successfully
2023-03-22 22:02:36,696:INFO:Starting cross validation
2023-03-22 22:02:36,697:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:02:52,248:INFO:Calculating mean and std
2023-03-22 22:02:52,248:INFO:Creating metrics dataframe
2023-03-22 22:02:53,608:INFO:Uploading results into container
2023-03-22 22:02:53,608:INFO:Uploading model into container now
2023-03-22 22:02:53,608:INFO:_master_model_container: 2
2023-03-22 22:02:53,608:INFO:_display_container: 2
2023-03-22 22:02:53,623:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-03-22 22:02:53,623:INFO:create_model() successfully completed......................................
2023-03-22 22:02:53,733:INFO:SubProcess create_model() end ==================================
2023-03-22 22:02:53,733:INFO:Creating metrics dataframe
2023-03-22 22:02:53,748:INFO:Initializing Naive Bayes
2023-03-22 22:02:53,748:INFO:Total runtime is 0.892388908068339 minutes
2023-03-22 22:02:53,748:INFO:SubProcess create_model() called ==================================
2023-03-22 22:02:53,748:INFO:Initializing create_model()
2023-03-22 22:02:53,748:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E209E0FE50>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:02:53,748:INFO:Checking exceptions
2023-03-22 22:02:53,748:INFO:Importing libraries
2023-03-22 22:02:53,748:INFO:Copying training dataset
2023-03-22 22:02:53,817:INFO:Defining folds
2023-03-22 22:02:53,817:INFO:Declaring metric variables
2023-03-22 22:02:53,820:INFO:Importing untrained model
2023-03-22 22:02:53,824:INFO:Naive Bayes Imported successfully
2023-03-22 22:02:53,830:INFO:Starting cross validation
2023-03-22 22:02:53,831:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:02:54,297:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:02:54,357:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:02:54,372:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:02:54,397:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:02:54,404:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:02:54,420:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:02:54,451:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:02:57,063:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:02:57,110:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:03:06,345:INFO:Calculating mean and std
2023-03-22 22:03:06,345:INFO:Creating metrics dataframe
2023-03-22 22:03:07,708:INFO:Uploading results into container
2023-03-22 22:03:07,708:INFO:Uploading model into container now
2023-03-22 22:03:07,708:INFO:_master_model_container: 3
2023-03-22 22:03:07,708:INFO:_display_container: 2
2023-03-22 22:03:07,708:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-03-22 22:03:07,708:INFO:create_model() successfully completed......................................
2023-03-22 22:03:07,825:INFO:SubProcess create_model() end ==================================
2023-03-22 22:03:07,825:INFO:Creating metrics dataframe
2023-03-22 22:03:07,842:INFO:Initializing Decision Tree Classifier
2023-03-22 22:03:07,842:INFO:Total runtime is 1.1272902727127074 minutes
2023-03-22 22:03:07,847:INFO:SubProcess create_model() called ==================================
2023-03-22 22:03:07,847:INFO:Initializing create_model()
2023-03-22 22:03:07,847:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E209E0FE50>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:03:07,847:INFO:Checking exceptions
2023-03-22 22:03:07,847:INFO:Importing libraries
2023-03-22 22:03:07,847:INFO:Copying training dataset
2023-03-22 22:03:07,909:INFO:Defining folds
2023-03-22 22:03:07,909:INFO:Declaring metric variables
2023-03-22 22:03:07,912:INFO:Importing untrained model
2023-03-22 22:03:07,916:INFO:Decision Tree Classifier Imported successfully
2023-03-22 22:03:07,922:INFO:Starting cross validation
2023-03-22 22:03:07,923:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:03:21,312:INFO:Calculating mean and std
2023-03-22 22:03:21,312:INFO:Creating metrics dataframe
2023-03-22 22:03:22,668:INFO:Uploading results into container
2023-03-22 22:03:22,668:INFO:Uploading model into container now
2023-03-22 22:03:22,668:INFO:_master_model_container: 4
2023-03-22 22:03:22,668:INFO:_display_container: 2
2023-03-22 22:03:22,684:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2134, splitter='best')
2023-03-22 22:03:22,684:INFO:create_model() successfully completed......................................
2023-03-22 22:03:22,795:INFO:SubProcess create_model() end ==================================
2023-03-22 22:03:22,795:INFO:Creating metrics dataframe
2023-03-22 22:03:22,795:INFO:Initializing SVM - Linear Kernel
2023-03-22 22:03:22,795:INFO:Total runtime is 1.376502760251363 minutes
2023-03-22 22:03:22,811:INFO:SubProcess create_model() called ==================================
2023-03-22 22:03:22,811:INFO:Initializing create_model()
2023-03-22 22:03:22,811:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E209E0FE50>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:03:22,811:INFO:Checking exceptions
2023-03-22 22:03:22,811:INFO:Importing libraries
2023-03-22 22:03:22,811:INFO:Copying training dataset
2023-03-22 22:03:22,869:INFO:Defining folds
2023-03-22 22:03:22,869:INFO:Declaring metric variables
2023-03-22 22:03:22,872:INFO:Importing untrained model
2023-03-22 22:03:22,876:INFO:SVM - Linear Kernel Imported successfully
2023-03-22 22:03:22,881:INFO:Starting cross validation
2023-03-22 22:03:22,883:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:03:35,125:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-22 22:03:35,173:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-22 22:03:35,173:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:03:35,502:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-22 22:03:36,239:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-22 22:03:36,286:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-22 22:03:36,301:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:03:36,631:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-22 22:03:38,745:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-22 22:03:38,855:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-22 22:03:46,387:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-22 22:03:47,503:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-22 22:03:51,128:INFO:Calculating mean and std
2023-03-22 22:03:51,128:INFO:Creating metrics dataframe
2023-03-22 22:03:52,544:INFO:Uploading results into container
2023-03-22 22:03:52,544:INFO:Uploading model into container now
2023-03-22 22:03:52,544:INFO:_master_model_container: 5
2023-03-22 22:03:52,544:INFO:_display_container: 2
2023-03-22 22:03:52,544:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=2134, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-03-22 22:03:52,544:INFO:create_model() successfully completed......................................
2023-03-22 22:03:52,654:INFO:SubProcess create_model() end ==================================
2023-03-22 22:03:52,654:INFO:Creating metrics dataframe
2023-03-22 22:03:52,674:INFO:Initializing Ridge Classifier
2023-03-22 22:03:52,674:INFO:Total runtime is 1.8744861761728921 minutes
2023-03-22 22:03:52,679:INFO:SubProcess create_model() called ==================================
2023-03-22 22:03:52,679:INFO:Initializing create_model()
2023-03-22 22:03:52,679:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E209E0FE50>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:03:52,679:INFO:Checking exceptions
2023-03-22 22:03:52,679:INFO:Importing libraries
2023-03-22 22:03:52,679:INFO:Copying training dataset
2023-03-22 22:03:52,739:INFO:Defining folds
2023-03-22 22:03:52,739:INFO:Declaring metric variables
2023-03-22 22:03:52,742:INFO:Importing untrained model
2023-03-22 22:03:52,745:INFO:Ridge Classifier Imported successfully
2023-03-22 22:03:52,752:INFO:Starting cross validation
2023-03-22 22:03:52,753:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:03:53,236:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-22 22:03:53,252:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:03:53,283:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-22 22:03:53,283:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:03:53,283:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-22 22:03:53,315:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:03:53,315:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-22 22:03:53,330:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:03:53,346:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-22 22:03:53,362:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-22 22:03:53,362:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:03:53,362:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:03:53,399:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-22 22:03:53,399:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-22 22:03:53,409:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:03:53,409:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:03:57,387:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-22 22:03:57,403:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:03:57,434:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-22 22:03:57,434:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:04:07,415:INFO:Calculating mean and std
2023-03-22 22:04:07,415:INFO:Creating metrics dataframe
2023-03-22 22:04:08,775:INFO:Uploading results into container
2023-03-22 22:04:08,791:INFO:Uploading model into container now
2023-03-22 22:04:08,791:INFO:_master_model_container: 6
2023-03-22 22:04:08,791:INFO:_display_container: 2
2023-03-22 22:04:08,791:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=2134, solver='auto', tol=0.001)
2023-03-22 22:04:08,791:INFO:create_model() successfully completed......................................
2023-03-22 22:04:08,901:INFO:SubProcess create_model() end ==================================
2023-03-22 22:04:08,901:INFO:Creating metrics dataframe
2023-03-22 22:04:08,918:INFO:Initializing Random Forest Classifier
2023-03-22 22:04:08,918:INFO:Total runtime is 2.145225489139557 minutes
2023-03-22 22:04:08,922:INFO:SubProcess create_model() called ==================================
2023-03-22 22:04:08,922:INFO:Initializing create_model()
2023-03-22 22:04:08,922:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E209E0FE50>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:04:08,922:INFO:Checking exceptions
2023-03-22 22:04:08,922:INFO:Importing libraries
2023-03-22 22:04:08,922:INFO:Copying training dataset
2023-03-22 22:04:08,979:INFO:Defining folds
2023-03-22 22:04:08,979:INFO:Declaring metric variables
2023-03-22 22:04:08,982:INFO:Importing untrained model
2023-03-22 22:04:08,986:INFO:Random Forest Classifier Imported successfully
2023-03-22 22:04:08,992:INFO:Starting cross validation
2023-03-22 22:04:08,992:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:04:18,641:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 2.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:04:19,023:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:04:37,391:INFO:Calculating mean and std
2023-03-22 22:04:37,391:INFO:Creating metrics dataframe
2023-03-22 22:04:38,773:INFO:Uploading results into container
2023-03-22 22:04:38,773:INFO:Uploading model into container now
2023-03-22 22:04:38,773:INFO:_master_model_container: 7
2023-03-22 22:04:38,773:INFO:_display_container: 2
2023-03-22 22:04:38,773:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=2134, verbose=0, warm_start=False)
2023-03-22 22:04:38,773:INFO:create_model() successfully completed......................................
2023-03-22 22:04:38,887:INFO:SubProcess create_model() end ==================================
2023-03-22 22:04:38,887:INFO:Creating metrics dataframe
2023-03-22 22:04:38,903:INFO:Initializing Quadratic Discriminant Analysis
2023-03-22 22:04:38,903:INFO:Total runtime is 2.644959823290507 minutes
2023-03-22 22:04:38,903:INFO:SubProcess create_model() called ==================================
2023-03-22 22:04:38,903:INFO:Initializing create_model()
2023-03-22 22:04:38,903:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E209E0FE50>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:04:38,903:INFO:Checking exceptions
2023-03-22 22:04:38,903:INFO:Importing libraries
2023-03-22 22:04:38,903:INFO:Copying training dataset
2023-03-22 22:04:38,973:INFO:Defining folds
2023-03-22 22:04:38,973:INFO:Declaring metric variables
2023-03-22 22:04:38,977:INFO:Importing untrained model
2023-03-22 22:04:38,980:INFO:Quadratic Discriminant Analysis Imported successfully
2023-03-22 22:04:38,987:INFO:Starting cross validation
2023-03-22 22:04:38,988:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:04:39,731:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-22 22:04:39,731:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-22 22:04:39,731:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-22 22:04:39,731:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-22 22:04:39,746:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-22 22:04:39,746:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-22 22:04:39,778:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-22 22:04:39,840:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-22 22:04:39,856:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,856:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,856:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:39,875:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,875:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,875:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,875:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,875:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:39,875:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:39,888:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,888:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,888:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:39,903:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,903:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,903:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:39,903:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,903:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,903:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:39,919:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 309, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_ranking.py", line 546, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 800, in check_array
    _assert_all_finite(array, allow_nan=force_all_finite == "allow-nan")
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 114, in _assert_all_finite
    raise ValueError(
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').

  warnings.warn(

2023-03-22 22:04:39,935:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,935:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,935:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:39,935:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,935:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,935:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 309, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_ranking.py", line 546, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 800, in check_array
    _assert_all_finite(array, allow_nan=force_all_finite == "allow-nan")
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 114, in _assert_all_finite
    raise ValueError(
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').

  warnings.warn(

2023-03-22 22:04:39,935:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:39,935:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,935:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,935:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:39,935:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 309, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_ranking.py", line 546, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 800, in check_array
    _assert_all_finite(array, allow_nan=force_all_finite == "allow-nan")
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 114, in _assert_all_finite
    raise ValueError(
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').

  warnings.warn(

2023-03-22 22:04:39,950:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,950:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,950:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 309, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_ranking.py", line 546, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 800, in check_array
    _assert_all_finite(array, allow_nan=force_all_finite == "allow-nan")
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 114, in _assert_all_finite
    raise ValueError(
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').

  warnings.warn(

2023-03-22 22:04:39,950:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:39,950:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,950:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,950:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 309, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_ranking.py", line 546, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 800, in check_array
    _assert_all_finite(array, allow_nan=force_all_finite == "allow-nan")
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 114, in _assert_all_finite
    raise ValueError(
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').

  warnings.warn(

2023-03-22 22:04:39,950:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:39,982:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,982:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:39,997:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:40,013:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:40,013:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:40,013:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:40,029:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:40,029:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:40,029:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:40,029:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 309, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_ranking.py", line 546, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 800, in check_array
    _assert_all_finite(array, allow_nan=force_all_finite == "allow-nan")
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 114, in _assert_all_finite
    raise ValueError(
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').

  warnings.warn(

2023-03-22 22:04:40,044:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:40,044:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:40,044:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:40,044:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 309, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_ranking.py", line 546, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 800, in check_array
    _assert_all_finite(array, allow_nan=force_all_finite == "allow-nan")
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 114, in _assert_all_finite
    raise ValueError(
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').

  warnings.warn(

2023-03-22 22:04:40,075:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:40,075:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:40,075:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:40,075:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 309, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_ranking.py", line 546, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 800, in check_array
    _assert_all_finite(array, allow_nan=force_all_finite == "allow-nan")
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 114, in _assert_all_finite
    raise ValueError(
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').

  warnings.warn(

2023-03-22 22:04:44,049:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-22 22:04:44,096:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-22 22:04:44,127:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:44,127:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:44,127:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:44,159:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:44,159:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:44,159:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:44,159:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:44,159:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:44,175:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:44,175:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 309, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_ranking.py", line 546, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 800, in check_array
    _assert_all_finite(array, allow_nan=force_all_finite == "allow-nan")
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 114, in _assert_all_finite
    raise ValueError(
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').

  warnings.warn(

2023-03-22 22:04:44,191:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:44,191:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-03-22 22:04:44,191:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-03-22 22:04:44,191:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 309, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_ranking.py", line 546, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 800, in check_array
    _assert_all_finite(array, allow_nan=force_all_finite == "allow-nan")
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 114, in _assert_all_finite
    raise ValueError(
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').

  warnings.warn(

2023-03-22 22:04:53,829:INFO:Calculating mean and std
2023-03-22 22:04:53,829:INFO:Creating metrics dataframe
2023-03-22 22:04:55,222:INFO:Uploading results into container
2023-03-22 22:04:55,222:INFO:Uploading model into container now
2023-03-22 22:04:55,222:INFO:_master_model_container: 8
2023-03-22 22:04:55,222:INFO:_display_container: 2
2023-03-22 22:04:55,222:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-03-22 22:04:55,222:INFO:create_model() successfully completed......................................
2023-03-22 22:04:55,333:INFO:SubProcess create_model() end ==================================
2023-03-22 22:04:55,333:INFO:Creating metrics dataframe
2023-03-22 22:04:55,345:INFO:Initializing Ada Boost Classifier
2023-03-22 22:04:55,345:INFO:Total runtime is 2.919001889228821 minutes
2023-03-22 22:04:55,345:INFO:SubProcess create_model() called ==================================
2023-03-22 22:04:55,345:INFO:Initializing create_model()
2023-03-22 22:04:55,345:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E209E0FE50>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:04:55,345:INFO:Checking exceptions
2023-03-22 22:04:55,345:INFO:Importing libraries
2023-03-22 22:04:55,345:INFO:Copying training dataset
2023-03-22 22:04:55,409:INFO:Defining folds
2023-03-22 22:04:55,409:INFO:Declaring metric variables
2023-03-22 22:04:55,412:INFO:Importing untrained model
2023-03-22 22:04:55,416:INFO:Ada Boost Classifier Imported successfully
2023-03-22 22:04:55,423:INFO:Starting cross validation
2023-03-22 22:04:55,424:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:05:21,938:INFO:Calculating mean and std
2023-03-22 22:05:21,938:INFO:Creating metrics dataframe
2023-03-22 22:05:23,352:INFO:Uploading results into container
2023-03-22 22:05:23,353:INFO:Uploading model into container now
2023-03-22 22:05:23,354:INFO:_master_model_container: 9
2023-03-22 22:05:23,354:INFO:_display_container: 2
2023-03-22 22:05:23,355:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=2134)
2023-03-22 22:05:23,355:INFO:create_model() successfully completed......................................
2023-03-22 22:05:23,463:INFO:SubProcess create_model() end ==================================
2023-03-22 22:05:23,463:INFO:Creating metrics dataframe
2023-03-22 22:05:23,479:INFO:Initializing Gradient Boosting Classifier
2023-03-22 22:05:23,479:INFO:Total runtime is 3.3878931959470115 minutes
2023-03-22 22:05:23,479:INFO:SubProcess create_model() called ==================================
2023-03-22 22:05:23,479:INFO:Initializing create_model()
2023-03-22 22:05:23,479:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E209E0FE50>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:05:23,479:INFO:Checking exceptions
2023-03-22 22:05:23,479:INFO:Importing libraries
2023-03-22 22:05:23,479:INFO:Copying training dataset
2023-03-22 22:05:23,543:INFO:Defining folds
2023-03-22 22:05:23,543:INFO:Declaring metric variables
2023-03-22 22:05:23,546:INFO:Importing untrained model
2023-03-22 22:05:23,550:INFO:Gradient Boosting Classifier Imported successfully
2023-03-22 22:05:23,557:INFO:Starting cross validation
2023-03-22 22:05:23,558:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:06:23,772:INFO:Calculating mean and std
2023-03-22 22:06:23,772:INFO:Creating metrics dataframe
2023-03-22 22:06:25,429:INFO:Uploading results into container
2023-03-22 22:06:25,429:INFO:Uploading model into container now
2023-03-22 22:06:25,429:INFO:_master_model_container: 10
2023-03-22 22:06:25,429:INFO:_display_container: 2
2023-03-22 22:06:25,429:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='deviance', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2134, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-03-22 22:06:25,429:INFO:create_model() successfully completed......................................
2023-03-22 22:06:25,558:INFO:SubProcess create_model() end ==================================
2023-03-22 22:06:25,558:INFO:Creating metrics dataframe
2023-03-22 22:06:25,579:INFO:Initializing Linear Discriminant Analysis
2023-03-22 22:06:25,580:INFO:Total runtime is 4.4229228615760805 minutes
2023-03-22 22:06:25,582:INFO:SubProcess create_model() called ==================================
2023-03-22 22:06:25,582:INFO:Initializing create_model()
2023-03-22 22:06:25,582:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E209E0FE50>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:06:25,582:INFO:Checking exceptions
2023-03-22 22:06:25,582:INFO:Importing libraries
2023-03-22 22:06:25,582:INFO:Copying training dataset
2023-03-22 22:06:25,642:INFO:Defining folds
2023-03-22 22:06:25,643:INFO:Declaring metric variables
2023-03-22 22:06:25,647:INFO:Importing untrained model
2023-03-22 22:06:25,650:INFO:Linear Discriminant Analysis Imported successfully
2023-03-22 22:06:25,657:INFO:Starting cross validation
2023-03-22 22:06:25,658:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:06:26,761:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:06:26,808:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:06:26,839:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:06:26,902:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:06:26,917:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:06:26,949:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:06:26,959:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:06:26,996:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:06:31,331:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:06:31,362:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:06:41,002:INFO:Calculating mean and std
2023-03-22 22:06:41,002:INFO:Creating metrics dataframe
2023-03-22 22:06:42,407:INFO:Uploading results into container
2023-03-22 22:06:42,407:INFO:Uploading model into container now
2023-03-22 22:06:42,407:INFO:_master_model_container: 11
2023-03-22 22:06:42,407:INFO:_display_container: 2
2023-03-22 22:06:42,407:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-03-22 22:06:42,407:INFO:create_model() successfully completed......................................
2023-03-22 22:06:42,520:INFO:SubProcess create_model() end ==================================
2023-03-22 22:06:42,520:INFO:Creating metrics dataframe
2023-03-22 22:06:42,536:INFO:Initializing Extra Trees Classifier
2023-03-22 22:06:42,536:INFO:Total runtime is 4.705510719617208 minutes
2023-03-22 22:06:42,542:INFO:SubProcess create_model() called ==================================
2023-03-22 22:06:42,542:INFO:Initializing create_model()
2023-03-22 22:06:42,542:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E209E0FE50>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:06:42,542:INFO:Checking exceptions
2023-03-22 22:06:42,542:INFO:Importing libraries
2023-03-22 22:06:42,542:INFO:Copying training dataset
2023-03-22 22:06:42,600:INFO:Defining folds
2023-03-22 22:06:42,600:INFO:Declaring metric variables
2023-03-22 22:06:42,605:INFO:Importing untrained model
2023-03-22 22:06:42,609:INFO:Extra Trees Classifier Imported successfully
2023-03-22 22:06:42,615:INFO:Starting cross validation
2023-03-22 22:06:42,616:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:06:50,033:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:06:50,629:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:06:50,644:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:07:07,192:INFO:Calculating mean and std
2023-03-22 22:07:07,192:INFO:Creating metrics dataframe
2023-03-22 22:07:08,587:INFO:Uploading results into container
2023-03-22 22:07:08,587:INFO:Uploading model into container now
2023-03-22 22:07:08,587:INFO:_master_model_container: 12
2023-03-22 22:07:08,587:INFO:_display_container: 2
2023-03-22 22:07:08,587:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=2134, verbose=0, warm_start=False)
2023-03-22 22:07:08,587:INFO:create_model() successfully completed......................................
2023-03-22 22:07:08,697:INFO:SubProcess create_model() end ==================================
2023-03-22 22:07:08,697:INFO:Creating metrics dataframe
2023-03-22 22:07:08,718:INFO:Initializing Extreme Gradient Boosting
2023-03-22 22:07:08,718:INFO:Total runtime is 5.141888097922007 minutes
2023-03-22 22:07:08,721:INFO:SubProcess create_model() called ==================================
2023-03-22 22:07:08,721:INFO:Initializing create_model()
2023-03-22 22:07:08,721:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E209E0FE50>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:07:08,721:INFO:Checking exceptions
2023-03-22 22:07:08,721:INFO:Importing libraries
2023-03-22 22:07:08,721:INFO:Copying training dataset
2023-03-22 22:07:08,780:INFO:Defining folds
2023-03-22 22:07:08,780:INFO:Declaring metric variables
2023-03-22 22:07:08,784:INFO:Importing untrained model
2023-03-22 22:07:08,788:INFO:Extreme Gradient Boosting Imported successfully
2023-03-22 22:07:08,794:INFO:Starting cross validation
2023-03-22 22:07:08,795:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:07:54,281:INFO:Calculating mean and std
2023-03-22 22:07:54,281:INFO:Creating metrics dataframe
2023-03-22 22:07:55,898:INFO:Uploading results into container
2023-03-22 22:07:55,898:INFO:Uploading model into container now
2023-03-22 22:07:55,902:INFO:_master_model_container: 13
2023-03-22 22:07:55,902:INFO:_display_container: 2
2023-03-22 22:07:55,902:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-03-22 22:07:55,902:INFO:create_model() successfully completed......................................
2023-03-22 22:07:56,012:INFO:SubProcess create_model() end ==================================
2023-03-22 22:07:56,012:INFO:Creating metrics dataframe
2023-03-22 22:07:56,023:INFO:Initializing Light Gradient Boosting Machine
2023-03-22 22:07:56,023:INFO:Total runtime is 5.930307312806447 minutes
2023-03-22 22:07:56,023:INFO:SubProcess create_model() called ==================================
2023-03-22 22:07:56,031:INFO:Initializing create_model()
2023-03-22 22:07:56,031:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E209E0FE50>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:07:56,031:INFO:Checking exceptions
2023-03-22 22:07:56,031:INFO:Importing libraries
2023-03-22 22:07:56,031:INFO:Copying training dataset
2023-03-22 22:07:56,089:INFO:Defining folds
2023-03-22 22:07:56,089:INFO:Declaring metric variables
2023-03-22 22:07:56,092:INFO:Importing untrained model
2023-03-22 22:07:56,096:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-22 22:07:56,103:INFO:Starting cross validation
2023-03-22 22:07:56,105:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:08:15,265:INFO:Calculating mean and std
2023-03-22 22:08:15,265:INFO:Creating metrics dataframe
2023-03-22 22:08:16,665:INFO:Uploading results into container
2023-03-22 22:08:16,665:INFO:Uploading model into container now
2023-03-22 22:08:16,665:INFO:_master_model_container: 14
2023-03-22 22:08:16,665:INFO:_display_container: 2
2023-03-22 22:08:16,665:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2134, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-03-22 22:08:16,665:INFO:create_model() successfully completed......................................
2023-03-22 22:08:16,775:INFO:SubProcess create_model() end ==================================
2023-03-22 22:08:16,775:INFO:Creating metrics dataframe
2023-03-22 22:08:16,791:INFO:Initializing CatBoost Classifier
2023-03-22 22:08:16,791:INFO:Total runtime is 6.276431822776795 minutes
2023-03-22 22:08:16,791:INFO:SubProcess create_model() called ==================================
2023-03-22 22:08:16,791:INFO:Initializing create_model()
2023-03-22 22:08:16,791:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E209E0FE50>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:08:16,791:INFO:Checking exceptions
2023-03-22 22:08:16,791:INFO:Importing libraries
2023-03-22 22:08:16,791:INFO:Copying training dataset
2023-03-22 22:08:16,859:INFO:Defining folds
2023-03-22 22:08:16,859:INFO:Declaring metric variables
2023-03-22 22:08:16,862:INFO:Importing untrained model
2023-03-22 22:08:16,866:INFO:CatBoost Classifier Imported successfully
2023-03-22 22:08:16,873:INFO:Starting cross validation
2023-03-22 22:08:16,874:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:10:43,812:INFO:Calculating mean and std
2023-03-22 22:10:43,814:INFO:Creating metrics dataframe
2023-03-22 22:10:45,480:INFO:Uploading results into container
2023-03-22 22:10:45,496:INFO:Uploading model into container now
2023-03-22 22:10:45,496:INFO:_master_model_container: 15
2023-03-22 22:10:45,496:INFO:_display_container: 2
2023-03-22 22:10:45,496:INFO:<catboost.core.CatBoostClassifier object at 0x000001E21DCFF790>
2023-03-22 22:10:45,496:INFO:create_model() successfully completed......................................
2023-03-22 22:10:45,605:INFO:SubProcess create_model() end ==================================
2023-03-22 22:10:45,616:INFO:Creating metrics dataframe
2023-03-22 22:10:45,630:INFO:Initializing Dummy Classifier
2023-03-22 22:10:45,631:INFO:Total runtime is 8.757108183701833 minutes
2023-03-22 22:10:45,632:INFO:SubProcess create_model() called ==================================
2023-03-22 22:10:45,632:INFO:Initializing create_model()
2023-03-22 22:10:45,632:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E209E0FE50>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:10:45,632:INFO:Checking exceptions
2023-03-22 22:10:45,632:INFO:Importing libraries
2023-03-22 22:10:45,632:INFO:Copying training dataset
2023-03-22 22:10:45,689:INFO:Defining folds
2023-03-22 22:10:45,689:INFO:Declaring metric variables
2023-03-22 22:10:45,692:INFO:Importing untrained model
2023-03-22 22:10:45,696:INFO:Dummy Classifier Imported successfully
2023-03-22 22:10:45,702:INFO:Starting cross validation
2023-03-22 22:10:45,703:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:10:46,089:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:10:46,121:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:10:46,152:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:10:46,168:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:10:46,183:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:10:46,183:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:10:46,217:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:10:46,246:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:10:50,237:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:10:50,253:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-22 22:11:00,127:INFO:Calculating mean and std
2023-03-22 22:11:00,127:INFO:Creating metrics dataframe
2023-03-22 22:11:01,541:INFO:Uploading results into container
2023-03-22 22:11:01,541:INFO:Uploading model into container now
2023-03-22 22:11:01,541:INFO:_master_model_container: 16
2023-03-22 22:11:01,541:INFO:_display_container: 2
2023-03-22 22:11:01,541:INFO:DummyClassifier(constant=None, random_state=2134, strategy='prior')
2023-03-22 22:11:01,541:INFO:create_model() successfully completed......................................
2023-03-22 22:11:01,651:INFO:SubProcess create_model() end ==================================
2023-03-22 22:11:01,651:INFO:Creating metrics dataframe
2023-03-22 22:11:01,683:INFO:Initializing create_model()
2023-03-22 22:11:01,683:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2134, splitter='best'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:11:01,683:INFO:Checking exceptions
2023-03-22 22:11:01,683:INFO:Importing libraries
2023-03-22 22:11:01,683:INFO:Copying training dataset
2023-03-22 22:11:01,758:INFO:Defining folds
2023-03-22 22:11:01,758:INFO:Declaring metric variables
2023-03-22 22:11:01,758:INFO:Importing untrained model
2023-03-22 22:11:01,758:INFO:Declaring custom model
2023-03-22 22:11:01,758:INFO:Decision Tree Classifier Imported successfully
2023-03-22 22:11:01,758:INFO:Cross validation set to False
2023-03-22 22:11:01,758:INFO:Fitting Model
2023-03-22 22:11:03,123:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2134, splitter='best')
2023-03-22 22:11:03,123:INFO:create_model() successfully completed......................................
2023-03-22 22:11:03,272:INFO:_master_model_container: 16
2023-03-22 22:11:03,272:INFO:_display_container: 2
2023-03-22 22:11:03,273:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2134, splitter='best')
2023-03-22 22:11:03,273:INFO:compare_models() successfully completed......................................
2023-03-22 22:11:03,279:INFO:Initializing tune_model()
2023-03-22 22:11:03,279:INFO:tune_model(estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2134, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>)
2023-03-22 22:11:03,279:INFO:Checking exceptions
2023-03-22 22:11:03,332:INFO:Copying training dataset
2023-03-22 22:11:03,377:INFO:Checking base model
2023-03-22 22:11:03,377:INFO:Base model : Decision Tree Classifier
2023-03-22 22:11:03,380:INFO:Declaring metric variables
2023-03-22 22:11:03,382:INFO:Defining Hyperparameters
2023-03-22 22:11:03,488:INFO:Tuning with n_jobs=-1
2023-03-22 22:11:03,488:INFO:Initializing RandomizedSearchCV
2023-03-22 22:13:34,587:INFO:best_params: {'actual_estimator__min_samples_split': 7, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 16, 'actual_estimator__criterion': 'gini'}
2023-03-22 22:13:34,587:INFO:Hyperparameter search completed
2023-03-22 22:13:34,587:INFO:SubProcess create_model() called ==================================
2023-03-22 22:13:34,587:INFO:Initializing create_model()
2023-03-22 22:13:34,587:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2134, splitter='best'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E2039D4E20>, model_only=True, return_train_score=False, kwargs={'min_samples_split': 7, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 1.0, 'max_depth': 16, 'criterion': 'gini'})
2023-03-22 22:13:34,587:INFO:Checking exceptions
2023-03-22 22:13:34,587:INFO:Importing libraries
2023-03-22 22:13:34,587:INFO:Copying training dataset
2023-03-22 22:13:34,648:INFO:Defining folds
2023-03-22 22:13:34,648:INFO:Declaring metric variables
2023-03-22 22:13:34,651:INFO:Importing untrained model
2023-03-22 22:13:34,651:INFO:Declaring custom model
2023-03-22 22:13:34,654:INFO:Decision Tree Classifier Imported successfully
2023-03-22 22:13:34,661:INFO:Starting cross validation
2023-03-22 22:13:34,662:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:13:48,596:INFO:Calculating mean and std
2023-03-22 22:13:48,596:INFO:Creating metrics dataframe
2023-03-22 22:13:48,596:INFO:Finalizing model
2023-03-22 22:13:50,164:INFO:Uploading results into container
2023-03-22 22:13:50,179:INFO:Uploading model into container now
2023-03-22 22:13:50,179:INFO:_master_model_container: 17
2023-03-22 22:13:50,179:INFO:_display_container: 3
2023-03-22 22:13:50,180:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=16, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=7, min_weight_fraction_leaf=0.0,
                       random_state=2134, splitter='best')
2023-03-22 22:13:50,180:INFO:create_model() successfully completed......................................
2023-03-22 22:13:50,284:INFO:SubProcess create_model() end ==================================
2023-03-22 22:13:50,284:INFO:choose_better activated
2023-03-22 22:13:50,300:INFO:SubProcess create_model() called ==================================
2023-03-22 22:13:50,300:INFO:Initializing create_model()
2023-03-22 22:13:50,300:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2134, splitter='best'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:13:50,300:INFO:Checking exceptions
2023-03-22 22:13:50,300:INFO:Importing libraries
2023-03-22 22:13:50,300:INFO:Copying training dataset
2023-03-22 22:13:50,359:INFO:Defining folds
2023-03-22 22:13:50,359:INFO:Declaring metric variables
2023-03-22 22:13:50,359:INFO:Importing untrained model
2023-03-22 22:13:50,359:INFO:Declaring custom model
2023-03-22 22:13:50,360:INFO:Decision Tree Classifier Imported successfully
2023-03-22 22:13:50,360:INFO:Starting cross validation
2023-03-22 22:13:50,361:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:14:04,505:INFO:Calculating mean and std
2023-03-22 22:14:04,505:INFO:Creating metrics dataframe
2023-03-22 22:14:04,505:INFO:Finalizing model
2023-03-22 22:14:06,086:INFO:Uploading results into container
2023-03-22 22:14:06,086:INFO:Uploading model into container now
2023-03-22 22:14:06,086:INFO:_master_model_container: 18
2023-03-22 22:14:06,086:INFO:_display_container: 4
2023-03-22 22:14:06,086:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2134, splitter='best')
2023-03-22 22:14:06,086:INFO:create_model() successfully completed......................................
2023-03-22 22:14:06,196:INFO:SubProcess create_model() end ==================================
2023-03-22 22:14:06,196:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2134, splitter='best') result for Accuracy is 0.9955
2023-03-22 22:14:06,196:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=16, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=7, min_weight_fraction_leaf=0.0,
                       random_state=2134, splitter='best') result for Accuracy is 0.9881
2023-03-22 22:14:06,196:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2134, splitter='best') is best model
2023-03-22 22:14:06,196:INFO:choose_better completed
2023-03-22 22:14:06,196:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-03-22 22:14:06,216:INFO:_master_model_container: 18
2023-03-22 22:14:06,216:INFO:_display_container: 3
2023-03-22 22:14:06,216:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2134, splitter='best')
2023-03-22 22:14:06,217:INFO:tune_model() successfully completed......................................
2023-03-22 22:14:07,546:INFO:Initializing plot_model()
2023-03-22 22:14:07,547:INFO:plot_model(plot=confusion_matrix, fold=None, use_train_data=False, verbose=True, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2134, splitter='best'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E21CCDD580>, system=True)
2023-03-22 22:14:07,547:INFO:Checking exceptions
2023-03-22 22:14:07,571:INFO:Preloading libraries
2023-03-22 22:14:07,572:INFO:Copying training dataset
2023-03-22 22:14:07,572:INFO:Plot type: confusion_matrix
2023-03-22 22:14:07,864:INFO:Fitting Model
2023-03-22 22:14:07,864:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\base.py:450: UserWarning:

X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names


2023-03-22 22:14:07,864:INFO:Scoring test/hold-out set
2023-03-22 22:14:07,997:INFO:Visual Rendered Successfully
2023-03-22 22:14:08,114:INFO:plot_model() successfully completed......................................
2023-03-22 22:14:09,453:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\feature_selection\_univariate_selection.py:289: RuntimeWarning:

invalid value encountered in true_divide


2023-03-22 22:14:13,142:INFO:PyCaret RegressionExperiment
2023-03-22 22:14:13,142:INFO:Logging name: reg-default-name
2023-03-22 22:14:13,142:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-22 22:14:13,142:INFO:version 3.0.0
2023-03-22 22:14:13,142:INFO:Initializing setup()
2023-03-22 22:14:13,142:INFO:self.USI: 29af
2023-03-22 22:14:13,158:INFO:self._variable_keys: {'_available_plots', 'idx', 'fold_shuffle_param', 'target_param', 'y', 'data', 'transform_target_param', 'gpu_param', 'exp_id', 'X_test', 'html_param', 'fold_generator', 'y_train', 'pipeline', '_ml_usecase', 'gpu_n_jobs_param', 'memory', 'log_plots_param', 'X_train', 'X', 'logging_param', 'exp_name_log', 'USI', 'n_jobs_param', 'seed', 'fold_groups_param', 'y_test'}
2023-03-22 22:14:13,158:INFO:Checking environment
2023-03-22 22:14:13,158:INFO:python_version: 3.9.12
2023-03-22 22:14:13,158:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-22 22:14:13,158:INFO:machine: AMD64
2023-03-22 22:14:13,158:INFO:platform: Windows-10-10.0.22624-SP0
2023-03-22 22:14:13,158:INFO:Memory: svmem(total=8378363904, available=1594617856, percent=81.0, used=6783746048, free=1594617856)
2023-03-22 22:14:13,158:INFO:Physical Core: 4
2023-03-22 22:14:13,158:INFO:Logical Core: 8
2023-03-22 22:14:13,158:INFO:Checking libraries
2023-03-22 22:14:13,158:INFO:System:
2023-03-22 22:14:13,158:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-22 22:14:13,158:INFO:executable: C:\Users\Mujahid\anaconda3\python.exe
2023-03-22 22:14:13,158:INFO:   machine: Windows-10-10.0.22624-SP0
2023-03-22 22:14:13,158:INFO:PyCaret required dependencies:
2023-03-22 22:14:13,158:INFO:                 pip: 23.0.1
2023-03-22 22:14:13,158:INFO:          setuptools: 61.2.0
2023-03-22 22:14:13,158:INFO:             pycaret: 3.0.0
2023-03-22 22:14:13,158:INFO:             IPython: 8.2.0
2023-03-22 22:14:13,158:INFO:          ipywidgets: 7.6.5
2023-03-22 22:14:13,158:INFO:                tqdm: 4.64.0
2023-03-22 22:14:13,158:INFO:               numpy: 1.21.5
2023-03-22 22:14:13,158:INFO:              pandas: 1.4.2
2023-03-22 22:14:13,158:INFO:              jinja2: 3.1.2
2023-03-22 22:14:13,158:INFO:               scipy: 1.7.3
2023-03-22 22:14:13,158:INFO:              joblib: 1.2.0
2023-03-22 22:14:13,158:INFO:             sklearn: 1.0.2
2023-03-22 22:14:13,158:INFO:                pyod: 1.0.8
2023-03-22 22:14:13,158:INFO:            imblearn: 0.10.1
2023-03-22 22:14:13,158:INFO:   category_encoders: 2.6.0
2023-03-22 22:14:13,158:INFO:            lightgbm: 3.3.5
2023-03-22 22:14:13,158:INFO:               numba: 0.55.1
2023-03-22 22:14:13,158:INFO:            requests: 2.27.1
2023-03-22 22:14:13,158:INFO:          matplotlib: 3.5.1
2023-03-22 22:14:13,158:INFO:          scikitplot: 0.3.7
2023-03-22 22:14:13,158:INFO:         yellowbrick: 1.5
2023-03-22 22:14:13,158:INFO:              plotly: 5.6.0
2023-03-22 22:14:13,158:INFO:             kaleido: 0.2.1
2023-03-22 22:14:13,158:INFO:         statsmodels: 0.13.2
2023-03-22 22:14:13,158:INFO:              sktime: 0.16.1
2023-03-22 22:14:13,158:INFO:               tbats: 1.1.2
2023-03-22 22:14:13,158:INFO:            pmdarima: 2.0.3
2023-03-22 22:14:13,158:INFO:              psutil: 5.9.4
2023-03-22 22:14:13,158:INFO:PyCaret optional dependencies:
2023-03-22 22:14:13,158:INFO:                shap: 0.41.0
2023-03-22 22:14:13,158:INFO:           interpret: Not installed
2023-03-22 22:14:13,158:INFO:                umap: 0.5.3
2023-03-22 22:14:13,158:INFO:    pandas_profiling: 4.0.0
2023-03-22 22:14:13,158:INFO:  explainerdashboard: Not installed
2023-03-22 22:14:13,158:INFO:             autoviz: Not installed
2023-03-22 22:14:13,158:INFO:           fairlearn: Not installed
2023-03-22 22:14:13,158:INFO:             xgboost: 1.7.4
2023-03-22 22:14:13,158:INFO:            catboost: 1.1.1
2023-03-22 22:14:13,158:INFO:              kmodes: 0.12.2
2023-03-22 22:14:13,158:INFO:             mlxtend: 0.21.0
2023-03-22 22:14:13,158:INFO:       statsforecast: Not installed
2023-03-22 22:14:13,158:INFO:        tune_sklearn: Not installed
2023-03-22 22:14:13,158:INFO:                 ray: Not installed
2023-03-22 22:14:13,158:INFO:            hyperopt: Not installed
2023-03-22 22:14:13,158:INFO:              optuna: Not installed
2023-03-22 22:14:13,158:INFO:               skopt: Not installed
2023-03-22 22:14:13,158:INFO:              mlflow: 2.2.2
2023-03-22 22:14:13,158:INFO:              gradio: Not installed
2023-03-22 22:14:13,158:INFO:             fastapi: 0.95.0
2023-03-22 22:14:13,158:INFO:             uvicorn: 0.21.1
2023-03-22 22:14:13,158:INFO:              m2cgen: Not installed
2023-03-22 22:14:13,158:INFO:           evidently: Not installed
2023-03-22 22:14:13,158:INFO:               fugue: Not installed
2023-03-22 22:14:13,158:INFO:           streamlit: Not installed
2023-03-22 22:14:13,158:INFO:             prophet: 1.1.2
2023-03-22 22:14:13,158:INFO:None
2023-03-22 22:14:13,158:INFO:Set up data.
2023-03-22 22:14:13,227:INFO:Set up train/test split.
2023-03-22 22:14:13,298:INFO:Set up index.
2023-03-22 22:14:13,298:INFO:Set up folding strategy.
2023-03-22 22:14:13,298:INFO:Assigning column types.
2023-03-22 22:14:13,345:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-22 22:14:13,345:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,345:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,360:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,454:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,502:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,502:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:14:13,502:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:14:13,502:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,502:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,502:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,611:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,658:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,658:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:14:13,658:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:14:13,658:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-22 22:14:13,658:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,658:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,752:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,800:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,800:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:14:13,800:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:14:13,815:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,815:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,909:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,956:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 22:14:13,956:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:14:13,956:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:14:13,956:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-22 22:14:13,956:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 22:14:14,066:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 22:14:14,098:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 22:14:14,098:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:14:14,113:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:14:14,113:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 22:14:14,223:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 22:14:14,254:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 22:14:14,254:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:14:14,254:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:14:14,254:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-22 22:14:14,364:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 22:14:14,411:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 22:14:14,411:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:14:14,411:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:14:14,521:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 22:14:14,552:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 22:14:14,552:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:14:14,552:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:14:14,552:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-22 22:14:14,662:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 22:14:14,709:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:14:14,709:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:14:14,819:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 22:14:14,851:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:14:14,851:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:14:14,866:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-22 22:14:15,007:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:14:15,007:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:14:15,148:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:14:15,148:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:14:15,164:INFO:Preparing preprocessing pipeline...
2023-03-22 22:14:15,164:INFO:Set up simple imputation.
2023-03-22 22:14:15,164:INFO:Set up column name cleaning.
2023-03-22 22:14:15,321:INFO:Finished creating preprocessing pipeline.
2023-03-22 22:14:15,321:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Mujahid\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Order Id', 'Order Item Discount',
                                             'Order Item Cardprod Id',
                                             'shipping date (DateOrders)',
                                             'order date (DateOrders)',
                                             'Order Customer Id',
                                             'Order Profit Per Order', 'Market',
                                             'Order Region', 'Order State',
                                             'Order Item Total',
                                             'Depa...
                                                              verbose=0))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-03-22 22:14:15,321:INFO:Creating final display dataframe.
2023-03-22 22:14:15,777:INFO:Setup _display_container:                     Description             Value
0                    Session id              5595
1                        Target             Sales
2                   Target type        Regression
3           Original data shape      (180519, 26)
4        Transformed data shape      (180519, 26)
5   Transformed train set shape      (126363, 26)
6    Transformed test set shape       (54156, 26)
7              Numeric features                25
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              29af
2023-03-22 22:14:15,946:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:14:15,946:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:14:16,087:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-22 22:14:16,100:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-22 22:14:16,103:INFO:setup() successfully completed in 4.14s...............
2023-03-22 22:14:16,122:INFO:Initializing compare_models()
2023-03-22 22:14:16,122:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-22 22:14:16,122:INFO:Checking exceptions
2023-03-22 22:14:16,152:INFO:Preparing display monitor
2023-03-22 22:14:16,188:INFO:Initializing Linear Regression
2023-03-22 22:14:16,188:INFO:Total runtime is 0.0 minutes
2023-03-22 22:14:16,191:INFO:SubProcess create_model() called ==================================
2023-03-22 22:14:16,192:INFO:Initializing create_model()
2023-03-22 22:14:16,192:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:14:16,192:INFO:Checking exceptions
2023-03-22 22:14:16,192:INFO:Importing libraries
2023-03-22 22:14:16,192:INFO:Copying training dataset
2023-03-22 22:14:16,273:INFO:Defining folds
2023-03-22 22:14:16,273:INFO:Declaring metric variables
2023-03-22 22:14:16,276:INFO:Importing untrained model
2023-03-22 22:14:16,279:INFO:Linear Regression Imported successfully
2023-03-22 22:14:16,286:INFO:Starting cross validation
2023-03-22 22:14:16,286:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:14:31,515:INFO:Calculating mean and std
2023-03-22 22:14:31,515:INFO:Creating metrics dataframe
2023-03-22 22:14:32,934:INFO:Uploading results into container
2023-03-22 22:14:32,934:INFO:Uploading model into container now
2023-03-22 22:14:32,934:INFO:_master_model_container: 1
2023-03-22 22:14:32,934:INFO:_display_container: 2
2023-03-22 22:14:32,934:INFO:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1,
                 normalize='deprecated', positive=False)
2023-03-22 22:14:32,934:INFO:create_model() successfully completed......................................
2023-03-22 22:14:33,062:INFO:SubProcess create_model() end ==================================
2023-03-22 22:14:33,062:INFO:Creating metrics dataframe
2023-03-22 22:14:33,077:INFO:Initializing Lasso Regression
2023-03-22 22:14:33,077:INFO:Total runtime is 0.28149141470591227 minutes
2023-03-22 22:14:33,077:INFO:SubProcess create_model() called ==================================
2023-03-22 22:14:33,077:INFO:Initializing create_model()
2023-03-22 22:14:33,077:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:14:33,077:INFO:Checking exceptions
2023-03-22 22:14:33,077:INFO:Importing libraries
2023-03-22 22:14:33,077:INFO:Copying training dataset
2023-03-22 22:14:33,160:INFO:Defining folds
2023-03-22 22:14:33,160:INFO:Declaring metric variables
2023-03-22 22:14:33,162:INFO:Importing untrained model
2023-03-22 22:14:33,167:INFO:Lasso Regression Imported successfully
2023-03-22 22:14:33,169:INFO:Starting cross validation
2023-03-22 22:14:33,169:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:15:27,375:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.039e+08, tolerance: 1.985e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:15:27,454:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.044e+08, tolerance: 1.989e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:15:27,516:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.996e+08, tolerance: 1.972e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:15:27,652:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.996e+08, tolerance: 1.971e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:15:27,720:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.041e+08, tolerance: 1.989e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:15:27,720:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.035e+08, tolerance: 1.982e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:15:27,846:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.056e+08, tolerance: 1.991e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:15:27,909:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.026e+08, tolerance: 1.981e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:15:44,357:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.014e+08, tolerance: 1.974e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:15:44,515:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.018e+08, tolerance: 1.979e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:15:54,903:INFO:Calculating mean and std
2023-03-22 22:15:54,903:INFO:Creating metrics dataframe
2023-03-22 22:15:57,311:INFO:Uploading results into container
2023-03-22 22:15:57,311:INFO:Uploading model into container now
2023-03-22 22:15:57,311:INFO:_master_model_container: 2
2023-03-22 22:15:57,311:INFO:_display_container: 2
2023-03-22 22:15:57,311:INFO:Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,
      normalize='deprecated', positive=False, precompute=False,
      random_state=5595, selection='cyclic', tol=0.0001, warm_start=False)
2023-03-22 22:15:57,315:INFO:create_model() successfully completed......................................
2023-03-22 22:15:57,448:INFO:SubProcess create_model() end ==================================
2023-03-22 22:15:57,448:INFO:Creating metrics dataframe
2023-03-22 22:15:57,464:INFO:Initializing Ridge Regression
2023-03-22 22:15:57,464:INFO:Total runtime is 1.6879322052001953 minutes
2023-03-22 22:15:57,477:INFO:SubProcess create_model() called ==================================
2023-03-22 22:15:57,477:INFO:Initializing create_model()
2023-03-22 22:15:57,477:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:15:57,477:INFO:Checking exceptions
2023-03-22 22:15:57,477:INFO:Importing libraries
2023-03-22 22:15:57,477:INFO:Copying training dataset
2023-03-22 22:15:57,587:INFO:Defining folds
2023-03-22 22:15:57,587:INFO:Declaring metric variables
2023-03-22 22:15:57,599:INFO:Importing untrained model
2023-03-22 22:15:57,604:INFO:Ridge Regression Imported successfully
2023-03-22 22:15:57,607:INFO:Starting cross validation
2023-03-22 22:15:57,607:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:16:22,157:INFO:Calculating mean and std
2023-03-22 22:16:22,157:INFO:Creating metrics dataframe
2023-03-22 22:16:24,629:INFO:Uploading results into container
2023-03-22 22:16:24,629:INFO:Uploading model into container now
2023-03-22 22:16:24,629:INFO:_master_model_container: 3
2023-03-22 22:16:24,629:INFO:_display_container: 2
2023-03-22 22:16:24,642:INFO:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
      normalize='deprecated', positive=False, random_state=5595, solver='auto',
      tol=0.001)
2023-03-22 22:16:24,642:INFO:create_model() successfully completed......................................
2023-03-22 22:16:24,778:INFO:SubProcess create_model() end ==================================
2023-03-22 22:16:24,778:INFO:Creating metrics dataframe
2023-03-22 22:16:24,794:INFO:Initializing Elastic Net
2023-03-22 22:16:24,794:INFO:Total runtime is 2.1434298276901247 minutes
2023-03-22 22:16:24,794:INFO:SubProcess create_model() called ==================================
2023-03-22 22:16:24,794:INFO:Initializing create_model()
2023-03-22 22:16:24,794:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:16:24,794:INFO:Checking exceptions
2023-03-22 22:16:24,794:INFO:Importing libraries
2023-03-22 22:16:24,794:INFO:Copying training dataset
2023-03-22 22:16:24,930:INFO:Defining folds
2023-03-22 22:16:24,930:INFO:Declaring metric variables
2023-03-22 22:16:24,949:INFO:Importing untrained model
2023-03-22 22:16:24,954:INFO:Elastic Net Imported successfully
2023-03-22 22:16:24,954:INFO:Starting cross validation
2023-03-22 22:16:24,954:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:17:32,555:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.074e+08, tolerance: 1.991e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:17:32,617:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.062e+08, tolerance: 1.989e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:17:33,197:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.058e+08, tolerance: 1.985e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:17:33,216:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.014e+08, tolerance: 1.971e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:17:33,260:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.014e+08, tolerance: 1.972e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:17:33,433:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.054e+08, tolerance: 1.982e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:17:33,495:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.060e+08, tolerance: 1.989e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:17:33,516:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.044e+08, tolerance: 1.981e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:17:55,377:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.031e+08, tolerance: 1.974e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:17:55,408:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.036e+08, tolerance: 1.979e+05
  model = cd_fast.enet_coordinate_descent(

2023-03-22 22:18:05,653:INFO:Calculating mean and std
2023-03-22 22:18:05,653:INFO:Creating metrics dataframe
2023-03-22 22:18:08,055:INFO:Uploading results into container
2023-03-22 22:18:08,055:INFO:Uploading model into container now
2023-03-22 22:18:08,055:INFO:_master_model_container: 4
2023-03-22 22:18:08,055:INFO:_display_container: 2
2023-03-22 22:18:08,055:INFO:ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,
           max_iter=1000, normalize='deprecated', positive=False,
           precompute=False, random_state=5595, selection='cyclic', tol=0.0001,
           warm_start=False)
2023-03-22 22:18:08,055:INFO:create_model() successfully completed......................................
2023-03-22 22:18:08,197:INFO:SubProcess create_model() end ==================================
2023-03-22 22:18:08,197:INFO:Creating metrics dataframe
2023-03-22 22:18:08,216:INFO:Initializing Least Angle Regression
2023-03-22 22:18:08,216:INFO:Total runtime is 3.867132178942363 minutes
2023-03-22 22:18:08,222:INFO:SubProcess create_model() called ==================================
2023-03-22 22:18:08,223:INFO:Initializing create_model()
2023-03-22 22:18:08,223:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:18:08,223:INFO:Checking exceptions
2023-03-22 22:18:08,223:INFO:Importing libraries
2023-03-22 22:18:08,223:INFO:Copying training dataset
2023-03-22 22:18:08,336:INFO:Defining folds
2023-03-22 22:18:08,336:INFO:Declaring metric variables
2023-03-22 22:18:08,341:INFO:Importing untrained model
2023-03-22 22:18:08,348:INFO:Least Angle Regression Imported successfully
2023-03-22 22:18:08,357:INFO:Starting cross validation
2023-03-22 22:18:08,358:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:18:08,719:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:18:08,779:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:18:08,813:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:18:08,860:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:18:08,876:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.917e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,879:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.773e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,879:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.816e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,879:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.816e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,879:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.816e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,879:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.816e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,879:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.751e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,879:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.809e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,879:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.809e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,879:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.809e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,879:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.789e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,892:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=5.306e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,892:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.655e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,892:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.655e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,892:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.655e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,892:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.445e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,892:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=8.028e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,892:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=8.028e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,892:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=8.028e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,892:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:18:08,908:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.695e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,908:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.020e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,923:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.020e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,923:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.038e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,923:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.038e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,923:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.252e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,923:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.697e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,923:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.697e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,923:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.315e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,923:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.868e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,923:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=6.319e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,923:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=6.319e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,923:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.799e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,923:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.466e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,923:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.466e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,923:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.929e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,939:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.929e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,939:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.929e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,939:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.661e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,939:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:18:08,954:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.881e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,970:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.684e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,970:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.043e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,970:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.043e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,970:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.553e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,979:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.232e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,979:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.607e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,979:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.270e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,979:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.091e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,979:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.893e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,979:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.852e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,979:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.393e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,979:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.393e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,986:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:18:08,986:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=8.853e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,986:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=4.906e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,986:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=4.906e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,986:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=5.607e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,986:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=5.607e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:08,986:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=5.607e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,002:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.207e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,002:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.118e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,002:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.118e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,002:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.834e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,017:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.045e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,017:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.045e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,017:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.801e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,017:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.778e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,017:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.372e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,017:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.372e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,017:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.911e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,017:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.127e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,017:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.127e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,033:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.680e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,033:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.016e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,033:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.016e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,049:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.800e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,049:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.800e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,049:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.800e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,049:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.012e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,049:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=5.608e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,049:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.412e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,049:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.412e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,049:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.219e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,049:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.815e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,049:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.023e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,049:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.264e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,049:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.264e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,049:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.264e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,080:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.304e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,080:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.823e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,080:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.823e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,080:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.823e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,080:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.078e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,080:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=8.453e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,080:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=8.453e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,080:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.208e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,080:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.454e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,080:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.199e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,096:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.331e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,096:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.351e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,096:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=6.368e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

odel = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:18:09,096:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=4.183e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,096:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.752e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,096:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.171e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,096:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.171e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,111:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.403e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,111:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.403e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,111:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.403e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,111:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=8.288e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,111:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.549e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,111:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.038e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,111:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.694e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,111:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.694e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,127:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.363e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,127:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.206e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,127:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.206e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,127:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.519e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,127:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.519e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,127:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.519e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,127:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=9.030e-06, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,206:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.902e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,221:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.737e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,221:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.512e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,221:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.299e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,221:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.143e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,221:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.119e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,221:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.927e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,221:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.927e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,221:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.219e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,221:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.314e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,221:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.989e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,237:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.441e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,237:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.220e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,237:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.156e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,237:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.000e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:09,237:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=6.677e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,293:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:18:18,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.880e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.970e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.372e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.372e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.372e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.236e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.204e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.303e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.303e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.303e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.303e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:18:18,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.448e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.075e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.075e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,435:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.356e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,435:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.597e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.224e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.754e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.754e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.754e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.028e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=8.185e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=8.185e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.492e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.492e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.351e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.793e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.525e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.315e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=6.913e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=6.913e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.160e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,545:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.160e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:18,545:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.160e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 22:18:33,804:INFO:Calculating mean and std
2023-03-22 22:18:33,804:INFO:Creating metrics dataframe
2023-03-22 22:18:36,225:INFO:Uploading results into container
2023-03-22 22:18:36,225:INFO:Uploading model into container now
2023-03-22 22:18:36,225:INFO:_master_model_container: 5
2023-03-22 22:18:36,225:INFO:_display_container: 2
2023-03-22 22:18:36,225:INFO:Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,
     jitter=None, n_nonzero_coefs=500, normalize='deprecated',
     precompute='auto', random_state=5595, verbose=False)
2023-03-22 22:18:36,225:INFO:create_model() successfully completed......................................
2023-03-22 22:18:36,362:INFO:SubProcess create_model() end ==================================
2023-03-22 22:18:36,362:INFO:Creating metrics dataframe
2023-03-22 22:18:36,378:INFO:Initializing Lasso Least Angle Regression
2023-03-22 22:18:36,378:INFO:Total runtime is 4.336501888434093 minutes
2023-03-22 22:18:36,389:INFO:SubProcess create_model() called ==================================
2023-03-22 22:18:36,389:INFO:Initializing create_model()
2023-03-22 22:18:36,389:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:18:36,389:INFO:Checking exceptions
2023-03-22 22:18:36,390:INFO:Importing libraries
2023-03-22 22:18:36,390:INFO:Copying training dataset
2023-03-22 22:18:36,482:INFO:Defining folds
2023-03-22 22:18:36,482:INFO:Declaring metric variables
2023-03-22 22:18:36,500:INFO:Importing untrained model
2023-03-22 22:18:36,505:INFO:Lasso Least Angle Regression Imported successfully
2023-03-22 22:18:36,516:INFO:Starting cross validation
2023-03-22 22:18:36,518:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:18:36,854:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 22:18:36,948:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 22:18:36,979:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 22:18:37,010:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 22:18:37,042:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 22:18:37,058:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 22:18:37,058:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 22:18:37,127:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 22:18:45,885:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 22:18:45,949:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 22:19:00,451:INFO:Calculating mean and std
2023-03-22 22:19:00,451:INFO:Creating metrics dataframe
2023-03-22 22:19:02,837:INFO:Uploading results into container
2023-03-22 22:19:02,837:INFO:Uploading model into container now
2023-03-22 22:19:02,837:INFO:_master_model_container: 6
2023-03-22 22:19:02,837:INFO:_display_container: 2
2023-03-22 22:19:02,837:INFO:LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,
          fit_path=True, jitter=None, max_iter=500, normalize='deprecated',
          positive=False, precompute='auto', random_state=5595, verbose=False)
2023-03-22 22:19:02,837:INFO:create_model() successfully completed......................................
2023-03-22 22:19:02,974:INFO:SubProcess create_model() end ==================================
2023-03-22 22:19:02,974:INFO:Creating metrics dataframe
2023-03-22 22:19:02,999:INFO:Initializing Orthogonal Matching Pursuit
2023-03-22 22:19:02,999:INFO:Total runtime is 4.78019054333369 minutes
2023-03-22 22:19:03,003:INFO:SubProcess create_model() called ==================================
2023-03-22 22:19:03,003:INFO:Initializing create_model()
2023-03-22 22:19:03,003:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:19:03,003:INFO:Checking exceptions
2023-03-22 22:19:03,003:INFO:Importing libraries
2023-03-22 22:19:03,003:INFO:Copying training dataset
2023-03-22 22:19:03,110:INFO:Defining folds
2023-03-22 22:19:03,110:INFO:Declaring metric variables
2023-03-22 22:19:03,116:INFO:Importing untrained model
2023-03-22 22:19:03,120:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-22 22:19:03,131:INFO:Starting cross validation
2023-03-22 22:19:03,132:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:19:03,433:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:19:03,511:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:19:03,558:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:19:03,590:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:19:03,606:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:19:03,653:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:19:03,684:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:19:03,747:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:19:11,982:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:19:11,998:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 22:19:26,017:INFO:Calculating mean and std
2023-03-22 22:19:26,017:INFO:Creating metrics dataframe
2023-03-22 22:19:28,258:INFO:Uploading results into container
2023-03-22 22:19:28,258:INFO:Uploading model into container now
2023-03-22 22:19:28,258:INFO:_master_model_container: 7
2023-03-22 22:19:28,258:INFO:_display_container: 2
2023-03-22 22:19:28,258:INFO:OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          normalize='deprecated', precompute='auto', tol=None)
2023-03-22 22:19:28,258:INFO:create_model() successfully completed......................................
2023-03-22 22:19:28,383:INFO:SubProcess create_model() end ==================================
2023-03-22 22:19:28,383:INFO:Creating metrics dataframe
2023-03-22 22:19:28,412:INFO:Initializing Bayesian Ridge
2023-03-22 22:19:28,412:INFO:Total runtime is 5.203730217615764 minutes
2023-03-22 22:19:28,417:INFO:SubProcess create_model() called ==================================
2023-03-22 22:19:28,417:INFO:Initializing create_model()
2023-03-22 22:19:28,417:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:19:28,417:INFO:Checking exceptions
2023-03-22 22:19:28,417:INFO:Importing libraries
2023-03-22 22:19:28,418:INFO:Copying training dataset
2023-03-22 22:19:28,510:INFO:Defining folds
2023-03-22 22:19:28,510:INFO:Declaring metric variables
2023-03-22 22:19:28,523:INFO:Importing untrained model
2023-03-22 22:19:28,528:INFO:Bayesian Ridge Imported successfully
2023-03-22 22:19:28,532:INFO:Starting cross validation
2023-03-22 22:19:28,532:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:19:51,863:INFO:Calculating mean and std
2023-03-22 22:19:51,863:INFO:Creating metrics dataframe
2023-03-22 22:19:54,134:INFO:Uploading results into container
2023-03-22 22:19:54,134:INFO:Uploading model into container now
2023-03-22 22:19:54,134:INFO:_master_model_container: 8
2023-03-22 22:19:54,134:INFO:_display_container: 2
2023-03-22 22:19:54,134:INFO:BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,
              compute_score=False, copy_X=True, fit_intercept=True,
              lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, n_iter=300,
              normalize='deprecated', tol=0.001, verbose=False)
2023-03-22 22:19:54,134:INFO:create_model() successfully completed......................................
2023-03-22 22:19:54,261:INFO:SubProcess create_model() end ==================================
2023-03-22 22:19:54,261:INFO:Creating metrics dataframe
2023-03-22 22:19:54,285:INFO:Initializing Passive Aggressive Regressor
2023-03-22 22:19:54,285:INFO:Total runtime is 5.634960754712423 minutes
2023-03-22 22:19:54,291:INFO:SubProcess create_model() called ==================================
2023-03-22 22:19:54,291:INFO:Initializing create_model()
2023-03-22 22:19:54,291:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:19:54,292:INFO:Checking exceptions
2023-03-22 22:19:54,292:INFO:Importing libraries
2023-03-22 22:19:54,292:INFO:Copying training dataset
2023-03-22 22:19:54,373:INFO:Defining folds
2023-03-22 22:19:54,373:INFO:Declaring metric variables
2023-03-22 22:19:54,389:INFO:Importing untrained model
2023-03-22 22:19:54,396:INFO:Passive Aggressive Regressor Imported successfully
2023-03-22 22:19:54,399:INFO:Starting cross validation
2023-03-22 22:19:54,406:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:20:21,445:INFO:Calculating mean and std
2023-03-22 22:20:21,445:INFO:Creating metrics dataframe
2023-03-22 22:20:23,542:INFO:Uploading results into container
2023-03-22 22:20:23,542:INFO:Uploading model into container now
2023-03-22 22:20:23,542:INFO:_master_model_container: 9
2023-03-22 22:20:23,542:INFO:_display_container: 2
2023-03-22 22:20:23,542:INFO:PassiveAggressiveRegressor(C=1.0, average=False, early_stopping=False,
                           epsilon=0.1, fit_intercept=True,
                           loss='epsilon_insensitive', max_iter=1000,
                           n_iter_no_change=5, random_state=5595, shuffle=True,
                           tol=0.001, validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-03-22 22:20:23,542:INFO:create_model() successfully completed......................................
2023-03-22 22:20:23,683:INFO:SubProcess create_model() end ==================================
2023-03-22 22:20:23,683:INFO:Creating metrics dataframe
2023-03-22 22:20:23,683:INFO:Initializing Huber Regressor
2023-03-22 22:20:23,683:INFO:Total runtime is 6.124926515420278 minutes
2023-03-22 22:20:23,703:INFO:SubProcess create_model() called ==================================
2023-03-22 22:20:23,703:INFO:Initializing create_model()
2023-03-22 22:20:23,703:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:20:23,704:INFO:Checking exceptions
2023-03-22 22:20:23,704:INFO:Importing libraries
2023-03-22 22:20:23,704:INFO:Copying training dataset
2023-03-22 22:20:23,801:INFO:Defining folds
2023-03-22 22:20:23,801:INFO:Declaring metric variables
2023-03-22 22:20:23,801:INFO:Importing untrained model
2023-03-22 22:20:23,812:INFO:Huber Regressor Imported successfully
2023-03-22 22:20:23,821:INFO:Starting cross validation
2023-03-22 22:20:23,822:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:20:32,291:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 22:20:32,432:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 22:20:32,965:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 22:20:32,965:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 22:20:33,044:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 22:20:33,060:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 22:20:33,154:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 22:20:33,185:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 22:20:43,421:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 22:20:43,689:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 22:20:55,036:INFO:Calculating mean and std
2023-03-22 22:20:55,036:INFO:Creating metrics dataframe
2023-03-22 22:20:57,201:INFO:Uploading results into container
2023-03-22 22:20:57,201:INFO:Uploading model into container now
2023-03-22 22:20:57,201:INFO:_master_model_container: 10
2023-03-22 22:20:57,201:INFO:_display_container: 2
2023-03-22 22:20:57,217:INFO:HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,
               tol=1e-05, warm_start=False)
2023-03-22 22:20:57,217:INFO:create_model() successfully completed......................................
2023-03-22 22:20:57,358:INFO:SubProcess create_model() end ==================================
2023-03-22 22:20:57,358:INFO:Creating metrics dataframe
2023-03-22 22:20:57,371:INFO:Initializing K Neighbors Regressor
2023-03-22 22:20:57,371:INFO:Total runtime is 6.686387550830841 minutes
2023-03-22 22:20:57,371:INFO:SubProcess create_model() called ==================================
2023-03-22 22:20:57,371:INFO:Initializing create_model()
2023-03-22 22:20:57,371:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:20:57,371:INFO:Checking exceptions
2023-03-22 22:20:57,371:INFO:Importing libraries
2023-03-22 22:20:57,371:INFO:Copying training dataset
2023-03-22 22:20:57,465:INFO:Defining folds
2023-03-22 22:20:57,465:INFO:Declaring metric variables
2023-03-22 22:20:57,480:INFO:Importing untrained model
2023-03-22 22:20:57,487:INFO:K Neighbors Regressor Imported successfully
2023-03-22 22:20:57,487:INFO:Starting cross validation
2023-03-22 22:20:57,487:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:20:59,530:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.24s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-22 22:20:59,562:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-22 22:20:59,988:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-22 22:21:00,051:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-22 22:21:02,635:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-22 22:21:04,716:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-22 22:21:05,377:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 229, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1434, in _parallel_pairwise
    ret = np.empty((X.shape[0], Y.shape[0]), dtype=dtype, order="F")
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.00 GiB for an array with shape (1180, 113727) and data type float64

  warnings.warn(

2023-03-22 22:21:50,911:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 229, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (1180, 14216) and data type float64

  warnings.warn(

2023-03-22 22:21:54,306:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 229, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (1180, 14216) and data type float64

  warnings.warn(

2023-03-22 22:22:03,776:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 229, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (1180, 14216) and data type float64

  warnings.warn(

2023-03-22 22:22:30,187:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 229, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (1180, 14216) and data type float64

  warnings.warn(

2023-03-22 22:22:35,041:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 17.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-22 22:23:04,402:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 229, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (1180, 14216) and data type float64

  warnings.warn(

2023-03-22 22:23:06,148:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 229, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (1180, 14216) and data type float64

  warnings.warn(

2023-03-22 22:25:37,085:INFO:Calculating mean and std
2023-03-22 22:25:37,210:INFO:Creating metrics dataframe
2023-03-22 22:25:40,133:INFO:Uploading results into container
2023-03-22 22:25:40,148:INFO:Uploading model into container now
2023-03-22 22:25:40,148:INFO:_master_model_container: 11
2023-03-22 22:25:40,148:INFO:_display_container: 2
2023-03-22 22:25:40,169:INFO:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                    weights='uniform')
2023-03-22 22:25:40,169:INFO:create_model() successfully completed......................................
2023-03-22 22:25:41,293:WARNING:create_model() for KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                    weights='uniform') raised an exception or returned all 0.0, trying without fit_kwargs:
2023-03-22 22:25:41,299:WARNING:Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 796, in compare_models
    assert (
AssertionError

2023-03-22 22:25:41,300:INFO:Initializing create_model()
2023-03-22 22:25:41,300:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:25:41,300:INFO:Checking exceptions
2023-03-22 22:25:41,300:INFO:Importing libraries
2023-03-22 22:25:41,301:INFO:Copying training dataset
2023-03-22 22:25:41,455:INFO:Defining folds
2023-03-22 22:25:41,455:INFO:Declaring metric variables
2023-03-22 22:25:41,455:INFO:Importing untrained model
2023-03-22 22:25:41,469:INFO:K Neighbors Regressor Imported successfully
2023-03-22 22:25:41,473:INFO:Starting cross validation
2023-03-22 22:25:41,480:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:25:45,170:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-22 22:25:45,578:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 2.38s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-22 22:25:47,741:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-22 22:25:48,368:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 2.17s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-22 22:25:48,556:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 5.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-22 22:25:49,670:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.16s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-22 22:25:50,282:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.00s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:25:50,925:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:26:21,281:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 39.14s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-22 22:26:21,626:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 32.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-22 22:26:25,203:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 38.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-22 22:26:33,995:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 2.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:26:34,416:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 2.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-22 22:26:34,823:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 2.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-22 22:26:35,858:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-22 22:26:36,834:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-22 22:26:37,775:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:26:40,019:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.46s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-22 22:26:41,404:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-22 22:26:42,819:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.38s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-22 22:26:59,674:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 229, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (1180, 14216) and data type float64

  warnings.warn(

2023-03-22 22:27:02,721:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 229, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (1180, 14216) and data type float64

  warnings.warn(

2023-03-22 22:27:04,478:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-22 22:27:05,501:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-22 22:27:06,937:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 229, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\extmath.py", line 153, in safe_sparse_dot
    ret = a @ b
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (1180, 14216) and data type float64

  warnings.warn(

2023-03-22 22:27:54,676:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 229, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (1180, 14216) and data type float64

  warnings.warn(

2023-03-22 22:42:04,144:INFO:Calculating mean and std
2023-03-22 22:42:04,222:INFO:Creating metrics dataframe
2023-03-22 22:42:07,533:INFO:Uploading results into container
2023-03-22 22:42:07,549:INFO:Uploading model into container now
2023-03-22 22:42:07,549:INFO:_master_model_container: 12
2023-03-22 22:42:07,549:INFO:_display_container: 2
2023-03-22 22:42:07,565:INFO:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                    weights='uniform')
2023-03-22 22:42:07,565:INFO:create_model() successfully completed......................................
2023-03-22 22:42:08,819:ERROR:create_model() for KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                    weights='uniform') raised an exception or returned all 0.0:
2023-03-22 22:42:08,819:ERROR:Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 796, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 812, in compare_models
    assert (
AssertionError

2023-03-22 22:42:08,819:INFO:Initializing Decision Tree Regressor
2023-03-22 22:42:08,819:INFO:Total runtime is 27.877189485232034 minutes
2023-03-22 22:42:08,819:INFO:SubProcess create_model() called ==================================
2023-03-22 22:42:08,819:INFO:Initializing create_model()
2023-03-22 22:42:08,819:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:42:08,819:INFO:Checking exceptions
2023-03-22 22:42:08,819:INFO:Importing libraries
2023-03-22 22:42:08,819:INFO:Copying training dataset
2023-03-22 22:42:08,991:INFO:Defining folds
2023-03-22 22:42:08,991:INFO:Declaring metric variables
2023-03-22 22:42:08,991:INFO:Importing untrained model
2023-03-22 22:42:08,991:INFO:Decision Tree Regressor Imported successfully
2023-03-22 22:42:09,007:INFO:Starting cross validation
2023-03-22 22:42:09,007:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:42:56,850:INFO:Calculating mean and std
2023-03-22 22:42:56,850:INFO:Creating metrics dataframe
2023-03-22 22:42:59,157:INFO:Uploading results into container
2023-03-22 22:42:59,157:INFO:Uploading model into container now
2023-03-22 22:42:59,157:INFO:_master_model_container: 13
2023-03-22 22:42:59,157:INFO:_display_container: 2
2023-03-22 22:42:59,157:INFO:DecisionTreeRegressor(ccp_alpha=0.0, criterion='squared_error', max_depth=None,
                      max_features=None, max_leaf_nodes=None,
                      min_impurity_decrease=0.0, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      random_state=5595, splitter='best')
2023-03-22 22:42:59,157:INFO:create_model() successfully completed......................................
2023-03-22 22:42:59,298:INFO:SubProcess create_model() end ==================================
2023-03-22 22:42:59,298:INFO:Creating metrics dataframe
2023-03-22 22:42:59,330:INFO:Initializing Random Forest Regressor
2023-03-22 22:42:59,330:INFO:Total runtime is 28.719030686219533 minutes
2023-03-22 22:42:59,330:INFO:SubProcess create_model() called ==================================
2023-03-22 22:42:59,330:INFO:Initializing create_model()
2023-03-22 22:42:59,330:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:42:59,330:INFO:Checking exceptions
2023-03-22 22:42:59,330:INFO:Importing libraries
2023-03-22 22:42:59,330:INFO:Copying training dataset
2023-03-22 22:42:59,423:INFO:Defining folds
2023-03-22 22:42:59,423:INFO:Declaring metric variables
2023-03-22 22:42:59,439:INFO:Importing untrained model
2023-03-22 22:42:59,439:INFO:Random Forest Regressor Imported successfully
2023-03-22 22:42:59,439:INFO:Starting cross validation
2023-03-22 22:42:59,454:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:51:48,501:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:51:48,689:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:51:48,971:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:51:49,096:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:51:49,243:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:51:49,379:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:51:49,881:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:51:50,383:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-22 22:54:29,356:INFO:Calculating mean and std
2023-03-22 22:54:29,356:INFO:Creating metrics dataframe
2023-03-22 22:54:32,306:INFO:Uploading results into container
2023-03-22 22:54:32,306:INFO:Uploading model into container now
2023-03-22 22:54:32,306:INFO:_master_model_container: 14
2023-03-22 22:54:32,306:INFO:_display_container: 2
2023-03-22 22:54:32,306:INFO:RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='squared_error',
                      max_depth=None, max_features='auto', max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_samples_leaf=1, min_samples_split=2,
                      min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,
                      oob_score=False, random_state=5595, verbose=0,
                      warm_start=False)
2023-03-22 22:54:32,306:INFO:create_model() successfully completed......................................
2023-03-22 22:54:32,463:INFO:SubProcess create_model() end ==================================
2023-03-22 22:54:32,463:INFO:Creating metrics dataframe
2023-03-22 22:54:32,479:INFO:Initializing Extra Trees Regressor
2023-03-22 22:54:32,479:INFO:Total runtime is 40.27151523828506 minutes
2023-03-22 22:54:32,479:INFO:SubProcess create_model() called ==================================
2023-03-22 22:54:32,479:INFO:Initializing create_model()
2023-03-22 22:54:32,479:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:54:32,479:INFO:Checking exceptions
2023-03-22 22:54:32,479:INFO:Importing libraries
2023-03-22 22:54:32,479:INFO:Copying training dataset
2023-03-22 22:54:32,588:INFO:Defining folds
2023-03-22 22:54:32,588:INFO:Declaring metric variables
2023-03-22 22:54:32,588:INFO:Importing untrained model
2023-03-22 22:54:32,588:INFO:Extra Trees Regressor Imported successfully
2023-03-22 22:54:32,604:INFO:Starting cross validation
2023-03-22 22:54:32,604:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 22:57:32,374:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.21s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:57:33,229:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:57:33,300:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:57:33,692:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:57:34,116:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:57:34,163:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:57:34,383:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:57:34,383:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 22:57:35,451:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-22 22:58:35,170:INFO:Calculating mean and std
2023-03-22 22:58:35,170:INFO:Creating metrics dataframe
2023-03-22 22:58:37,335:INFO:Uploading results into container
2023-03-22 22:58:37,335:INFO:Uploading model into container now
2023-03-22 22:58:37,335:INFO:_master_model_container: 15
2023-03-22 22:58:37,335:INFO:_display_container: 2
2023-03-22 22:58:37,335:INFO:ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',
                    max_depth=None, max_features='auto', max_leaf_nodes=None,
                    max_samples=None, min_impurity_decrease=0.0,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,
                    oob_score=False, random_state=5595, verbose=0,
                    warm_start=False)
2023-03-22 22:58:37,335:INFO:create_model() successfully completed......................................
2023-03-22 22:58:37,492:INFO:SubProcess create_model() end ==================================
2023-03-22 22:58:37,492:INFO:Creating metrics dataframe
2023-03-22 22:58:37,508:INFO:Initializing AdaBoost Regressor
2023-03-22 22:58:37,508:INFO:Total runtime is 44.35533507664998 minutes
2023-03-22 22:58:37,508:INFO:SubProcess create_model() called ==================================
2023-03-22 22:58:37,508:INFO:Initializing create_model()
2023-03-22 22:58:37,508:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 22:58:37,508:INFO:Checking exceptions
2023-03-22 22:58:37,508:INFO:Importing libraries
2023-03-22 22:58:37,508:INFO:Copying training dataset
2023-03-22 22:58:37,602:INFO:Defining folds
2023-03-22 22:58:37,602:INFO:Declaring metric variables
2023-03-22 22:58:37,602:INFO:Importing untrained model
2023-03-22 22:58:37,618:INFO:AdaBoost Regressor Imported successfully
2023-03-22 22:58:37,618:INFO:Starting cross validation
2023-03-22 22:58:37,618:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 23:00:11,286:INFO:Calculating mean and std
2023-03-22 23:00:11,286:INFO:Creating metrics dataframe
2023-03-22 23:00:12,991:INFO:Uploading results into container
2023-03-22 23:00:12,991:INFO:Uploading model into container now
2023-03-22 23:00:12,991:INFO:_master_model_container: 16
2023-03-22 23:00:12,991:INFO:_display_container: 2
2023-03-22 23:00:12,991:INFO:AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',
                  n_estimators=50, random_state=5595)
2023-03-22 23:00:12,991:INFO:create_model() successfully completed......................................
2023-03-22 23:00:13,131:INFO:SubProcess create_model() end ==================================
2023-03-22 23:00:13,131:INFO:Creating metrics dataframe
2023-03-22 23:00:13,146:INFO:Initializing Gradient Boosting Regressor
2023-03-22 23:00:13,146:INFO:Total runtime is 45.949301421642296 minutes
2023-03-22 23:00:13,151:INFO:SubProcess create_model() called ==================================
2023-03-22 23:00:13,151:INFO:Initializing create_model()
2023-03-22 23:00:13,151:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 23:00:13,151:INFO:Checking exceptions
2023-03-22 23:00:13,151:INFO:Importing libraries
2023-03-22 23:00:13,151:INFO:Copying training dataset
2023-03-22 23:00:13,216:INFO:Defining folds
2023-03-22 23:00:13,216:INFO:Declaring metric variables
2023-03-22 23:00:13,223:INFO:Importing untrained model
2023-03-22 23:00:13,229:INFO:Gradient Boosting Regressor Imported successfully
2023-03-22 23:00:13,231:INFO:Starting cross validation
2023-03-22 23:00:13,236:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 23:03:00,691:INFO:Calculating mean and std
2023-03-22 23:03:00,691:INFO:Creating metrics dataframe
2023-03-22 23:03:02,401:INFO:Uploading results into container
2023-03-22 23:03:02,401:INFO:Uploading model into container now
2023-03-22 23:03:02,406:INFO:_master_model_container: 17
2023-03-22 23:03:02,406:INFO:_display_container: 2
2023-03-22 23:03:02,406:INFO:GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',
                          init=None, learning_rate=0.1, loss='squared_error',
                          max_depth=3, max_features=None, max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_samples_leaf=1,
                          min_samples_split=2, min_weight_fraction_leaf=0.0,
                          n_estimators=100, n_iter_no_change=None,
                          random_state=5595, subsample=1.0, tol=0.0001,
                          validation_fraction=0.1, verbose=0, warm_start=False)
2023-03-22 23:03:02,406:INFO:create_model() successfully completed......................................
2023-03-22 23:03:02,541:INFO:SubProcess create_model() end ==================================
2023-03-22 23:03:02,541:INFO:Creating metrics dataframe
2023-03-22 23:03:02,561:INFO:Initializing Extreme Gradient Boosting
2023-03-22 23:03:02,561:INFO:Total runtime is 48.77288448810577 minutes
2023-03-22 23:03:02,561:INFO:SubProcess create_model() called ==================================
2023-03-22 23:03:02,561:INFO:Initializing create_model()
2023-03-22 23:03:02,561:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 23:03:02,561:INFO:Checking exceptions
2023-03-22 23:03:02,561:INFO:Importing libraries
2023-03-22 23:03:02,561:INFO:Copying training dataset
2023-03-22 23:03:02,634:INFO:Defining folds
2023-03-22 23:03:02,634:INFO:Declaring metric variables
2023-03-22 23:03:02,636:INFO:Importing untrained model
2023-03-22 23:03:02,641:INFO:Extreme Gradient Boosting Imported successfully
2023-03-22 23:03:02,647:INFO:Starting cross validation
2023-03-22 23:03:02,647:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 23:04:47,633:INFO:Calculating mean and std
2023-03-22 23:04:47,633:INFO:Creating metrics dataframe
2023-03-22 23:04:49,333:INFO:Uploading results into container
2023-03-22 23:04:49,333:INFO:Uploading model into container now
2023-03-22 23:04:49,333:INFO:_master_model_container: 18
2023-03-22 23:04:49,333:INFO:_display_container: 2
2023-03-22 23:04:49,338:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             objective='reg:squarederror', predictor=None, ...)
2023-03-22 23:04:49,338:INFO:create_model() successfully completed......................................
2023-03-22 23:04:49,473:INFO:SubProcess create_model() end ==================================
2023-03-22 23:04:49,473:INFO:Creating metrics dataframe
2023-03-22 23:04:49,493:INFO:Initializing Light Gradient Boosting Machine
2023-03-22 23:04:49,493:INFO:Total runtime is 50.555080223083486 minutes
2023-03-22 23:04:49,493:INFO:SubProcess create_model() called ==================================
2023-03-22 23:04:49,493:INFO:Initializing create_model()
2023-03-22 23:04:49,493:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 23:04:49,493:INFO:Checking exceptions
2023-03-22 23:04:49,493:INFO:Importing libraries
2023-03-22 23:04:49,493:INFO:Copying training dataset
2023-03-22 23:04:49,563:INFO:Defining folds
2023-03-22 23:04:49,563:INFO:Declaring metric variables
2023-03-22 23:04:49,568:INFO:Importing untrained model
2023-03-22 23:04:49,573:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-22 23:04:49,578:INFO:Starting cross validation
2023-03-22 23:04:49,578:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 23:05:10,613:INFO:Calculating mean and std
2023-03-22 23:05:10,613:INFO:Creating metrics dataframe
2023-03-22 23:05:12,130:INFO:Uploading results into container
2023-03-22 23:05:12,135:INFO:Uploading model into container now
2023-03-22 23:05:12,135:INFO:_master_model_container: 19
2023-03-22 23:05:12,135:INFO:_display_container: 2
2023-03-22 23:05:12,135:INFO:LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
              importance_type='split', learning_rate=0.1, max_depth=-1,
              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
              random_state=5595, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-03-22 23:05:12,135:INFO:create_model() successfully completed......................................
2023-03-22 23:05:12,268:INFO:SubProcess create_model() end ==================================
2023-03-22 23:05:12,268:INFO:Creating metrics dataframe
2023-03-22 23:05:12,283:INFO:Initializing CatBoost Regressor
2023-03-22 23:05:12,283:INFO:Total runtime is 50.934918975830065 minutes
2023-03-22 23:05:12,288:INFO:SubProcess create_model() called ==================================
2023-03-22 23:05:12,288:INFO:Initializing create_model()
2023-03-22 23:05:12,288:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 23:05:12,288:INFO:Checking exceptions
2023-03-22 23:05:12,288:INFO:Importing libraries
2023-03-22 23:05:12,288:INFO:Copying training dataset
2023-03-22 23:05:12,358:INFO:Defining folds
2023-03-22 23:05:12,358:INFO:Declaring metric variables
2023-03-22 23:05:12,363:INFO:Importing untrained model
2023-03-22 23:05:12,373:INFO:CatBoost Regressor Imported successfully
2023-03-22 23:05:12,378:INFO:Starting cross validation
2023-03-22 23:05:12,378:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 23:07:09,793:INFO:Calculating mean and std
2023-03-22 23:07:09,793:INFO:Creating metrics dataframe
2023-03-22 23:07:11,933:INFO:Uploading results into container
2023-03-22 23:07:11,933:INFO:Uploading model into container now
2023-03-22 23:07:11,933:INFO:_master_model_container: 20
2023-03-22 23:07:11,933:INFO:_display_container: 2
2023-03-22 23:07:11,933:INFO:<catboost.core.CatBoostRegressor object at 0x000001E227AC6400>
2023-03-22 23:07:11,933:INFO:create_model() successfully completed......................................
2023-03-22 23:07:12,068:INFO:SubProcess create_model() end ==================================
2023-03-22 23:07:12,068:INFO:Creating metrics dataframe
2023-03-22 23:07:12,088:INFO:Initializing Dummy Regressor
2023-03-22 23:07:12,088:INFO:Total runtime is 52.93166906436283 minutes
2023-03-22 23:07:12,093:INFO:SubProcess create_model() called ==================================
2023-03-22 23:07:12,093:INFO:Initializing create_model()
2023-03-22 23:07:12,093:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E21DBF3310>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 23:07:12,093:INFO:Checking exceptions
2023-03-22 23:07:12,093:INFO:Importing libraries
2023-03-22 23:07:12,093:INFO:Copying training dataset
2023-03-22 23:07:12,163:INFO:Defining folds
2023-03-22 23:07:12,163:INFO:Declaring metric variables
2023-03-22 23:07:12,163:INFO:Importing untrained model
2023-03-22 23:07:12,168:INFO:Dummy Regressor Imported successfully
2023-03-22 23:07:12,173:INFO:Starting cross validation
2023-03-22 23:07:12,178:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 23:07:27,333:INFO:Calculating mean and std
2023-03-22 23:07:27,338:INFO:Creating metrics dataframe
2023-03-22 23:07:28,863:INFO:Uploading results into container
2023-03-22 23:07:28,863:INFO:Uploading model into container now
2023-03-22 23:07:28,863:INFO:_master_model_container: 21
2023-03-22 23:07:28,863:INFO:_display_container: 2
2023-03-22 23:07:28,863:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2023-03-22 23:07:28,863:INFO:create_model() successfully completed......................................
2023-03-22 23:07:28,993:INFO:SubProcess create_model() end ==================================
2023-03-22 23:07:28,993:INFO:Creating metrics dataframe
2023-03-22 23:07:29,018:INFO:Initializing create_model()
2023-03-22 23:07:29,018:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, estimator=ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',
                    max_depth=None, max_features='auto', max_leaf_nodes=None,
                    max_samples=None, min_impurity_decrease=0.0,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,
                    oob_score=False, random_state=5595, verbose=0,
                    warm_start=False), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-22 23:07:29,018:INFO:Checking exceptions
2023-03-22 23:07:29,023:INFO:Importing libraries
2023-03-22 23:07:29,023:INFO:Copying training dataset
2023-03-22 23:07:29,088:INFO:Defining folds
2023-03-22 23:07:29,088:INFO:Declaring metric variables
2023-03-22 23:07:29,088:INFO:Importing untrained model
2023-03-22 23:07:29,088:INFO:Declaring custom model
2023-03-22 23:07:29,093:INFO:Extra Trees Regressor Imported successfully
2023-03-22 23:07:29,093:INFO:Cross validation set to False
2023-03-22 23:07:29,093:INFO:Fitting Model
2023-03-22 23:07:45,613:INFO:ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',
                    max_depth=None, max_features='auto', max_leaf_nodes=None,
                    max_samples=None, min_impurity_decrease=0.0,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,
                    oob_score=False, random_state=5595, verbose=0,
                    warm_start=False)
2023-03-22 23:07:45,613:INFO:create_model() successfully completed......................................
2023-03-22 23:07:45,773:INFO:_master_model_container: 21
2023-03-22 23:07:45,773:INFO:_display_container: 2
2023-03-22 23:07:45,773:INFO:ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',
                    max_depth=None, max_features='auto', max_leaf_nodes=None,
                    max_samples=None, min_impurity_decrease=0.0,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,
                    oob_score=False, random_state=5595, verbose=0,
                    warm_start=False)
2023-03-22 23:07:45,773:INFO:compare_models() successfully completed......................................
2023-03-22 23:07:45,791:INFO:Initializing plot_model()
2023-03-22 23:07:45,796:INFO:plot_model(plot=residuals, fold=None, use_train_data=False, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',
                    max_depth=None, max_features='auto', max_leaf_nodes=None,
                    max_samples=None, min_impurity_decrease=0.0,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,
                    oob_score=False, random_state=5595, verbose=0,
                    warm_start=False), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>, system=True)
2023-03-22 23:07:45,796:INFO:Checking exceptions
2023-03-22 23:07:45,841:INFO:Preloading libraries
2023-03-22 23:07:45,868:INFO:Copying training dataset
2023-03-22 23:07:45,868:INFO:Plot type: residuals
2023-03-22 23:07:46,468:INFO:Fitting Model
2023-03-22 23:07:46,468:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\base.py:450: UserWarning:

X does not have valid feature names, but ExtraTreesRegressor was fitted with feature names


2023-03-22 23:07:47,048:INFO:Scoring test/hold-out set
2023-03-22 23:07:48,871:INFO:Visual Rendered Successfully
2023-03-22 23:07:49,008:INFO:plot_model() successfully completed......................................
2023-03-22 23:07:49,028:INFO:Initializing tune_model()
2023-03-22 23:07:49,028:INFO:tune_model(estimator=ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',
                    max_depth=None, max_features='auto', max_leaf_nodes=None,
                    max_samples=None, min_impurity_decrease=0.0,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,
                    oob_score=False, random_state=5595, verbose=0,
                    warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=R2, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E21DC913D0>)
2023-03-22 23:07:49,028:INFO:Checking exceptions
2023-03-22 23:07:49,078:INFO:Copying training dataset
2023-03-22 23:07:49,120:INFO:Checking base model
2023-03-22 23:07:49,120:INFO:Base model : Extra Trees Regressor
2023-03-22 23:07:49,125:INFO:Declaring metric variables
2023-03-22 23:07:49,130:INFO:Defining Hyperparameters
2023-03-22 23:07:49,259:INFO:Tuning with n_jobs=-1
2023-03-22 23:07:49,259:INFO:Initializing RandomizedSearchCV
2023-03-31 19:40:05,421:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 19:40:05,421:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 19:40:05,421:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 19:40:05,421:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 19:40:06,429:INFO:Soft dependency imported: prophet: 1.1.2
2023-03-31 19:43:39,812:INFO:PyCaret ClassificationExperiment
2023-03-31 19:43:39,812:INFO:Logging name: clf-default-name
2023-03-31 19:43:39,812:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-31 19:43:39,812:INFO:version 3.0.0
2023-03-31 19:43:39,812:INFO:Initializing setup()
2023-03-31 19:43:39,812:INFO:self.USI: 6d04
2023-03-31 19:43:39,812:INFO:self._variable_keys: {'y_train', 'USI', 'fold_shuffle_param', 'is_multiclass', 'gpu_param', 'idx', 'target_param', 'log_plots_param', 'X', 'n_jobs_param', 'exp_name_log', 'html_param', '_ml_usecase', '_available_plots', 'memory', 'exp_id', 'fold_generator', 'X_train', 'logging_param', 'gpu_n_jobs_param', 'y', 'y_test', 'fold_groups_param', 'seed', 'fix_imbalance', 'X_test', 'pipeline', 'data'}
2023-03-31 19:43:39,812:INFO:Checking environment
2023-03-31 19:43:39,812:INFO:python_version: 3.9.12
2023-03-31 19:43:39,812:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-31 19:43:39,812:INFO:machine: AMD64
2023-03-31 19:43:39,812:INFO:platform: Windows-10-10.0.22624-SP0
2023-03-31 19:43:39,812:INFO:Memory: svmem(total=8378363904, available=1337782272, percent=84.0, used=7040581632, free=1337782272)
2023-03-31 19:43:39,812:INFO:Physical Core: 4
2023-03-31 19:43:39,812:INFO:Logical Core: 8
2023-03-31 19:43:39,812:INFO:Checking libraries
2023-03-31 19:43:39,812:INFO:System:
2023-03-31 19:43:39,812:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-31 19:43:39,812:INFO:executable: C:\Users\Mujahid\anaconda3\python.exe
2023-03-31 19:43:39,812:INFO:   machine: Windows-10-10.0.22624-SP0
2023-03-31 19:43:39,812:INFO:PyCaret required dependencies:
2023-03-31 19:43:39,812:INFO:                 pip: 23.0.1
2023-03-31 19:43:39,812:INFO:          setuptools: 61.2.0
2023-03-31 19:43:39,812:INFO:             pycaret: 3.0.0
2023-03-31 19:43:39,812:INFO:             IPython: 8.2.0
2023-03-31 19:43:39,812:INFO:          ipywidgets: 7.6.5
2023-03-31 19:43:39,812:INFO:                tqdm: 4.64.0
2023-03-31 19:43:39,812:INFO:               numpy: 1.21.5
2023-03-31 19:43:39,812:INFO:              pandas: 1.4.2
2023-03-31 19:43:39,812:INFO:              jinja2: 3.1.2
2023-03-31 19:43:39,812:INFO:               scipy: 1.7.3
2023-03-31 19:43:39,812:INFO:              joblib: 1.2.0
2023-03-31 19:43:39,812:INFO:             sklearn: 1.0.2
2023-03-31 19:43:39,812:INFO:                pyod: 1.0.8
2023-03-31 19:43:39,812:INFO:            imblearn: 0.10.1
2023-03-31 19:43:39,812:INFO:   category_encoders: 2.6.0
2023-03-31 19:43:39,812:INFO:            lightgbm: 3.3.5
2023-03-31 19:43:39,812:INFO:               numba: 0.55.1
2023-03-31 19:43:39,812:INFO:            requests: 2.27.1
2023-03-31 19:43:39,812:INFO:          matplotlib: 3.5.1
2023-03-31 19:43:39,812:INFO:          scikitplot: 0.3.7
2023-03-31 19:43:39,812:INFO:         yellowbrick: 1.5
2023-03-31 19:43:39,812:INFO:              plotly: 5.6.0
2023-03-31 19:43:39,812:INFO:             kaleido: 0.2.1
2023-03-31 19:43:39,812:INFO:         statsmodels: 0.13.2
2023-03-31 19:43:39,812:INFO:              sktime: 0.16.1
2023-03-31 19:43:39,812:INFO:               tbats: 1.1.2
2023-03-31 19:43:39,812:INFO:            pmdarima: 2.0.3
2023-03-31 19:43:39,812:INFO:              psutil: 5.9.4
2023-03-31 19:43:39,812:INFO:PyCaret optional dependencies:
2023-03-31 19:43:40,079:INFO:                shap: 0.41.0
2023-03-31 19:43:40,079:INFO:           interpret: Not installed
2023-03-31 19:43:40,079:INFO:                umap: 0.5.3
2023-03-31 19:43:40,079:INFO:    pandas_profiling: 4.0.0
2023-03-31 19:43:40,079:INFO:  explainerdashboard: Not installed
2023-03-31 19:43:40,079:INFO:             autoviz: Not installed
2023-03-31 19:43:40,079:INFO:           fairlearn: Not installed
2023-03-31 19:43:40,079:INFO:             xgboost: 1.7.4
2023-03-31 19:43:40,079:INFO:            catboost: 1.1.1
2023-03-31 19:43:40,079:INFO:              kmodes: 0.12.2
2023-03-31 19:43:40,079:INFO:             mlxtend: 0.21.0
2023-03-31 19:43:40,079:INFO:       statsforecast: Not installed
2023-03-31 19:43:40,079:INFO:        tune_sklearn: Not installed
2023-03-31 19:43:40,079:INFO:                 ray: Not installed
2023-03-31 19:43:40,079:INFO:            hyperopt: Not installed
2023-03-31 19:43:40,079:INFO:              optuna: 3.1.0
2023-03-31 19:43:40,079:INFO:               skopt: Not installed
2023-03-31 19:43:40,079:INFO:              mlflow: 2.2.2
2023-03-31 19:43:40,079:INFO:              gradio: Not installed
2023-03-31 19:43:40,079:INFO:             fastapi: 0.95.0
2023-03-31 19:43:40,079:INFO:             uvicorn: 0.21.1
2023-03-31 19:43:40,079:INFO:              m2cgen: Not installed
2023-03-31 19:43:40,079:INFO:           evidently: Not installed
2023-03-31 19:43:40,079:INFO:               fugue: Not installed
2023-03-31 19:43:40,079:INFO:           streamlit: 1.20.0
2023-03-31 19:43:40,079:INFO:             prophet: 1.1.2
2023-03-31 19:43:40,079:INFO:None
2023-03-31 19:43:40,079:INFO:Set up data.
2023-03-31 19:43:40,220:INFO:Set up train/test split.
2023-03-31 19:43:40,370:INFO:Set up index.
2023-03-31 19:43:40,386:INFO:Set up folding strategy.
2023-03-31 19:43:40,386:INFO:Assigning column types.
2023-03-31 19:43:40,478:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-31 19:43:40,509:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-31 19:43:40,525:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 19:43:40,559:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 19:43:40,714:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 19:43:40,853:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-31 19:43:40,853:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 19:43:40,893:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 19:43:40,893:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 19:43:40,893:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-31 19:43:40,939:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 19:43:40,965:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 19:43:40,965:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 19:43:41,005:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 19:43:41,029:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 19:43:41,029:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 19:43:41,029:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-03-31 19:43:41,092:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 19:43:41,107:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 19:43:41,179:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 19:43:41,179:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 19:43:41,195:INFO:Preparing preprocessing pipeline...
2023-03-31 19:43:41,203:INFO:Set up simple imputation.
2023-03-31 19:43:41,203:INFO:Set up imbalanced handling.
2023-03-31 19:43:41,218:INFO:Set up column name cleaning.
2023-03-31 19:43:42,380:INFO:Finished creating preprocessing pipeline.
2023-03-31 19:43:42,380:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Mujahid\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Type', 'Benefit per order',
                                             'Sales per customer',
                                             'Delivery Status',
                                             'Late_delivery_risk',
                                             'Category Name', 'Customer City',
                                             'Customer Country', 'Customer Id',
                                             'Customer Segment',
                                             'Customer State',
                                             'Department Name', 'Latitu...
                                                              strategy='most_frequent',
                                                              verbose=0))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-03-31 19:43:42,380:INFO:Creating final display dataframe.
2023-03-31 19:43:44,169:INFO:Setup _display_container:                     Description             Value
0                    Session id              4794
1                        Target   SUSPECTED_FRAUD
2                   Target type            Binary
3           Original data shape      (180519, 43)
4        Transformed data shape      (301196, 43)
5   Transformed train set shape      (247040, 43)
6    Transformed test set shape       (54156, 43)
7              Numeric features                42
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                Fix imbalance              True
13         Fix imbalance method             SMOTE
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              6d04
2023-03-31 19:43:44,258:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 19:43:44,263:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 19:43:44,332:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 19:43:44,332:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 19:43:44,332:INFO:setup() successfully completed in 6.89s...............
2023-03-31 19:44:33,788:INFO:Initializing compare_models()
2023-03-31 19:44:33,788:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE54F60910>, include=None, fold=5, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001EE54F60910>, 'include': None, 'exclude': None, 'fold': 5, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-03-31 19:44:33,788:INFO:Checking exceptions
2023-03-31 19:44:33,866:INFO:Preparing display monitor
2023-03-31 19:44:33,906:INFO:Initializing Logistic Regression
2023-03-31 19:44:33,906:INFO:Total runtime is 0.0 minutes
2023-03-31 19:44:33,910:INFO:SubProcess create_model() called ==================================
2023-03-31 19:44:33,911:INFO:Initializing create_model()
2023-03-31 19:44:33,911:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE54F60910>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3C010D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 19:44:33,911:INFO:Checking exceptions
2023-03-31 19:44:33,911:INFO:Importing libraries
2023-03-31 19:44:33,911:INFO:Copying training dataset
2023-03-31 19:44:34,041:INFO:Defining folds
2023-03-31 19:44:34,041:INFO:Declaring metric variables
2023-03-31 19:44:34,044:INFO:Importing untrained model
2023-03-31 19:44:34,047:INFO:Logistic Regression Imported successfully
2023-03-31 19:44:34,053:INFO:Starting cross validation
2023-03-31 19:44:34,055:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 19:45:31,196:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 19:45:31,259:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 19:45:31,291:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 19:45:31,322:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 19:45:31,433:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 19:45:35,705:INFO:Calculating mean and std
2023-03-31 19:45:35,705:INFO:Creating metrics dataframe
2023-03-31 19:45:36,670:INFO:Uploading results into container
2023-03-31 19:45:36,670:INFO:Uploading model into container now
2023-03-31 19:45:36,670:INFO:_master_model_container: 1
2023-03-31 19:45:36,670:INFO:_display_container: 2
2023-03-31 19:45:36,670:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=4794, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-03-31 19:45:36,670:INFO:create_model() successfully completed......................................
2023-03-31 19:45:42,836:INFO:SubProcess create_model() end ==================================
2023-03-31 19:45:42,836:INFO:Creating metrics dataframe
2023-03-31 19:45:42,851:INFO:Initializing K Neighbors Classifier
2023-03-31 19:45:42,851:INFO:Total runtime is 1.149083129564921 minutes
2023-03-31 19:45:42,866:INFO:SubProcess create_model() called ==================================
2023-03-31 19:45:42,866:INFO:Initializing create_model()
2023-03-31 19:45:42,866:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE54F60910>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3C010D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 19:45:42,866:INFO:Checking exceptions
2023-03-31 19:45:42,866:INFO:Importing libraries
2023-03-31 19:45:42,866:INFO:Copying training dataset
2023-03-31 19:45:42,978:INFO:Defining folds
2023-03-31 19:45:42,978:INFO:Declaring metric variables
2023-03-31 19:45:42,991:INFO:Importing untrained model
2023-03-31 19:45:42,996:INFO:K Neighbors Classifier Imported successfully
2023-03-31 19:45:43,006:INFO:Starting cross validation
2023-03-31 19:45:43,008:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 19:45:56,578:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 20:09:04,198:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 20:09:04,198:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 20:09:04,198:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 20:09:04,198:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 20:09:05,147:INFO:Soft dependency imported: prophet: 1.1.2
2023-03-31 20:09:08,513:INFO:PyCaret ClassificationExperiment
2023-03-31 20:09:08,513:INFO:Logging name: clf-default-name
2023-03-31 20:09:08,513:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-31 20:09:08,513:INFO:version 3.0.0
2023-03-31 20:09:08,513:INFO:Initializing setup()
2023-03-31 20:09:08,513:INFO:self.USI: 126e
2023-03-31 20:09:08,513:INFO:self._variable_keys: {'seed', '_ml_usecase', 'html_param', 'y_train', 'X_test', 'fix_imbalance', 'fold_groups_param', 'target_param', 'data', 'log_plots_param', 'X_train', 'memory', 'USI', 'n_jobs_param', 'logging_param', 'exp_id', 'exp_name_log', 'is_multiclass', 'pipeline', 'idx', 'y', 'X', 'fold_shuffle_param', 'fold_generator', 'gpu_param', 'gpu_n_jobs_param', '_available_plots', 'y_test'}
2023-03-31 20:09:08,513:INFO:Checking environment
2023-03-31 20:09:08,513:INFO:python_version: 3.9.12
2023-03-31 20:09:08,513:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-31 20:09:08,513:INFO:machine: AMD64
2023-03-31 20:09:08,513:INFO:platform: Windows-10-10.0.22624-SP0
2023-03-31 20:09:08,513:INFO:Memory: svmem(total=8378363904, available=501391360, percent=94.0, used=7876972544, free=501391360)
2023-03-31 20:09:08,513:INFO:Physical Core: 4
2023-03-31 20:09:08,513:INFO:Logical Core: 8
2023-03-31 20:09:08,513:INFO:Checking libraries
2023-03-31 20:09:08,513:INFO:System:
2023-03-31 20:09:08,513:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-31 20:09:08,513:INFO:executable: C:\Users\Mujahid\anaconda3\python.exe
2023-03-31 20:09:08,513:INFO:   machine: Windows-10-10.0.22624-SP0
2023-03-31 20:09:08,513:INFO:PyCaret required dependencies:
2023-03-31 20:09:08,513:INFO:                 pip: 23.0.1
2023-03-31 20:09:08,513:INFO:          setuptools: 61.2.0
2023-03-31 20:09:08,513:INFO:             pycaret: 3.0.0
2023-03-31 20:09:08,513:INFO:             IPython: 8.2.0
2023-03-31 20:09:08,513:INFO:          ipywidgets: 7.6.5
2023-03-31 20:09:08,513:INFO:                tqdm: 4.64.0
2023-03-31 20:09:08,513:INFO:               numpy: 1.21.5
2023-03-31 20:09:08,513:INFO:              pandas: 1.4.2
2023-03-31 20:09:08,513:INFO:              jinja2: 3.1.2
2023-03-31 20:09:08,513:INFO:               scipy: 1.7.3
2023-03-31 20:09:08,513:INFO:              joblib: 1.2.0
2023-03-31 20:09:08,513:INFO:             sklearn: 1.0.2
2023-03-31 20:09:08,513:INFO:                pyod: 1.0.8
2023-03-31 20:09:08,513:INFO:            imblearn: 0.10.1
2023-03-31 20:09:08,513:INFO:   category_encoders: 2.6.0
2023-03-31 20:09:08,513:INFO:            lightgbm: 3.3.5
2023-03-31 20:09:08,513:INFO:               numba: 0.55.1
2023-03-31 20:09:08,513:INFO:            requests: 2.27.1
2023-03-31 20:09:08,513:INFO:          matplotlib: 3.5.1
2023-03-31 20:09:08,513:INFO:          scikitplot: 0.3.7
2023-03-31 20:09:08,513:INFO:         yellowbrick: 1.5
2023-03-31 20:09:08,513:INFO:              plotly: 5.6.0
2023-03-31 20:09:08,513:INFO:             kaleido: 0.2.1
2023-03-31 20:09:08,513:INFO:         statsmodels: 0.13.2
2023-03-31 20:09:08,513:INFO:              sktime: 0.16.1
2023-03-31 20:09:08,513:INFO:               tbats: 1.1.2
2023-03-31 20:09:08,513:INFO:            pmdarima: 2.0.3
2023-03-31 20:09:08,513:INFO:              psutil: 5.9.4
2023-03-31 20:09:08,513:INFO:PyCaret optional dependencies:
2023-03-31 20:09:08,701:INFO:                shap: 0.41.0
2023-03-31 20:09:08,701:INFO:           interpret: Not installed
2023-03-31 20:09:08,701:INFO:                umap: 0.5.3
2023-03-31 20:09:08,701:INFO:    pandas_profiling: 4.0.0
2023-03-31 20:09:08,701:INFO:  explainerdashboard: Not installed
2023-03-31 20:09:08,701:INFO:             autoviz: Not installed
2023-03-31 20:09:08,701:INFO:           fairlearn: Not installed
2023-03-31 20:09:08,701:INFO:             xgboost: 1.7.4
2023-03-31 20:09:08,701:INFO:            catboost: 1.1.1
2023-03-31 20:09:08,701:INFO:              kmodes: 0.12.2
2023-03-31 20:09:08,701:INFO:             mlxtend: 0.21.0
2023-03-31 20:09:08,701:INFO:       statsforecast: Not installed
2023-03-31 20:09:08,701:INFO:        tune_sklearn: Not installed
2023-03-31 20:09:08,701:INFO:                 ray: Not installed
2023-03-31 20:09:08,701:INFO:            hyperopt: Not installed
2023-03-31 20:09:08,701:INFO:              optuna: 3.1.0
2023-03-31 20:09:08,701:INFO:               skopt: Not installed
2023-03-31 20:09:08,701:INFO:              mlflow: 2.2.2
2023-03-31 20:09:08,701:INFO:              gradio: Not installed
2023-03-31 20:09:08,701:INFO:             fastapi: 0.95.0
2023-03-31 20:09:08,701:INFO:             uvicorn: 0.21.1
2023-03-31 20:09:08,701:INFO:              m2cgen: Not installed
2023-03-31 20:09:08,701:INFO:           evidently: Not installed
2023-03-31 20:09:08,701:INFO:               fugue: Not installed
2023-03-31 20:09:08,701:INFO:           streamlit: 1.20.0
2023-03-31 20:09:08,701:INFO:             prophet: 1.1.2
2023-03-31 20:09:08,701:INFO:None
2023-03-31 20:09:08,701:INFO:Set up data.
2023-03-31 20:15:29,189:INFO:PyCaret ClassificationExperiment
2023-03-31 20:15:29,189:INFO:Logging name: clf-default-name
2023-03-31 20:15:29,189:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-31 20:15:29,189:INFO:version 3.0.0
2023-03-31 20:15:29,189:INFO:Initializing setup()
2023-03-31 20:15:29,189:INFO:self.USI: a46a
2023-03-31 20:15:29,189:INFO:self._variable_keys: {'seed', '_ml_usecase', 'html_param', 'y_train', 'X_test', 'fix_imbalance', 'fold_groups_param', 'target_param', 'data', 'log_plots_param', 'X_train', 'memory', 'USI', 'n_jobs_param', 'logging_param', 'exp_id', 'exp_name_log', 'is_multiclass', 'pipeline', 'idx', 'y', 'X', 'fold_shuffle_param', 'fold_generator', 'gpu_param', 'gpu_n_jobs_param', '_available_plots', 'y_test'}
2023-03-31 20:15:29,189:INFO:Checking environment
2023-03-31 20:15:29,189:INFO:python_version: 3.9.12
2023-03-31 20:15:29,189:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-31 20:15:29,189:INFO:machine: AMD64
2023-03-31 20:15:29,189:INFO:platform: Windows-10-10.0.22624-SP0
2023-03-31 20:15:29,189:INFO:Memory: svmem(total=8378363904, available=1540395008, percent=81.6, used=6837968896, free=1540395008)
2023-03-31 20:15:29,189:INFO:Physical Core: 4
2023-03-31 20:15:29,189:INFO:Logical Core: 8
2023-03-31 20:15:29,189:INFO:Checking libraries
2023-03-31 20:15:29,189:INFO:System:
2023-03-31 20:15:29,189:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-31 20:15:29,189:INFO:executable: C:\Users\Mujahid\anaconda3\python.exe
2023-03-31 20:15:29,189:INFO:   machine: Windows-10-10.0.22624-SP0
2023-03-31 20:15:29,189:INFO:PyCaret required dependencies:
2023-03-31 20:15:29,189:INFO:                 pip: 23.0.1
2023-03-31 20:15:29,189:INFO:          setuptools: 61.2.0
2023-03-31 20:15:29,189:INFO:             pycaret: 3.0.0
2023-03-31 20:15:29,189:INFO:             IPython: 8.2.0
2023-03-31 20:15:29,189:INFO:          ipywidgets: 7.6.5
2023-03-31 20:15:29,189:INFO:                tqdm: 4.64.0
2023-03-31 20:15:29,189:INFO:               numpy: 1.21.5
2023-03-31 20:15:29,189:INFO:              pandas: 1.4.2
2023-03-31 20:15:29,189:INFO:              jinja2: 3.1.2
2023-03-31 20:15:29,189:INFO:               scipy: 1.7.3
2023-03-31 20:15:29,189:INFO:              joblib: 1.2.0
2023-03-31 20:15:29,189:INFO:             sklearn: 1.0.2
2023-03-31 20:15:29,189:INFO:                pyod: 1.0.8
2023-03-31 20:15:29,189:INFO:            imblearn: 0.10.1
2023-03-31 20:15:29,189:INFO:   category_encoders: 2.6.0
2023-03-31 20:15:29,189:INFO:            lightgbm: 3.3.5
2023-03-31 20:15:29,189:INFO:               numba: 0.55.1
2023-03-31 20:15:29,189:INFO:            requests: 2.27.1
2023-03-31 20:15:29,189:INFO:          matplotlib: 3.5.1
2023-03-31 20:15:29,189:INFO:          scikitplot: 0.3.7
2023-03-31 20:15:29,189:INFO:         yellowbrick: 1.5
2023-03-31 20:15:29,189:INFO:              plotly: 5.6.0
2023-03-31 20:15:29,189:INFO:             kaleido: 0.2.1
2023-03-31 20:15:29,189:INFO:         statsmodels: 0.13.2
2023-03-31 20:15:29,189:INFO:              sktime: 0.16.1
2023-03-31 20:15:29,189:INFO:               tbats: 1.1.2
2023-03-31 20:15:29,189:INFO:            pmdarima: 2.0.3
2023-03-31 20:15:29,189:INFO:              psutil: 5.9.4
2023-03-31 20:15:29,189:INFO:PyCaret optional dependencies:
2023-03-31 20:15:29,189:INFO:                shap: 0.41.0
2023-03-31 20:15:29,189:INFO:           interpret: Not installed
2023-03-31 20:15:29,189:INFO:                umap: 0.5.3
2023-03-31 20:15:29,189:INFO:    pandas_profiling: 4.0.0
2023-03-31 20:15:29,189:INFO:  explainerdashboard: Not installed
2023-03-31 20:15:29,189:INFO:             autoviz: Not installed
2023-03-31 20:15:29,189:INFO:           fairlearn: Not installed
2023-03-31 20:15:29,189:INFO:             xgboost: 1.7.4
2023-03-31 20:15:29,189:INFO:            catboost: 1.1.1
2023-03-31 20:15:29,189:INFO:              kmodes: 0.12.2
2023-03-31 20:15:29,189:INFO:             mlxtend: 0.21.0
2023-03-31 20:15:29,189:INFO:       statsforecast: Not installed
2023-03-31 20:15:29,189:INFO:        tune_sklearn: Not installed
2023-03-31 20:15:29,189:INFO:                 ray: Not installed
2023-03-31 20:15:29,189:INFO:            hyperopt: Not installed
2023-03-31 20:15:29,189:INFO:              optuna: 3.1.0
2023-03-31 20:15:29,189:INFO:               skopt: Not installed
2023-03-31 20:15:29,189:INFO:              mlflow: 2.2.2
2023-03-31 20:15:29,189:INFO:              gradio: Not installed
2023-03-31 20:15:29,189:INFO:             fastapi: 0.95.0
2023-03-31 20:15:29,189:INFO:             uvicorn: 0.21.1
2023-03-31 20:15:29,189:INFO:              m2cgen: Not installed
2023-03-31 20:15:29,189:INFO:           evidently: Not installed
2023-03-31 20:15:29,189:INFO:               fugue: Not installed
2023-03-31 20:15:29,189:INFO:           streamlit: 1.20.0
2023-03-31 20:15:29,189:INFO:             prophet: 1.1.2
2023-03-31 20:15:29,189:INFO:None
2023-03-31 20:15:29,189:INFO:Set up data.
2023-03-31 20:15:50,089:INFO:PyCaret ClassificationExperiment
2023-03-31 20:15:50,089:INFO:Logging name: clf-default-name
2023-03-31 20:15:50,089:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-31 20:15:50,089:INFO:version 3.0.0
2023-03-31 20:15:50,089:INFO:Initializing setup()
2023-03-31 20:15:50,089:INFO:self.USI: f4f7
2023-03-31 20:15:50,089:INFO:self._variable_keys: {'seed', '_ml_usecase', 'html_param', 'y_train', 'X_test', 'fix_imbalance', 'fold_groups_param', 'target_param', 'data', 'log_plots_param', 'X_train', 'memory', 'USI', 'n_jobs_param', 'logging_param', 'exp_id', 'exp_name_log', 'is_multiclass', 'pipeline', 'idx', 'y', 'X', 'fold_shuffle_param', 'fold_generator', 'gpu_param', 'gpu_n_jobs_param', '_available_plots', 'y_test'}
2023-03-31 20:15:50,089:INFO:Checking environment
2023-03-31 20:15:50,089:INFO:python_version: 3.9.12
2023-03-31 20:15:50,089:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-31 20:15:50,089:INFO:machine: AMD64
2023-03-31 20:15:50,089:INFO:platform: Windows-10-10.0.22624-SP0
2023-03-31 20:15:50,089:INFO:Memory: svmem(total=8378363904, available=1515855872, percent=81.9, used=6862508032, free=1515855872)
2023-03-31 20:15:50,089:INFO:Physical Core: 4
2023-03-31 20:15:50,089:INFO:Logical Core: 8
2023-03-31 20:15:50,089:INFO:Checking libraries
2023-03-31 20:15:50,089:INFO:System:
2023-03-31 20:15:50,089:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-31 20:15:50,089:INFO:executable: C:\Users\Mujahid\anaconda3\python.exe
2023-03-31 20:15:50,089:INFO:   machine: Windows-10-10.0.22624-SP0
2023-03-31 20:15:50,089:INFO:PyCaret required dependencies:
2023-03-31 20:15:50,089:INFO:                 pip: 23.0.1
2023-03-31 20:15:50,089:INFO:          setuptools: 61.2.0
2023-03-31 20:15:50,089:INFO:             pycaret: 3.0.0
2023-03-31 20:15:50,089:INFO:             IPython: 8.2.0
2023-03-31 20:15:50,089:INFO:          ipywidgets: 7.6.5
2023-03-31 20:15:50,089:INFO:                tqdm: 4.64.0
2023-03-31 20:15:50,089:INFO:               numpy: 1.21.5
2023-03-31 20:15:50,089:INFO:              pandas: 1.4.2
2023-03-31 20:15:50,089:INFO:              jinja2: 3.1.2
2023-03-31 20:15:50,089:INFO:               scipy: 1.7.3
2023-03-31 20:15:50,089:INFO:              joblib: 1.2.0
2023-03-31 20:15:50,089:INFO:             sklearn: 1.0.2
2023-03-31 20:15:50,089:INFO:                pyod: 1.0.8
2023-03-31 20:15:50,089:INFO:            imblearn: 0.10.1
2023-03-31 20:15:50,089:INFO:   category_encoders: 2.6.0
2023-03-31 20:15:50,089:INFO:            lightgbm: 3.3.5
2023-03-31 20:15:50,089:INFO:               numba: 0.55.1
2023-03-31 20:15:50,089:INFO:            requests: 2.27.1
2023-03-31 20:15:50,089:INFO:          matplotlib: 3.5.1
2023-03-31 20:15:50,089:INFO:          scikitplot: 0.3.7
2023-03-31 20:15:50,089:INFO:         yellowbrick: 1.5
2023-03-31 20:15:50,089:INFO:              plotly: 5.6.0
2023-03-31 20:15:50,089:INFO:             kaleido: 0.2.1
2023-03-31 20:15:50,089:INFO:         statsmodels: 0.13.2
2023-03-31 20:15:50,089:INFO:              sktime: 0.16.1
2023-03-31 20:15:50,089:INFO:               tbats: 1.1.2
2023-03-31 20:15:50,089:INFO:            pmdarima: 2.0.3
2023-03-31 20:15:50,089:INFO:              psutil: 5.9.4
2023-03-31 20:15:50,089:INFO:PyCaret optional dependencies:
2023-03-31 20:15:50,089:INFO:                shap: 0.41.0
2023-03-31 20:15:50,089:INFO:           interpret: Not installed
2023-03-31 20:15:50,089:INFO:                umap: 0.5.3
2023-03-31 20:15:50,089:INFO:    pandas_profiling: 4.0.0
2023-03-31 20:15:50,089:INFO:  explainerdashboard: Not installed
2023-03-31 20:15:50,105:INFO:             autoviz: Not installed
2023-03-31 20:15:50,105:INFO:           fairlearn: Not installed
2023-03-31 20:15:50,105:INFO:             xgboost: 1.7.4
2023-03-31 20:15:50,105:INFO:            catboost: 1.1.1
2023-03-31 20:15:50,105:INFO:              kmodes: 0.12.2
2023-03-31 20:15:50,105:INFO:             mlxtend: 0.21.0
2023-03-31 20:15:50,105:INFO:       statsforecast: Not installed
2023-03-31 20:15:50,105:INFO:        tune_sklearn: Not installed
2023-03-31 20:15:50,105:INFO:                 ray: Not installed
2023-03-31 20:15:50,105:INFO:            hyperopt: Not installed
2023-03-31 20:15:50,105:INFO:              optuna: 3.1.0
2023-03-31 20:15:50,105:INFO:               skopt: Not installed
2023-03-31 20:15:50,105:INFO:              mlflow: 2.2.2
2023-03-31 20:15:50,105:INFO:              gradio: Not installed
2023-03-31 20:15:50,105:INFO:             fastapi: 0.95.0
2023-03-31 20:15:50,105:INFO:             uvicorn: 0.21.1
2023-03-31 20:15:50,105:INFO:              m2cgen: Not installed
2023-03-31 20:15:50,105:INFO:           evidently: Not installed
2023-03-31 20:15:50,105:INFO:               fugue: Not installed
2023-03-31 20:15:50,105:INFO:           streamlit: 1.20.0
2023-03-31 20:15:50,105:INFO:             prophet: 1.1.2
2023-03-31 20:15:50,105:INFO:None
2023-03-31 20:15:50,105:INFO:Set up data.
2023-03-31 20:25:19,000:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 20:25:19,000:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 20:25:19,000:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 20:25:19,000:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 20:25:20,573:INFO:Soft dependency imported: prophet: 1.1.2
2023-03-31 20:25:25,434:INFO:PyCaret ClassificationExperiment
2023-03-31 20:25:25,434:INFO:Logging name: clf-default-name
2023-03-31 20:25:25,434:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-31 20:25:25,434:INFO:version 3.0.0
2023-03-31 20:25:25,434:INFO:Initializing setup()
2023-03-31 20:25:25,434:INFO:self.USI: ae2f
2023-03-31 20:25:25,434:INFO:self._variable_keys: {'html_param', 'log_plots_param', 'y_test', 'USI', 'is_multiclass', 'pipeline', 'X_test', 'data', 'X_train', '_ml_usecase', 'idx', 'target_param', 'logging_param', 'seed', '_available_plots', 'fold_generator', 'n_jobs_param', 'gpu_param', 'X', 'y', 'gpu_n_jobs_param', 'exp_name_log', 'exp_id', 'fold_shuffle_param', 'y_train', 'memory', 'fix_imbalance', 'fold_groups_param'}
2023-03-31 20:25:25,434:INFO:Checking environment
2023-03-31 20:25:25,434:INFO:python_version: 3.9.12
2023-03-31 20:25:25,434:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-31 20:25:25,434:INFO:machine: AMD64
2023-03-31 20:25:25,434:INFO:platform: Windows-10-10.0.22624-SP0
2023-03-31 20:25:25,434:INFO:Memory: svmem(total=8378363904, available=4422586368, percent=47.2, used=3955777536, free=4422586368)
2023-03-31 20:25:25,434:INFO:Physical Core: 4
2023-03-31 20:25:25,434:INFO:Logical Core: 8
2023-03-31 20:25:25,434:INFO:Checking libraries
2023-03-31 20:25:25,434:INFO:System:
2023-03-31 20:25:25,434:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-31 20:25:25,434:INFO:executable: C:\Users\Mujahid\anaconda3\python.exe
2023-03-31 20:25:25,434:INFO:   machine: Windows-10-10.0.22624-SP0
2023-03-31 20:25:25,434:INFO:PyCaret required dependencies:
2023-03-31 20:25:25,434:INFO:                 pip: 23.0.1
2023-03-31 20:25:25,434:INFO:          setuptools: 61.2.0
2023-03-31 20:25:25,434:INFO:             pycaret: 3.0.0
2023-03-31 20:25:25,434:INFO:             IPython: 8.2.0
2023-03-31 20:25:25,434:INFO:          ipywidgets: 7.6.5
2023-03-31 20:25:25,434:INFO:                tqdm: 4.64.0
2023-03-31 20:25:25,434:INFO:               numpy: 1.21.5
2023-03-31 20:25:25,434:INFO:              pandas: 1.4.2
2023-03-31 20:25:25,434:INFO:              jinja2: 3.1.2
2023-03-31 20:25:25,434:INFO:               scipy: 1.7.3
2023-03-31 20:25:25,434:INFO:              joblib: 1.2.0
2023-03-31 20:25:25,434:INFO:             sklearn: 1.0.2
2023-03-31 20:25:25,434:INFO:                pyod: 1.0.8
2023-03-31 20:25:25,434:INFO:            imblearn: 0.10.1
2023-03-31 20:25:25,434:INFO:   category_encoders: 2.6.0
2023-03-31 20:25:25,434:INFO:            lightgbm: 3.3.5
2023-03-31 20:25:25,434:INFO:               numba: 0.55.1
2023-03-31 20:25:25,434:INFO:            requests: 2.27.1
2023-03-31 20:25:25,434:INFO:          matplotlib: 3.5.1
2023-03-31 20:25:25,434:INFO:          scikitplot: 0.3.7
2023-03-31 20:25:25,434:INFO:         yellowbrick: 1.5
2023-03-31 20:25:25,434:INFO:              plotly: 5.6.0
2023-03-31 20:25:25,434:INFO:             kaleido: 0.2.1
2023-03-31 20:25:25,434:INFO:         statsmodels: 0.13.2
2023-03-31 20:25:25,434:INFO:              sktime: 0.16.1
2023-03-31 20:25:25,434:INFO:               tbats: 1.1.2
2023-03-31 20:25:25,434:INFO:            pmdarima: 2.0.3
2023-03-31 20:25:25,434:INFO:              psutil: 5.9.4
2023-03-31 20:25:25,434:INFO:PyCaret optional dependencies:
2023-03-31 20:25:25,687:INFO:                shap: 0.41.0
2023-03-31 20:25:25,687:INFO:           interpret: Not installed
2023-03-31 20:25:25,687:INFO:                umap: 0.5.3
2023-03-31 20:25:25,687:INFO:    pandas_profiling: 4.0.0
2023-03-31 20:25:25,687:INFO:  explainerdashboard: Not installed
2023-03-31 20:25:25,687:INFO:             autoviz: Not installed
2023-03-31 20:25:25,687:INFO:           fairlearn: Not installed
2023-03-31 20:25:25,687:INFO:             xgboost: 1.7.4
2023-03-31 20:25:25,687:INFO:            catboost: 1.1.1
2023-03-31 20:25:25,687:INFO:              kmodes: 0.12.2
2023-03-31 20:25:25,687:INFO:             mlxtend: 0.21.0
2023-03-31 20:25:25,687:INFO:       statsforecast: Not installed
2023-03-31 20:25:25,687:INFO:        tune_sklearn: Not installed
2023-03-31 20:25:25,687:INFO:                 ray: Not installed
2023-03-31 20:25:25,687:INFO:            hyperopt: Not installed
2023-03-31 20:25:25,687:INFO:              optuna: 3.1.0
2023-03-31 20:25:25,687:INFO:               skopt: Not installed
2023-03-31 20:25:25,687:INFO:              mlflow: 2.2.2
2023-03-31 20:25:25,687:INFO:              gradio: Not installed
2023-03-31 20:25:25,687:INFO:             fastapi: 0.95.0
2023-03-31 20:25:25,687:INFO:             uvicorn: 0.21.1
2023-03-31 20:25:25,687:INFO:              m2cgen: Not installed
2023-03-31 20:25:25,687:INFO:           evidently: Not installed
2023-03-31 20:25:25,687:INFO:               fugue: Not installed
2023-03-31 20:25:25,687:INFO:           streamlit: 1.20.0
2023-03-31 20:25:25,687:INFO:             prophet: 1.1.2
2023-03-31 20:25:25,687:INFO:None
2023-03-31 20:25:25,687:INFO:Set up data.
2023-03-31 20:27:52,682:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 20:27:52,682:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 20:27:52,682:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 20:27:52,682:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 20:27:53,693:INFO:Soft dependency imported: prophet: 1.1.2
2023-03-31 20:27:56,205:INFO:PyCaret ClassificationExperiment
2023-03-31 20:27:56,205:INFO:Logging name: clf-default-name
2023-03-31 20:27:56,205:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-31 20:27:56,205:INFO:version 3.0.0
2023-03-31 20:27:56,205:INFO:Initializing setup()
2023-03-31 20:27:56,205:INFO:self.USI: 5c80
2023-03-31 20:27:56,205:INFO:self._variable_keys: {'memory', 'exp_id', 'logging_param', 'gpu_n_jobs_param', 'seed', '_ml_usecase', 'pipeline', 'X_test', 'X', 'USI', 'y_test', 'X_train', 'fix_imbalance', 'fold_shuffle_param', 'log_plots_param', '_available_plots', 'fold_groups_param', 'exp_name_log', 'y', 'y_train', 'html_param', 'gpu_param', 'n_jobs_param', 'data', 'idx', 'target_param', 'fold_generator', 'is_multiclass'}
2023-03-31 20:27:56,205:INFO:Checking environment
2023-03-31 20:27:56,205:INFO:python_version: 3.9.12
2023-03-31 20:27:56,205:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-31 20:27:56,205:INFO:machine: AMD64
2023-03-31 20:27:56,205:INFO:platform: Windows-10-10.0.22624-SP0
2023-03-31 20:27:56,205:INFO:Memory: svmem(total=8378363904, available=748732416, percent=91.1, used=7629631488, free=748732416)
2023-03-31 20:27:56,205:INFO:Physical Core: 4
2023-03-31 20:27:56,205:INFO:Logical Core: 8
2023-03-31 20:27:56,205:INFO:Checking libraries
2023-03-31 20:27:56,205:INFO:System:
2023-03-31 20:27:56,205:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-31 20:27:56,205:INFO:executable: C:\Users\Mujahid\anaconda3\python.exe
2023-03-31 20:27:56,205:INFO:   machine: Windows-10-10.0.22624-SP0
2023-03-31 20:27:56,205:INFO:PyCaret required dependencies:
2023-03-31 20:27:56,205:INFO:                 pip: 23.0.1
2023-03-31 20:27:56,205:INFO:          setuptools: 61.2.0
2023-03-31 20:27:56,205:INFO:             pycaret: 3.0.0
2023-03-31 20:27:56,205:INFO:             IPython: 8.2.0
2023-03-31 20:27:56,205:INFO:          ipywidgets: 7.6.5
2023-03-31 20:27:56,205:INFO:                tqdm: 4.64.0
2023-03-31 20:27:56,205:INFO:               numpy: 1.21.5
2023-03-31 20:27:56,205:INFO:              pandas: 1.4.2
2023-03-31 20:27:56,205:INFO:              jinja2: 3.1.2
2023-03-31 20:27:56,205:INFO:               scipy: 1.7.3
2023-03-31 20:27:56,205:INFO:              joblib: 1.2.0
2023-03-31 20:27:56,221:INFO:             sklearn: 1.0.2
2023-03-31 20:27:56,221:INFO:                pyod: 1.0.8
2023-03-31 20:27:56,221:INFO:            imblearn: 0.10.1
2023-03-31 20:27:56,221:INFO:   category_encoders: 2.6.0
2023-03-31 20:27:56,221:INFO:            lightgbm: 3.3.5
2023-03-31 20:27:56,221:INFO:               numba: 0.55.1
2023-03-31 20:27:56,221:INFO:            requests: 2.27.1
2023-03-31 20:27:56,221:INFO:          matplotlib: 3.5.1
2023-03-31 20:27:56,221:INFO:          scikitplot: 0.3.7
2023-03-31 20:27:56,221:INFO:         yellowbrick: 1.5
2023-03-31 20:27:56,221:INFO:              plotly: 5.6.0
2023-03-31 20:27:56,221:INFO:             kaleido: 0.2.1
2023-03-31 20:27:56,221:INFO:         statsmodels: 0.13.2
2023-03-31 20:27:56,221:INFO:              sktime: 0.16.1
2023-03-31 20:27:56,221:INFO:               tbats: 1.1.2
2023-03-31 20:27:56,221:INFO:            pmdarima: 2.0.3
2023-03-31 20:27:56,221:INFO:              psutil: 5.9.4
2023-03-31 20:27:56,221:INFO:PyCaret optional dependencies:
2023-03-31 20:27:56,377:INFO:                shap: 0.41.0
2023-03-31 20:27:56,377:INFO:           interpret: Not installed
2023-03-31 20:27:56,377:INFO:                umap: 0.5.3
2023-03-31 20:27:56,377:INFO:    pandas_profiling: 4.0.0
2023-03-31 20:27:56,377:INFO:  explainerdashboard: Not installed
2023-03-31 20:27:56,377:INFO:             autoviz: Not installed
2023-03-31 20:27:56,377:INFO:           fairlearn: Not installed
2023-03-31 20:27:56,377:INFO:             xgboost: 1.7.4
2023-03-31 20:27:56,377:INFO:            catboost: 1.1.1
2023-03-31 20:27:56,377:INFO:              kmodes: 0.12.2
2023-03-31 20:27:56,377:INFO:             mlxtend: 0.21.0
2023-03-31 20:27:56,377:INFO:       statsforecast: Not installed
2023-03-31 20:27:56,377:INFO:        tune_sklearn: Not installed
2023-03-31 20:27:56,377:INFO:                 ray: Not installed
2023-03-31 20:27:56,377:INFO:            hyperopt: Not installed
2023-03-31 20:27:56,377:INFO:              optuna: 3.1.0
2023-03-31 20:27:56,377:INFO:               skopt: Not installed
2023-03-31 20:27:56,377:INFO:              mlflow: 2.2.2
2023-03-31 20:27:56,377:INFO:              gradio: Not installed
2023-03-31 20:27:56,377:INFO:             fastapi: 0.95.0
2023-03-31 20:27:56,377:INFO:             uvicorn: 0.21.1
2023-03-31 20:27:56,377:INFO:              m2cgen: Not installed
2023-03-31 20:27:56,377:INFO:           evidently: Not installed
2023-03-31 20:27:56,377:INFO:               fugue: Not installed
2023-03-31 20:27:56,377:INFO:           streamlit: 1.20.0
2023-03-31 20:27:56,377:INFO:             prophet: 1.1.2
2023-03-31 20:27:56,377:INFO:None
2023-03-31 20:27:56,377:INFO:Set up data.
2023-03-31 20:27:56,691:INFO:Set up train/test split.
2023-03-31 20:27:57,070:INFO:Set up index.
2023-03-31 20:27:57,075:INFO:Set up folding strategy.
2023-03-31 20:27:57,075:INFO:Assigning column types.
2023-03-31 20:27:57,291:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-31 20:27:57,345:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-31 20:27:57,348:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 20:27:57,395:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 20:27:57,513:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 20:27:57,638:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-31 20:27:57,638:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 20:27:57,669:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 20:27:57,669:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 20:27:57,669:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-31 20:27:57,723:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 20:27:57,748:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 20:27:57,748:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 20:27:57,795:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 20:27:57,827:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 20:27:57,827:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 20:27:57,827:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-03-31 20:27:57,905:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 20:27:57,905:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 20:27:57,968:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 20:27:57,968:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 20:27:57,983:INFO:Preparing preprocessing pipeline...
2023-03-31 20:27:58,015:INFO:Set up simple imputation.
2023-03-31 20:27:58,031:INFO:Set up column name cleaning.
2023-03-31 20:27:58,969:INFO:Finished creating preprocessing pipeline.
2023-03-31 20:27:58,969:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Mujahid\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Type', 'Benefit per order',
                                             'Sales per customer',
                                             'Delivery Status',
                                             'Late_delivery_risk',
                                             'Category Name', 'Customer City',
                                             'Customer Country', 'Customer Id',
                                             'Customer Segment',
                                             'Customer State',
                                             'Department Name', 'Latitu...
                                                              verbose=0))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-03-31 20:27:58,969:INFO:Creating final display dataframe.
2023-03-31 20:28:00,463:INFO:Setup _display_container:                     Description             Value
0                    Session id              2494
1                        Target   SUSPECTED_FRAUD
2                   Target type            Binary
3           Original data shape      (352914, 43)
4        Transformed data shape      (352914, 43)
5   Transformed train set shape      (247039, 43)
6    Transformed test set shape      (105875, 43)
7              Numeric features                42
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              5c80
2023-03-31 20:28:00,554:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 20:28:00,556:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 20:28:00,616:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 20:28:00,616:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 20:28:00,616:INFO:setup() successfully completed in 5.64s...............
2023-03-31 20:28:00,635:INFO:Initializing compare_models()
2023-03-31 20:28:00,636:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-03-31 20:28:00,636:INFO:Checking exceptions
2023-03-31 20:28:00,771:INFO:Preparing display monitor
2023-03-31 20:28:00,809:INFO:Initializing Logistic Regression
2023-03-31 20:28:00,809:INFO:Total runtime is 0.0 minutes
2023-03-31 20:28:00,812:INFO:SubProcess create_model() called ==================================
2023-03-31 20:28:00,812:INFO:Initializing create_model()
2023-03-31 20:28:00,812:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4690E0FA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 20:28:00,813:INFO:Checking exceptions
2023-03-31 20:28:00,813:INFO:Importing libraries
2023-03-31 20:28:00,813:INFO:Copying training dataset
2023-03-31 20:28:01,030:INFO:Defining folds
2023-03-31 20:28:01,030:INFO:Declaring metric variables
2023-03-31 20:28:01,030:INFO:Importing untrained model
2023-03-31 20:28:01,030:INFO:Logistic Regression Imported successfully
2023-03-31 20:28:01,051:INFO:Starting cross validation
2023-03-31 20:28:01,052:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 20:29:44,578:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 20:29:44,846:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 20:29:45,536:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 20:29:45,677:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 20:29:45,787:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 20:29:46,086:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 20:29:46,274:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 20:29:46,730:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 20:30:20,652:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 20:30:20,669:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 20:30:22,111:INFO:Calculating mean and std
2023-03-31 20:30:22,126:INFO:Creating metrics dataframe
2023-03-31 20:30:23,307:INFO:Uploading results into container
2023-03-31 20:30:23,307:INFO:Uploading model into container now
2023-03-31 20:30:23,323:INFO:_master_model_container: 1
2023-03-31 20:30:23,323:INFO:_display_container: 2
2023-03-31 20:30:23,323:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=2494, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-03-31 20:30:23,323:INFO:create_model() successfully completed......................................
2023-03-31 20:30:29,701:INFO:SubProcess create_model() end ==================================
2023-03-31 20:30:29,701:INFO:Creating metrics dataframe
2023-03-31 20:30:29,719:INFO:Initializing K Neighbors Classifier
2023-03-31 20:30:29,719:INFO:Total runtime is 2.4818315466245013 minutes
2023-03-31 20:30:29,729:INFO:SubProcess create_model() called ==================================
2023-03-31 20:30:29,729:INFO:Initializing create_model()
2023-03-31 20:30:29,729:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4690E0FA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 20:30:29,729:INFO:Checking exceptions
2023-03-31 20:30:29,729:INFO:Importing libraries
2023-03-31 20:30:29,729:INFO:Copying training dataset
2023-03-31 20:30:30,037:INFO:Defining folds
2023-03-31 20:30:30,037:INFO:Declaring metric variables
2023-03-31 20:30:30,045:INFO:Importing untrained model
2023-03-31 20:30:30,049:INFO:K Neighbors Classifier Imported successfully
2023-03-31 20:30:30,056:INFO:Starting cross validation
2023-03-31 20:30:30,056:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 20:30:32,367:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 20:30:33,612:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 20:30:42,543:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 20:30:48,224:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 2.36s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 20:31:45,135:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 71.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 20:31:45,387:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 58.20s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 20:31:46,634:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 49.28s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 20:32:43,663:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py", line 214, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1726, in pairwise_distances_chunked
    D_chunk = reduce_func(D_chunk, sl.start)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 634, in _kneighbors_reduce_func
    neigh_ind = np.argpartition(dist, n_neighbors - 1, axis=1)
  File "<__array_function__ internals>", line 5, in argpartition
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 839, in argpartition
    return _wrapfunc(a, 'argpartition', kth, axis=axis, kind=kind, order=order)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 57, in _wrapfunc
    return bound(*args, **kwds)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1023. MiB for an array with shape (603, 222335) and data type int64

  warnings.warn(

2023-03-31 20:32:43,870:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py", line 214, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1726, in pairwise_distances_chunked
    D_chunk = reduce_func(D_chunk, sl.start)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 634, in _kneighbors_reduce_func
    neigh_ind = np.argpartition(dist, n_neighbors - 1, axis=1)
  File "<__array_function__ internals>", line 5, in argpartition
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 839, in argpartition
    return _wrapfunc(a, 'argpartition', kth, axis=axis, kind=kind, order=order)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 57, in _wrapfunc
    return bound(*args, **kwds)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1023. MiB for an array with shape (603, 222335) and data type int64

  warnings.warn(

2023-03-31 20:33:11,012:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 20:33:16,120:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 4.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 20:33:16,645:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 20:33:18,524:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 20:33:20,621:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.35s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 20:33:26,106:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 20:33:27,470:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 20:33:27,848:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py", line 214, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1726, in pairwise_distances_chunked
    D_chunk = reduce_func(D_chunk, sl.start)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 634, in _kneighbors_reduce_func
    neigh_ind = np.argpartition(dist, n_neighbors - 1, axis=1)
  File "<__array_function__ internals>", line 5, in argpartition
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 839, in argpartition
    return _wrapfunc(a, 'argpartition', kth, axis=axis, kind=kind, order=order)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 57, in _wrapfunc
    return bound(*args, **kwds)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1023. MiB for an array with shape (603, 222335) and data type int64

  warnings.warn(

2023-03-31 20:33:27,989:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py", line 214, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (603, 27792) and data type float64

  warnings.warn(

2023-03-31 21:14:03,497:INFO:Calculating mean and std
2023-03-31 21:14:03,591:INFO:Creating metrics dataframe
2023-03-31 21:14:04,789:INFO:Uploading results into container
2023-03-31 21:14:04,789:INFO:Uploading model into container now
2023-03-31 21:14:04,805:INFO:_master_model_container: 2
2023-03-31 21:14:04,805:INFO:_display_container: 2
2023-03-31 21:14:04,805:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-03-31 21:14:04,805:INFO:create_model() successfully completed......................................
2023-03-31 21:14:06,530:INFO:SubProcess create_model() end ==================================
2023-03-31 21:14:06,530:INFO:Creating metrics dataframe
2023-03-31 21:14:06,553:INFO:Initializing Naive Bayes
2023-03-31 21:14:06,553:INFO:Total runtime is 46.0957459171613 minutes
2023-03-31 21:14:06,564:INFO:SubProcess create_model() called ==================================
2023-03-31 21:14:06,564:INFO:Initializing create_model()
2023-03-31 21:14:06,564:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4690E0FA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 21:14:06,564:INFO:Checking exceptions
2023-03-31 21:14:06,565:INFO:Importing libraries
2023-03-31 21:14:06,565:INFO:Copying training dataset
2023-03-31 21:14:06,915:INFO:Defining folds
2023-03-31 21:14:06,915:INFO:Declaring metric variables
2023-03-31 21:14:06,919:INFO:Importing untrained model
2023-03-31 21:14:06,922:INFO:Naive Bayes Imported successfully
2023-03-31 21:14:06,929:INFO:Starting cross validation
2023-03-31 21:14:06,930:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 21:14:22,683:INFO:Calculating mean and std
2023-03-31 21:14:22,683:INFO:Creating metrics dataframe
2023-03-31 21:14:23,777:INFO:Uploading results into container
2023-03-31 21:14:23,777:INFO:Uploading model into container now
2023-03-31 21:14:23,777:INFO:_master_model_container: 3
2023-03-31 21:14:23,777:INFO:_display_container: 2
2023-03-31 21:14:23,777:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-03-31 21:14:23,777:INFO:create_model() successfully completed......................................
2023-03-31 21:14:24,044:INFO:SubProcess create_model() end ==================================
2023-03-31 21:14:24,044:INFO:Creating metrics dataframe
2023-03-31 21:14:24,061:INFO:Initializing Decision Tree Classifier
2023-03-31 21:14:24,061:INFO:Total runtime is 46.38753116130829 minutes
2023-03-31 21:14:24,064:INFO:SubProcess create_model() called ==================================
2023-03-31 21:14:24,064:INFO:Initializing create_model()
2023-03-31 21:14:24,064:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4690E0FA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 21:14:24,064:INFO:Checking exceptions
2023-03-31 21:14:24,064:INFO:Importing libraries
2023-03-31 21:14:24,064:INFO:Copying training dataset
2023-03-31 21:14:24,289:INFO:Defining folds
2023-03-31 21:14:24,289:INFO:Declaring metric variables
2023-03-31 21:14:24,304:INFO:Importing untrained model
2023-03-31 21:14:24,308:INFO:Decision Tree Classifier Imported successfully
2023-03-31 21:14:24,312:INFO:Starting cross validation
2023-03-31 21:14:24,312:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 21:14:36,671:INFO:Calculating mean and std
2023-03-31 21:14:36,671:INFO:Creating metrics dataframe
2023-03-31 21:14:37,745:INFO:Uploading results into container
2023-03-31 21:14:37,745:INFO:Uploading model into container now
2023-03-31 21:14:37,745:INFO:_master_model_container: 4
2023-03-31 21:14:37,745:INFO:_display_container: 2
2023-03-31 21:14:37,745:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2494, splitter='best')
2023-03-31 21:14:37,745:INFO:create_model() successfully completed......................................
2023-03-31 21:14:37,996:INFO:SubProcess create_model() end ==================================
2023-03-31 21:14:38,012:INFO:Creating metrics dataframe
2023-03-31 21:14:38,025:INFO:Initializing SVM - Linear Kernel
2023-03-31 21:14:38,025:INFO:Total runtime is 46.620262976487474 minutes
2023-03-31 21:14:38,028:INFO:SubProcess create_model() called ==================================
2023-03-31 21:14:38,028:INFO:Initializing create_model()
2023-03-31 21:14:38,028:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4690E0FA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 21:14:38,028:INFO:Checking exceptions
2023-03-31 21:14:38,028:INFO:Importing libraries
2023-03-31 21:14:38,028:INFO:Copying training dataset
2023-03-31 21:14:38,252:INFO:Defining folds
2023-03-31 21:14:38,252:INFO:Declaring metric variables
2023-03-31 21:14:38,252:INFO:Importing untrained model
2023-03-31 21:14:38,261:INFO:SVM - Linear Kernel Imported successfully
2023-03-31 21:14:38,265:INFO:Starting cross validation
2023-03-31 21:14:38,265:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 21:15:39,211:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-31 21:15:42,177:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-31 21:15:48,845:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-31 21:16:00,688:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-31 21:16:02,616:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-31 21:16:03,893:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-31 21:16:04,949:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-31 21:16:05,849:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-31 21:16:32,721:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-31 21:16:42,370:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-03-31 21:16:42,512:INFO:Calculating mean and std
2023-03-31 21:16:42,512:INFO:Creating metrics dataframe
2023-03-31 21:16:43,569:INFO:Uploading results into container
2023-03-31 21:16:43,569:INFO:Uploading model into container now
2023-03-31 21:16:43,569:INFO:_master_model_container: 5
2023-03-31 21:16:43,569:INFO:_display_container: 2
2023-03-31 21:16:43,585:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=2494, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-03-31 21:16:43,585:INFO:create_model() successfully completed......................................
2023-03-31 21:16:43,852:INFO:SubProcess create_model() end ==================================
2023-03-31 21:16:43,852:INFO:Creating metrics dataframe
2023-03-31 21:16:43,868:INFO:Initializing Ridge Classifier
2023-03-31 21:16:43,868:INFO:Total runtime is 48.717652678489685 minutes
2023-03-31 21:16:43,868:INFO:SubProcess create_model() called ==================================
2023-03-31 21:16:43,868:INFO:Initializing create_model()
2023-03-31 21:16:43,868:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4690E0FA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 21:16:43,868:INFO:Checking exceptions
2023-03-31 21:16:43,868:INFO:Importing libraries
2023-03-31 21:16:43,873:INFO:Copying training dataset
2023-03-31 21:16:44,089:INFO:Defining folds
2023-03-31 21:16:44,089:INFO:Declaring metric variables
2023-03-31 21:16:44,089:INFO:Importing untrained model
2023-03-31 21:16:44,104:INFO:Ridge Classifier Imported successfully
2023-03-31 21:16:44,112:INFO:Starting cross validation
2023-03-31 21:16:44,113:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 21:16:46,043:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-31 21:16:46,121:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-31 21:16:46,152:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-31 21:16:46,200:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-31 21:16:46,231:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-31 21:16:46,388:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-31 21:16:46,388:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-31 21:16:46,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-31 21:16:50,109:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-31 21:16:50,124:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-03-31 21:16:56,862:INFO:Calculating mean and std
2023-03-31 21:16:56,862:INFO:Creating metrics dataframe
2023-03-31 21:16:57,946:INFO:Uploading results into container
2023-03-31 21:16:57,946:INFO:Uploading model into container now
2023-03-31 21:16:57,946:INFO:_master_model_container: 6
2023-03-31 21:16:57,946:INFO:_display_container: 2
2023-03-31 21:16:57,946:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=2494, solver='auto', tol=0.001)
2023-03-31 21:16:57,946:INFO:create_model() successfully completed......................................
2023-03-31 21:16:58,213:INFO:SubProcess create_model() end ==================================
2023-03-31 21:16:58,213:INFO:Creating metrics dataframe
2023-03-31 21:16:58,242:INFO:Initializing Random Forest Classifier
2023-03-31 21:16:58,242:INFO:Total runtime is 48.957214923699695 minutes
2023-03-31 21:16:58,242:INFO:SubProcess create_model() called ==================================
2023-03-31 21:16:58,242:INFO:Initializing create_model()
2023-03-31 21:16:58,242:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4690E0FA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 21:16:58,242:INFO:Checking exceptions
2023-03-31 21:16:58,254:INFO:Importing libraries
2023-03-31 21:16:58,254:INFO:Copying training dataset
2023-03-31 21:16:58,468:INFO:Defining folds
2023-03-31 21:16:58,468:INFO:Declaring metric variables
2023-03-31 21:16:58,477:INFO:Importing untrained model
2023-03-31 21:16:58,481:INFO:Random Forest Classifier Imported successfully
2023-03-31 21:16:58,488:INFO:Starting cross validation
2023-03-31 21:16:58,489:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 21:17:54,543:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.48s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 21:17:54,763:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 21:17:55,799:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 21:17:56,097:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 21:17:56,301:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:319: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 21:18:19,362:INFO:Calculating mean and std
2023-03-31 21:18:19,362:INFO:Creating metrics dataframe
2023-03-31 21:18:20,468:INFO:Uploading results into container
2023-03-31 21:18:20,468:INFO:Uploading model into container now
2023-03-31 21:18:20,468:INFO:_master_model_container: 7
2023-03-31 21:18:20,468:INFO:_display_container: 2
2023-03-31 21:18:20,468:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=2494, verbose=0, warm_start=False)
2023-03-31 21:18:20,468:INFO:create_model() successfully completed......................................
2023-03-31 21:18:20,735:INFO:SubProcess create_model() end ==================================
2023-03-31 21:18:20,735:INFO:Creating metrics dataframe
2023-03-31 21:18:20,759:INFO:Initializing Quadratic Discriminant Analysis
2023-03-31 21:18:20,759:INFO:Total runtime is 50.3325071811676 minutes
2023-03-31 21:18:20,760:INFO:SubProcess create_model() called ==================================
2023-03-31 21:18:20,760:INFO:Initializing create_model()
2023-03-31 21:18:20,760:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4690E0FA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 21:18:20,760:INFO:Checking exceptions
2023-03-31 21:18:20,760:INFO:Importing libraries
2023-03-31 21:18:20,760:INFO:Copying training dataset
2023-03-31 21:18:20,992:INFO:Defining folds
2023-03-31 21:18:20,992:INFO:Declaring metric variables
2023-03-31 21:18:20,997:INFO:Importing untrained model
2023-03-31 21:18:21,001:INFO:Quadratic Discriminant Analysis Imported successfully
2023-03-31 21:18:21,003:INFO:Starting cross validation
2023-03-31 21:18:21,003:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 21:18:23,311:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-31 21:18:23,421:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-31 21:18:23,469:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-31 21:18:23,469:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-31 21:18:23,657:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-31 21:18:23,673:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-31 21:18:23,688:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-31 21:18:23,861:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-31 21:18:29,580:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-31 21:18:29,580:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-03-31 21:18:36,549:INFO:Calculating mean and std
2023-03-31 21:18:36,549:INFO:Creating metrics dataframe
2023-03-31 21:18:37,630:INFO:Uploading results into container
2023-03-31 21:18:37,630:INFO:Uploading model into container now
2023-03-31 21:18:37,630:INFO:_master_model_container: 8
2023-03-31 21:18:37,630:INFO:_display_container: 2
2023-03-31 21:18:37,630:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-03-31 21:18:37,630:INFO:create_model() successfully completed......................................
2023-03-31 21:18:37,911:INFO:SubProcess create_model() end ==================================
2023-03-31 21:18:37,911:INFO:Creating metrics dataframe
2023-03-31 21:18:37,925:INFO:Initializing Ada Boost Classifier
2023-03-31 21:18:37,925:INFO:Total runtime is 50.61860246658325 minutes
2023-03-31 21:18:37,929:INFO:SubProcess create_model() called ==================================
2023-03-31 21:18:37,929:INFO:Initializing create_model()
2023-03-31 21:18:37,929:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4690E0FA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 21:18:37,929:INFO:Checking exceptions
2023-03-31 21:18:37,930:INFO:Importing libraries
2023-03-31 21:18:37,930:INFO:Copying training dataset
2023-03-31 21:18:38,140:INFO:Defining folds
2023-03-31 21:18:38,140:INFO:Declaring metric variables
2023-03-31 21:18:38,156:INFO:Importing untrained model
2023-03-31 21:18:38,159:INFO:Ada Boost Classifier Imported successfully
2023-03-31 21:18:38,165:INFO:Starting cross validation
2023-03-31 21:18:38,165:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 21:18:51,896:INFO:Calculating mean and std
2023-03-31 21:18:51,896:INFO:Creating metrics dataframe
2023-03-31 21:18:52,964:INFO:Uploading results into container
2023-03-31 21:18:52,964:INFO:Uploading model into container now
2023-03-31 21:18:52,964:INFO:_master_model_container: 9
2023-03-31 21:18:52,964:INFO:_display_container: 2
2023-03-31 21:18:52,964:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=2494)
2023-03-31 21:18:52,964:INFO:create_model() successfully completed......................................
2023-03-31 21:18:53,231:INFO:SubProcess create_model() end ==================================
2023-03-31 21:18:53,231:INFO:Creating metrics dataframe
2023-03-31 21:18:53,249:INFO:Initializing Gradient Boosting Classifier
2023-03-31 21:18:53,249:INFO:Total runtime is 50.874006422360736 minutes
2023-03-31 21:18:53,249:INFO:SubProcess create_model() called ==================================
2023-03-31 21:18:53,249:INFO:Initializing create_model()
2023-03-31 21:18:53,249:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4690E0FA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 21:18:53,249:INFO:Checking exceptions
2023-03-31 21:18:53,249:INFO:Importing libraries
2023-03-31 21:18:53,249:INFO:Copying training dataset
2023-03-31 21:18:53,481:INFO:Defining folds
2023-03-31 21:18:53,481:INFO:Declaring metric variables
2023-03-31 21:18:53,481:INFO:Importing untrained model
2023-03-31 21:18:53,488:INFO:Gradient Boosting Classifier Imported successfully
2023-03-31 21:18:53,492:INFO:Starting cross validation
2023-03-31 21:18:53,492:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 21:22:19,124:INFO:Calculating mean and std
2023-03-31 21:22:19,124:INFO:Creating metrics dataframe
2023-03-31 21:22:20,435:INFO:Uploading results into container
2023-03-31 21:22:20,435:INFO:Uploading model into container now
2023-03-31 21:22:20,435:INFO:_master_model_container: 10
2023-03-31 21:22:20,435:INFO:_display_container: 2
2023-03-31 21:22:20,435:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='deviance', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2494, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-03-31 21:22:20,435:INFO:create_model() successfully completed......................................
2023-03-31 21:22:20,701:INFO:SubProcess create_model() end ==================================
2023-03-31 21:22:20,701:INFO:Creating metrics dataframe
2023-03-31 21:22:20,731:INFO:Initializing Linear Discriminant Analysis
2023-03-31 21:22:20,732:INFO:Total runtime is 54.33205593427022 minutes
2023-03-31 21:22:20,736:INFO:SubProcess create_model() called ==================================
2023-03-31 21:22:20,736:INFO:Initializing create_model()
2023-03-31 21:22:20,736:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4690E0FA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 21:22:20,736:INFO:Checking exceptions
2023-03-31 21:22:20,736:INFO:Importing libraries
2023-03-31 21:22:20,736:INFO:Copying training dataset
2023-03-31 21:22:20,961:INFO:Defining folds
2023-03-31 21:22:20,961:INFO:Declaring metric variables
2023-03-31 21:22:20,967:INFO:Importing untrained model
2023-03-31 21:22:20,974:INFO:Linear Discriminant Analysis Imported successfully
2023-03-31 21:22:20,980:INFO:Starting cross validation
2023-03-31 21:22:20,981:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 21:22:40,028:INFO:Calculating mean and std
2023-03-31 21:22:40,028:INFO:Creating metrics dataframe
2023-03-31 21:22:41,116:INFO:Uploading results into container
2023-03-31 21:22:41,116:INFO:Uploading model into container now
2023-03-31 21:22:41,116:INFO:_master_model_container: 11
2023-03-31 21:22:41,116:INFO:_display_container: 2
2023-03-31 21:22:41,116:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-03-31 21:22:41,116:INFO:create_model() successfully completed......................................
2023-03-31 21:22:41,491:INFO:SubProcess create_model() end ==================================
2023-03-31 21:22:41,491:INFO:Creating metrics dataframe
2023-03-31 21:22:41,507:INFO:Initializing Extra Trees Classifier
2023-03-31 21:22:41,507:INFO:Total runtime is 54.67829852898915 minutes
2023-03-31 21:22:41,511:INFO:SubProcess create_model() called ==================================
2023-03-31 21:22:41,511:INFO:Initializing create_model()
2023-03-31 21:22:41,511:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4690E0FA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 21:22:41,512:INFO:Checking exceptions
2023-03-31 21:22:41,512:INFO:Importing libraries
2023-03-31 21:22:41,512:INFO:Copying training dataset
2023-03-31 21:22:41,736:INFO:Defining folds
2023-03-31 21:22:41,736:INFO:Declaring metric variables
2023-03-31 21:22:41,740:INFO:Importing untrained model
2023-03-31 21:22:41,744:INFO:Extra Trees Classifier Imported successfully
2023-03-31 21:22:41,750:INFO:Starting cross validation
2023-03-31 21:22:41,751:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 21:23:30,270:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 21:23:32,326:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.00s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 21:23:32,404:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 21:23:32,502:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 21:23:32,624:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 21:23:32,687:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 21:23:33,047:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 21:23:33,628:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 21:23:33,769:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 21:23:33,957:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:319: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 21:23:34,305:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:319: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 21:23:55,272:INFO:Calculating mean and std
2023-03-31 21:23:55,272:INFO:Creating metrics dataframe
2023-03-31 21:23:56,384:INFO:Uploading results into container
2023-03-31 21:23:56,384:INFO:Uploading model into container now
2023-03-31 21:23:56,384:INFO:_master_model_container: 12
2023-03-31 21:23:56,384:INFO:_display_container: 2
2023-03-31 21:23:56,384:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=2494, verbose=0, warm_start=False)
2023-03-31 21:23:56,384:INFO:create_model() successfully completed......................................
2023-03-31 21:23:56,655:INFO:SubProcess create_model() end ==================================
2023-03-31 21:23:56,655:INFO:Creating metrics dataframe
2023-03-31 21:23:56,673:INFO:Initializing Extreme Gradient Boosting
2023-03-31 21:23:56,673:INFO:Total runtime is 55.93107675313949 minutes
2023-03-31 21:23:56,673:INFO:SubProcess create_model() called ==================================
2023-03-31 21:23:56,673:INFO:Initializing create_model()
2023-03-31 21:23:56,673:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4690E0FA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 21:23:56,673:INFO:Checking exceptions
2023-03-31 21:23:56,673:INFO:Importing libraries
2023-03-31 21:23:56,673:INFO:Copying training dataset
2023-03-31 21:23:56,899:INFO:Defining folds
2023-03-31 21:23:56,899:INFO:Declaring metric variables
2023-03-31 21:23:56,899:INFO:Importing untrained model
2023-03-31 21:23:56,917:INFO:Extreme Gradient Boosting Imported successfully
2023-03-31 21:23:56,924:INFO:Starting cross validation
2023-03-31 21:23:56,924:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 21:25:16,891:INFO:Calculating mean and std
2023-03-31 21:25:16,891:INFO:Creating metrics dataframe
2023-03-31 21:25:18,173:INFO:Uploading results into container
2023-03-31 21:25:18,173:INFO:Uploading model into container now
2023-03-31 21:25:18,173:INFO:_master_model_container: 13
2023-03-31 21:25:18,173:INFO:_display_container: 2
2023-03-31 21:25:18,173:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-03-31 21:25:18,173:INFO:create_model() successfully completed......................................
2023-03-31 21:25:18,440:INFO:SubProcess create_model() end ==================================
2023-03-31 21:25:18,440:INFO:Creating metrics dataframe
2023-03-31 21:25:18,455:INFO:Initializing Light Gradient Boosting Machine
2023-03-31 21:25:18,455:INFO:Total runtime is 57.294112336635585 minutes
2023-03-31 21:25:18,470:INFO:SubProcess create_model() called ==================================
2023-03-31 21:25:18,470:INFO:Initializing create_model()
2023-03-31 21:25:18,470:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4690E0FA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 21:25:18,470:INFO:Checking exceptions
2023-03-31 21:25:18,470:INFO:Importing libraries
2023-03-31 21:25:18,470:INFO:Copying training dataset
2023-03-31 21:25:18,699:INFO:Defining folds
2023-03-31 21:25:18,699:INFO:Declaring metric variables
2023-03-31 21:25:18,699:INFO:Importing untrained model
2023-03-31 21:25:18,706:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-31 21:25:18,709:INFO:Starting cross validation
2023-03-31 21:25:18,709:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 21:25:53,532:INFO:Calculating mean and std
2023-03-31 21:25:53,563:INFO:Creating metrics dataframe
2023-03-31 21:25:54,755:INFO:Uploading results into container
2023-03-31 21:25:54,755:INFO:Uploading model into container now
2023-03-31 21:25:54,764:INFO:_master_model_container: 14
2023-03-31 21:25:54,764:INFO:_display_container: 2
2023-03-31 21:25:54,767:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2494, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-03-31 21:25:54,768:INFO:create_model() successfully completed......................................
2023-03-31 21:25:55,963:INFO:SubProcess create_model() end ==================================
2023-03-31 21:25:55,963:INFO:Creating metrics dataframe
2023-03-31 21:25:55,990:INFO:Initializing CatBoost Classifier
2023-03-31 21:25:55,990:INFO:Total runtime is 57.919693470001214 minutes
2023-03-31 21:25:55,995:INFO:SubProcess create_model() called ==================================
2023-03-31 21:25:55,995:INFO:Initializing create_model()
2023-03-31 21:25:55,995:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4690E0FA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 21:25:55,995:INFO:Checking exceptions
2023-03-31 21:25:55,995:INFO:Importing libraries
2023-03-31 21:25:55,996:INFO:Copying training dataset
2023-03-31 21:25:56,280:INFO:Defining folds
2023-03-31 21:25:56,281:INFO:Declaring metric variables
2023-03-31 21:25:56,285:INFO:Importing untrained model
2023-03-31 21:25:56,292:INFO:CatBoost Classifier Imported successfully
2023-03-31 21:25:56,299:INFO:Starting cross validation
2023-03-31 21:25:56,301:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 21:30:50,713:INFO:Calculating mean and std
2023-03-31 21:30:50,713:INFO:Creating metrics dataframe
2023-03-31 21:30:52,004:INFO:Uploading results into container
2023-03-31 21:30:52,004:INFO:Uploading model into container now
2023-03-31 21:30:52,004:INFO:_master_model_container: 15
2023-03-31 21:30:52,004:INFO:_display_container: 2
2023-03-31 21:30:52,004:INFO:<catboost.core.CatBoostClassifier object at 0x000001E44BC045E0>
2023-03-31 21:30:52,004:INFO:create_model() successfully completed......................................
2023-03-31 21:30:52,271:INFO:SubProcess create_model() end ==================================
2023-03-31 21:30:52,271:INFO:Creating metrics dataframe
2023-03-31 21:30:52,290:INFO:Initializing Dummy Classifier
2023-03-31 21:30:52,290:INFO:Total runtime is 62.85802705287933 minutes
2023-03-31 21:30:52,290:INFO:SubProcess create_model() called ==================================
2023-03-31 21:30:52,290:INFO:Initializing create_model()
2023-03-31 21:30:52,290:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4690E0FA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 21:30:52,290:INFO:Checking exceptions
2023-03-31 21:30:52,290:INFO:Importing libraries
2023-03-31 21:30:52,290:INFO:Copying training dataset
2023-03-31 21:30:52,527:INFO:Defining folds
2023-03-31 21:30:52,527:INFO:Declaring metric variables
2023-03-31 21:30:52,527:INFO:Importing untrained model
2023-03-31 21:30:52,535:INFO:Dummy Classifier Imported successfully
2023-03-31 21:30:52,539:INFO:Starting cross validation
2023-03-31 21:30:52,539:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 21:30:57,704:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-03-31 21:31:04,673:INFO:Calculating mean and std
2023-03-31 21:31:04,673:INFO:Creating metrics dataframe
2023-03-31 21:31:05,748:INFO:Uploading results into container
2023-03-31 21:31:05,748:INFO:Uploading model into container now
2023-03-31 21:31:05,748:INFO:_master_model_container: 16
2023-03-31 21:31:05,748:INFO:_display_container: 2
2023-03-31 21:31:05,748:INFO:DummyClassifier(constant=None, random_state=2494, strategy='prior')
2023-03-31 21:31:05,748:INFO:create_model() successfully completed......................................
2023-03-31 21:31:06,014:INFO:SubProcess create_model() end ==================================
2023-03-31 21:31:06,014:INFO:Creating metrics dataframe
2023-03-31 21:31:06,035:INFO:Initializing create_model()
2023-03-31 21:31:06,035:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2494, splitter='best'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-31 21:31:06,035:INFO:Checking exceptions
2023-03-31 21:31:06,043:INFO:Importing libraries
2023-03-31 21:31:06,043:INFO:Copying training dataset
2023-03-31 21:31:06,260:INFO:Defining folds
2023-03-31 21:31:06,260:INFO:Declaring metric variables
2023-03-31 21:31:06,260:INFO:Importing untrained model
2023-03-31 21:31:06,260:INFO:Declaring custom model
2023-03-31 21:31:06,260:INFO:Decision Tree Classifier Imported successfully
2023-03-31 21:31:06,276:INFO:Cross validation set to False
2023-03-31 21:31:06,276:INFO:Fitting Model
2023-03-31 21:31:07,798:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2494, splitter='best')
2023-03-31 21:31:07,798:INFO:create_model() successfully completed......................................
2023-03-31 21:31:08,108:INFO:_master_model_container: 16
2023-03-31 21:31:08,108:INFO:_display_container: 2
2023-03-31 21:31:08,108:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2494, splitter='best')
2023-03-31 21:31:08,108:INFO:compare_models() successfully completed......................................
2023-03-31 21:31:08,138:INFO:Initializing tune_model()
2023-03-31 21:31:08,138:INFO:tune_model(estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2494, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>)
2023-03-31 21:31:08,138:INFO:Checking exceptions
2023-03-31 21:31:08,249:INFO:Copying training dataset
2023-03-31 21:31:08,398:INFO:Checking base model
2023-03-31 21:31:08,398:INFO:Base model : Decision Tree Classifier
2023-03-31 21:31:08,402:INFO:Declaring metric variables
2023-03-31 21:31:08,405:INFO:Defining Hyperparameters
2023-03-31 21:31:08,669:INFO:Tuning with n_jobs=-1
2023-03-31 21:31:08,669:INFO:Initializing RandomizedSearchCV
2023-03-31 21:33:16,055:INFO:best_params: {'actual_estimator__min_samples_split': 7, 'actual_estimator__min_samples_leaf': 3, 'actual_estimator__min_impurity_decrease': 0.02, 'actual_estimator__max_features': 'log2', 'actual_estimator__max_depth': 13, 'actual_estimator__criterion': 'entropy'}
2023-03-31 21:33:16,055:INFO:Hyperparameter search completed
2023-03-31 21:33:16,055:INFO:SubProcess create_model() called ==================================
2023-03-31 21:33:16,055:INFO:Initializing create_model()
2023-03-31 21:33:16,055:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2494, splitter='best'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E467F5FE50>, model_only=True, return_train_score=False, kwargs={'min_samples_split': 7, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.02, 'max_features': 'log2', 'max_depth': 13, 'criterion': 'entropy'})
2023-03-31 21:33:16,055:INFO:Checking exceptions
2023-03-31 21:33:16,055:INFO:Importing libraries
2023-03-31 21:33:16,055:INFO:Copying training dataset
2023-03-31 21:33:16,270:INFO:Defining folds
2023-03-31 21:33:16,270:INFO:Declaring metric variables
2023-03-31 21:33:16,288:INFO:Importing untrained model
2023-03-31 21:33:16,288:INFO:Declaring custom model
2023-03-31 21:33:16,294:INFO:Decision Tree Classifier Imported successfully
2023-03-31 21:33:16,297:INFO:Starting cross validation
2023-03-31 21:33:16,297:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 21:33:28,940:INFO:Calculating mean and std
2023-03-31 21:33:28,940:INFO:Creating metrics dataframe
2023-03-31 21:33:28,940:INFO:Finalizing model
2023-03-31 21:33:30,467:INFO:Uploading results into container
2023-03-31 21:33:30,467:INFO:Uploading model into container now
2023-03-31 21:33:30,467:INFO:_master_model_container: 17
2023-03-31 21:33:30,467:INFO:_display_container: 3
2023-03-31 21:33:30,473:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',
                       max_depth=13, max_features='log2', max_leaf_nodes=None,
                       min_impurity_decrease=0.02, min_samples_leaf=3,
                       min_samples_split=7, min_weight_fraction_leaf=0.0,
                       random_state=2494, splitter='best')
2023-03-31 21:33:30,473:INFO:create_model() successfully completed......................................
2023-03-31 21:33:30,733:INFO:SubProcess create_model() end ==================================
2023-03-31 21:33:30,733:INFO:choose_better activated
2023-03-31 21:33:30,733:INFO:SubProcess create_model() called ==================================
2023-03-31 21:33:30,733:INFO:Initializing create_model()
2023-03-31 21:33:30,733:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2494, splitter='best'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-31 21:33:30,733:INFO:Checking exceptions
2023-03-31 21:33:30,749:INFO:Importing libraries
2023-03-31 21:33:30,749:INFO:Copying training dataset
2023-03-31 21:33:30,970:INFO:Defining folds
2023-03-31 21:33:30,970:INFO:Declaring metric variables
2023-03-31 21:33:30,970:INFO:Importing untrained model
2023-03-31 21:33:30,970:INFO:Declaring custom model
2023-03-31 21:33:30,970:INFO:Decision Tree Classifier Imported successfully
2023-03-31 21:33:30,970:INFO:Starting cross validation
2023-03-31 21:33:30,970:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 21:33:33,404:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 21:33:43,656:INFO:Calculating mean and std
2023-03-31 21:33:43,656:INFO:Creating metrics dataframe
2023-03-31 21:33:43,656:INFO:Finalizing model
2023-03-31 21:33:44,991:INFO:Uploading results into container
2023-03-31 21:33:44,991:INFO:Uploading model into container now
2023-03-31 21:33:44,991:INFO:_master_model_container: 18
2023-03-31 21:33:44,991:INFO:_display_container: 4
2023-03-31 21:33:44,991:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2494, splitter='best')
2023-03-31 21:33:44,991:INFO:create_model() successfully completed......................................
2023-03-31 21:33:45,257:INFO:SubProcess create_model() end ==================================
2023-03-31 21:33:45,273:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2494, splitter='best') result for Accuracy is 1.0
2023-03-31 21:33:45,273:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',
                       max_depth=13, max_features='log2', max_leaf_nodes=None,
                       min_impurity_decrease=0.02, min_samples_leaf=3,
                       min_samples_split=7, min_weight_fraction_leaf=0.0,
                       random_state=2494, splitter='best') result for Accuracy is 1.0
2023-03-31 21:33:45,273:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2494, splitter='best') is best model
2023-03-31 21:33:45,273:INFO:choose_better completed
2023-03-31 21:33:45,273:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-03-31 21:33:45,287:INFO:_master_model_container: 18
2023-03-31 21:33:45,287:INFO:_display_container: 3
2023-03-31 21:33:45,287:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2494, splitter='best')
2023-03-31 21:33:45,288:INFO:tune_model() successfully completed......................................
2023-03-31 21:33:46,440:INFO:Initializing plot_model()
2023-03-31 21:33:46,440:INFO:plot_model(plot=confusion_matrix, fold=None, use_train_data=False, verbose=True, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2494, splitter='best'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E380005130>, system=True)
2023-03-31 21:33:46,440:INFO:Checking exceptions
2023-03-31 21:33:46,530:INFO:Preloading libraries
2023-03-31 21:33:46,530:INFO:Copying training dataset
2023-03-31 21:33:46,530:INFO:Plot type: confusion_matrix
2023-03-31 21:33:47,307:INFO:Fitting Model
2023-03-31 21:33:47,307:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\base.py:450: UserWarning:

X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names


2023-03-31 21:33:47,307:INFO:Scoring test/hold-out set
2023-03-31 21:33:47,553:INFO:Visual Rendered Successfully
2023-03-31 21:33:47,823:INFO:plot_model() successfully completed......................................
2023-03-31 22:51:34,388:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 22:51:34,388:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 22:51:34,388:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 22:51:34,388:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 22:51:35,308:INFO:Soft dependency imported: prophet: 1.1.2
2023-03-31 22:55:51,690:INFO:PyCaret ClassificationExperiment
2023-03-31 22:55:51,690:INFO:Logging name: clf-default-name
2023-03-31 22:55:51,690:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-31 22:55:51,690:INFO:version 3.0.0
2023-03-31 22:55:51,690:INFO:Initializing setup()
2023-03-31 22:55:51,690:INFO:self.USI: ef9c
2023-03-31 22:55:51,690:INFO:self._variable_keys: {'gpu_n_jobs_param', '_ml_usecase', 'y_train', 'USI', 'fold_shuffle_param', 'X_train', 'data', 'is_multiclass', 'seed', 'fold_generator', 'X', 'fold_groups_param', 'logging_param', '_available_plots', 'log_plots_param', 'idx', 'pipeline', 'gpu_param', 'html_param', 'y', 'fix_imbalance', 'X_test', 'exp_id', 'n_jobs_param', 'target_param', 'y_test', 'exp_name_log', 'memory'}
2023-03-31 22:55:51,690:INFO:Checking environment
2023-03-31 22:55:51,690:INFO:python_version: 3.9.12
2023-03-31 22:55:51,690:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-31 22:55:51,690:INFO:machine: AMD64
2023-03-31 22:55:51,690:INFO:platform: Windows-10-10.0.22624-SP0
2023-03-31 22:55:51,690:INFO:Memory: svmem(total=8378363904, available=891830272, percent=89.4, used=7486533632, free=891830272)
2023-03-31 22:55:51,690:INFO:Physical Core: 4
2023-03-31 22:55:51,690:INFO:Logical Core: 8
2023-03-31 22:55:51,690:INFO:Checking libraries
2023-03-31 22:55:51,690:INFO:System:
2023-03-31 22:55:51,690:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-31 22:55:51,690:INFO:executable: C:\Users\Mujahid\anaconda3\python.exe
2023-03-31 22:55:51,690:INFO:   machine: Windows-10-10.0.22624-SP0
2023-03-31 22:55:51,690:INFO:PyCaret required dependencies:
2023-03-31 22:55:51,690:INFO:                 pip: 23.0.1
2023-03-31 22:55:51,690:INFO:          setuptools: 61.2.0
2023-03-31 22:55:51,690:INFO:             pycaret: 3.0.0
2023-03-31 22:55:51,690:INFO:             IPython: 8.2.0
2023-03-31 22:55:51,690:INFO:          ipywidgets: 7.6.5
2023-03-31 22:55:51,690:INFO:                tqdm: 4.64.0
2023-03-31 22:55:51,690:INFO:               numpy: 1.21.5
2023-03-31 22:55:51,690:INFO:              pandas: 1.4.2
2023-03-31 22:55:51,690:INFO:              jinja2: 3.1.2
2023-03-31 22:55:51,690:INFO:               scipy: 1.7.3
2023-03-31 22:55:51,690:INFO:              joblib: 1.2.0
2023-03-31 22:55:51,690:INFO:             sklearn: 1.0.2
2023-03-31 22:55:51,690:INFO:                pyod: 1.0.8
2023-03-31 22:55:51,690:INFO:            imblearn: 0.10.1
2023-03-31 22:55:51,690:INFO:   category_encoders: 2.6.0
2023-03-31 22:55:51,690:INFO:            lightgbm: 3.3.5
2023-03-31 22:55:51,690:INFO:               numba: 0.55.1
2023-03-31 22:55:51,690:INFO:            requests: 2.27.1
2023-03-31 22:55:51,690:INFO:          matplotlib: 3.5.1
2023-03-31 22:55:51,690:INFO:          scikitplot: 0.3.7
2023-03-31 22:55:51,690:INFO:         yellowbrick: 1.5
2023-03-31 22:55:51,690:INFO:              plotly: 5.6.0
2023-03-31 22:55:51,690:INFO:             kaleido: 0.2.1
2023-03-31 22:55:51,690:INFO:         statsmodels: 0.13.2
2023-03-31 22:55:51,690:INFO:              sktime: 0.16.1
2023-03-31 22:55:51,690:INFO:               tbats: 1.1.2
2023-03-31 22:55:51,690:INFO:            pmdarima: 2.0.3
2023-03-31 22:55:51,690:INFO:              psutil: 5.9.4
2023-03-31 22:55:51,690:INFO:PyCaret optional dependencies:
2023-03-31 22:55:51,972:INFO:                shap: 0.41.0
2023-03-31 22:55:51,972:INFO:           interpret: Not installed
2023-03-31 22:55:51,972:INFO:                umap: 0.5.3
2023-03-31 22:55:51,972:INFO:    pandas_profiling: 4.0.0
2023-03-31 22:55:51,972:INFO:  explainerdashboard: Not installed
2023-03-31 22:55:51,972:INFO:             autoviz: Not installed
2023-03-31 22:55:51,972:INFO:           fairlearn: Not installed
2023-03-31 22:55:51,972:INFO:             xgboost: 1.7.4
2023-03-31 22:55:51,972:INFO:            catboost: 1.1.1
2023-03-31 22:55:51,972:INFO:              kmodes: 0.12.2
2023-03-31 22:55:51,972:INFO:             mlxtend: 0.21.0
2023-03-31 22:55:51,972:INFO:       statsforecast: Not installed
2023-03-31 22:55:51,972:INFO:        tune_sklearn: Not installed
2023-03-31 22:55:51,972:INFO:                 ray: Not installed
2023-03-31 22:55:51,972:INFO:            hyperopt: Not installed
2023-03-31 22:55:51,972:INFO:              optuna: 3.1.0
2023-03-31 22:55:51,972:INFO:               skopt: Not installed
2023-03-31 22:55:51,972:INFO:              mlflow: 2.2.2
2023-03-31 22:55:51,972:INFO:              gradio: Not installed
2023-03-31 22:55:51,972:INFO:             fastapi: 0.95.0
2023-03-31 22:55:51,972:INFO:             uvicorn: 0.21.1
2023-03-31 22:55:51,972:INFO:              m2cgen: Not installed
2023-03-31 22:55:51,972:INFO:           evidently: Not installed
2023-03-31 22:55:51,972:INFO:               fugue: Not installed
2023-03-31 22:55:51,972:INFO:           streamlit: 1.20.0
2023-03-31 22:55:51,972:INFO:             prophet: 1.1.2
2023-03-31 22:55:51,972:INFO:None
2023-03-31 22:55:51,972:INFO:Set up data.
2023-03-31 22:55:52,614:INFO:Set up train/test split.
2023-03-31 22:55:52,806:INFO:Set up index.
2023-03-31 22:55:52,814:INFO:Set up folding strategy.
2023-03-31 22:55:52,814:INFO:Assigning column types.
2023-03-31 22:55:52,870:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-31 22:55:52,918:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-31 22:55:52,918:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 22:55:52,958:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 22:55:53,093:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 22:55:53,250:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-31 22:55:53,250:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 22:55:53,297:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 22:55:53,297:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 22:55:53,297:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-31 22:55:53,375:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 22:55:53,417:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 22:55:53,423:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 22:55:53,485:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 22:55:53,533:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 22:55:53,533:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 22:55:53,533:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-03-31 22:55:53,643:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 22:55:53,658:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 22:55:53,768:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 22:55:53,768:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 22:55:53,784:INFO:Preparing preprocessing pipeline...
2023-03-31 22:55:53,784:INFO:Set up simple imputation.
2023-03-31 22:55:53,847:INFO:Set up encoding of ordinal features.
2023-03-31 22:55:53,862:INFO:Set up encoding of categorical features.
2023-03-31 22:55:53,878:INFO:Set up column name cleaning.
2023-03-31 22:56:05,970:INFO:Finished creating preprocessing pipeline.
2023-03-31 22:56:06,005:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Mujahid\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Days for shipping (real)',
                                             'Days for shipment (scheduled)',
                                             'Benefit per order',
                                             'Sales per customer',
                                             'Late_delivery_risk',
                                             'Category Id', 'Customer Id',
                                             'Department Id', 'Latitude',
                                             'Longitude', 'Order Customer Id',
                                             'Order Id...
                                                                         'Order '
                                                                         'State',
                                                                         'Product '
                                                                         'Image',
                                                                         'Product '
                                                                         'Name',
                                                                         'shipping '
                                                                         'date '
                                                                         '(DateOrders)'],
                                                                   drop_invariant=False,
                                                                   handle_missing='return_nan',
                                                                   handle_unknown='value',
                                                                   random_state=123,
                                                                   return_df=True,
                                                                   sigma=None,
                                                                   verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-03-31 22:56:06,005:INFO:Creating final display dataframe.
2023-03-31 22:56:14,618:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target   SUSPECTED_FRAUD
2                   Target type            Binary
3           Original data shape      (180519, 51)
4        Transformed data shape     (180519, 106)
5   Transformed train set shape     (126363, 106)
6    Transformed test set shape      (54156, 106)
7              Ordinal features                 1
8              Numeric features                26
9          Categorical features                24
10     Rows with missing values              0.0%
11                   Preprocess              True
12              Imputation type            simple
13           Numeric imputation              mean
14       Categorical imputation              mode
15     Maximum one-hot encoding                25
16              Encoding method              None
17               Fold Generator   StratifiedKFold
18                  Fold Number                10
19                     CPU Jobs                -1
20                      Use GPU             False
21               Log Experiment             False
22              Experiment Name  clf-default-name
23                          USI              ef9c
2023-03-31 22:56:14,699:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 22:56:14,699:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 22:56:14,771:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 22:56:14,779:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 22:56:14,779:INFO:setup() successfully completed in 24.6s...............
2023-03-31 22:56:14,779:INFO:Initializing create_model()
2023-03-31 22:56:14,779:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71CA83CA0>, estimator=lr, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={'class_weight': 'balanced'})
2023-03-31 22:56:14,779:INFO:Checking exceptions
2023-03-31 22:56:14,803:INFO:Importing libraries
2023-03-31 22:56:14,803:INFO:Copying training dataset
2023-03-31 22:56:14,907:INFO:Defining folds
2023-03-31 22:56:14,907:INFO:Declaring metric variables
2023-03-31 22:56:14,907:INFO:Importing untrained model
2023-03-31 22:56:14,915:INFO:Logistic Regression Imported successfully
2023-03-31 22:56:14,923:INFO:Starting cross validation
2023-03-31 22:56:14,923:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 22:56:25,440:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:25,676:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.29s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:26,524:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:26,747:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:26,778:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:27,039:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:27,556:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:27,895:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.06s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:28,151:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:28,762:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:28,968:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:28,979:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:30,310:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:30,981:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:31,476:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:31,492:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:32,913:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:33,498:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:33,522:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.06s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:33,586:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:33,696:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:33,838:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:33,979:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:34,999:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:35,934:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:36,328:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:36,424:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:36,590:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:36,769:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:36,905:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:37,256:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:37,640:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:39,849:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:40,278:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:40,482:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:40,734:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:40,790:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:41,035:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:41,298:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:41,631:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:44,425:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.18s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:45,745:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.43s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:45,893:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:46,131:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:46,283:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:47,050:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:47,278:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:47,295:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:50,736:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:52,004:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:52,131:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.16s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:52,430:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:52,445:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:53,322:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.18s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:53,418:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.06s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:53,602:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:56:55,354:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.21s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:56,954:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:57,449:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:57,777:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:58,161:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:58,301:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:58,649:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:58,793:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.34s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:58,857:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:58,945:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.42s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:59,169:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.35s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:59,281:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:56:59,761:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:57:00,063:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:57:00,063:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:58:27,035:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 22:58:28,216:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 22:58:30,772:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 22:58:31,286:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 22:58:31,334:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 22:58:31,384:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 22:58:31,440:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 22:58:31,664:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 22:58:31,914:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 22:58:32,777:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 22:58:33,158:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 22:58:34,293:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 22:58:34,989:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 22:58:35,323:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 22:58:35,402:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 22:58:35,554:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 22:58:35,554:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 22:58:37,251:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 22:58:41,700:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:58:41,984:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:58:44,010:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:58:44,288:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:58:47,214:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:58:47,523:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:58:50,229:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:58:50,433:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 22:58:53,731:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 22:59:24,155:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 22:59:24,202:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 22:59:26,746:INFO:Calculating mean and std
2023-03-31 22:59:26,777:INFO:Creating metrics dataframe
2023-03-31 22:59:26,809:INFO:Finalizing model
2023-03-31 22:59:52,948:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning:

lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression


2023-03-31 22:59:53,916:INFO:Uploading results into container
2023-03-31 22:59:53,916:INFO:Uploading model into container now
2023-03-31 22:59:53,932:INFO:_master_model_container: 1
2023-03-31 22:59:53,932:INFO:_display_container: 2
2023-03-31 22:59:53,932:INFO:LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-03-31 22:59:53,932:INFO:create_model() successfully completed......................................
2023-03-31 22:59:54,956:INFO:Initializing finalize_model()
2023-03-31 22:59:54,956:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71CA83CA0>, estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2023-03-31 22:59:54,956:INFO:Finalizing LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-03-31 22:59:55,035:INFO:Initializing create_model()
2023-03-31 22:59:55,035:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71CA83CA0>, estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, kwargs={})
2023-03-31 22:59:55,035:INFO:Checking exceptions
2023-03-31 22:59:55,035:INFO:Importing libraries
2023-03-31 22:59:55,035:INFO:Copying training dataset
2023-03-31 22:59:55,082:INFO:Defining folds
2023-03-31 22:59:55,082:INFO:Declaring metric variables
2023-03-31 22:59:55,082:INFO:Importing untrained model
2023-03-31 22:59:55,082:INFO:Declaring custom model
2023-03-31 22:59:55,082:INFO:Logistic Regression Imported successfully
2023-03-31 22:59:55,082:INFO:Cross validation set to False
2023-03-31 22:59:55,082:INFO:Fitting Model
2023-03-31 23:00:40,771:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning:

lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression


2023-03-31 23:00:40,859:INFO:Pipeline(memory=FastMemory(location=C:\Users\Mujahid\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Days for shipping (real)',
                                             'Days for shipment (scheduled)',
                                             'Benefit per order',
                                             'Sales per customer',
                                             'Late_delivery_risk',
                                             'Category Id', 'Customer Id',
                                             'Department Id', 'Latitude',
                                             'Longitude', 'Order Customer Id',
                                             'Order Id...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2023-03-31 23:00:40,859:INFO:create_model() successfully completed......................................
2023-03-31 23:00:41,177:INFO:_master_model_container: 1
2023-03-31 23:00:41,177:INFO:_display_container: 2
2023-03-31 23:00:41,209:INFO:Pipeline(memory=FastMemory(location=C:\Users\Mujahid\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Days for shipping (real)',
                                             'Days for shipment (scheduled)',
                                             'Benefit per order',
                                             'Sales per customer',
                                             'Late_delivery_risk',
                                             'Category Id', 'Customer Id',
                                             'Department Id', 'Latitude',
                                             'Longitude', 'Order Customer Id',
                                             'Order Id...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2023-03-31 23:00:41,209:INFO:finalize_model() successfully completed......................................
2023-03-31 23:00:41,518:INFO:Initializing predict_model()
2023-03-31 23:00:41,518:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71CA83CA0>, estimator=Pipeline(memory=FastMemory(location=C:\Users\Mujahid\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Days for shipping (real)',
                                             'Days for shipment (scheduled)',
                                             'Benefit per order',
                                             'Sales per customer',
                                             'Late_delivery_risk',
                                             'Category Id', 'Customer Id',
                                             'Department Id', 'Latitude',
                                             'Longitude', 'Order Customer Id',
                                             'Order Id...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x000001F725AFF040>)
2023-03-31 23:00:41,518:INFO:Checking exceptions
2023-03-31 23:00:41,518:INFO:Preloading libraries
2023-03-31 23:00:41,532:INFO:Set up data.
2023-03-31 23:00:42,209:INFO:Set up index.
2023-03-31 23:02:19,291:INFO:PyCaret ClassificationExperiment
2023-03-31 23:02:19,291:INFO:Logging name: clf-default-name
2023-03-31 23:02:19,291:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-31 23:02:19,291:INFO:version 3.0.0
2023-03-31 23:02:19,291:INFO:Initializing setup()
2023-03-31 23:02:19,291:INFO:self.USI: 99a7
2023-03-31 23:02:19,291:INFO:self._variable_keys: {'gpu_n_jobs_param', '_ml_usecase', 'y_train', 'USI', 'fold_shuffle_param', 'X_train', 'data', 'is_multiclass', 'seed', 'fold_generator', 'X', 'fold_groups_param', 'logging_param', '_available_plots', 'log_plots_param', 'idx', 'pipeline', 'gpu_param', 'html_param', 'y', 'fix_imbalance', 'X_test', 'exp_id', 'n_jobs_param', 'target_param', 'y_test', 'exp_name_log', 'memory'}
2023-03-31 23:02:19,291:INFO:Checking environment
2023-03-31 23:02:19,291:INFO:python_version: 3.9.12
2023-03-31 23:02:19,291:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-31 23:02:19,291:INFO:machine: AMD64
2023-03-31 23:02:19,291:INFO:platform: Windows-10-10.0.22624-SP0
2023-03-31 23:02:19,299:INFO:Memory: svmem(total=8378363904, available=1171902464, percent=86.0, used=7206461440, free=1171902464)
2023-03-31 23:02:19,299:INFO:Physical Core: 4
2023-03-31 23:02:19,299:INFO:Logical Core: 8
2023-03-31 23:02:19,299:INFO:Checking libraries
2023-03-31 23:02:19,299:INFO:System:
2023-03-31 23:02:19,299:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-31 23:02:19,299:INFO:executable: C:\Users\Mujahid\anaconda3\python.exe
2023-03-31 23:02:19,301:INFO:   machine: Windows-10-10.0.22624-SP0
2023-03-31 23:02:19,301:INFO:PyCaret required dependencies:
2023-03-31 23:02:19,301:INFO:                 pip: 23.0.1
2023-03-31 23:02:19,301:INFO:          setuptools: 61.2.0
2023-03-31 23:02:19,301:INFO:             pycaret: 3.0.0
2023-03-31 23:02:19,301:INFO:             IPython: 8.2.0
2023-03-31 23:02:19,301:INFO:          ipywidgets: 7.6.5
2023-03-31 23:02:19,301:INFO:                tqdm: 4.64.0
2023-03-31 23:02:19,301:INFO:               numpy: 1.21.5
2023-03-31 23:02:19,301:INFO:              pandas: 1.4.2
2023-03-31 23:02:19,301:INFO:              jinja2: 3.1.2
2023-03-31 23:02:19,301:INFO:               scipy: 1.7.3
2023-03-31 23:02:19,301:INFO:              joblib: 1.2.0
2023-03-31 23:02:19,301:INFO:             sklearn: 1.0.2
2023-03-31 23:02:19,301:INFO:                pyod: 1.0.8
2023-03-31 23:02:19,301:INFO:            imblearn: 0.10.1
2023-03-31 23:02:19,301:INFO:   category_encoders: 2.6.0
2023-03-31 23:02:19,301:INFO:            lightgbm: 3.3.5
2023-03-31 23:02:19,301:INFO:               numba: 0.55.1
2023-03-31 23:02:19,301:INFO:            requests: 2.27.1
2023-03-31 23:02:19,301:INFO:          matplotlib: 3.5.1
2023-03-31 23:02:19,301:INFO:          scikitplot: 0.3.7
2023-03-31 23:02:19,301:INFO:         yellowbrick: 1.5
2023-03-31 23:02:19,301:INFO:              plotly: 5.6.0
2023-03-31 23:02:19,301:INFO:             kaleido: 0.2.1
2023-03-31 23:02:19,301:INFO:         statsmodels: 0.13.2
2023-03-31 23:02:19,301:INFO:              sktime: 0.16.1
2023-03-31 23:02:19,301:INFO:               tbats: 1.1.2
2023-03-31 23:02:19,301:INFO:            pmdarima: 2.0.3
2023-03-31 23:02:19,301:INFO:              psutil: 5.9.4
2023-03-31 23:02:19,301:INFO:PyCaret optional dependencies:
2023-03-31 23:02:19,301:INFO:                shap: 0.41.0
2023-03-31 23:02:19,301:INFO:           interpret: Not installed
2023-03-31 23:02:19,301:INFO:                umap: 0.5.3
2023-03-31 23:02:19,301:INFO:    pandas_profiling: 4.0.0
2023-03-31 23:02:19,301:INFO:  explainerdashboard: Not installed
2023-03-31 23:02:19,301:INFO:             autoviz: Not installed
2023-03-31 23:02:19,301:INFO:           fairlearn: Not installed
2023-03-31 23:02:19,301:INFO:             xgboost: 1.7.4
2023-03-31 23:02:19,301:INFO:            catboost: 1.1.1
2023-03-31 23:02:19,301:INFO:              kmodes: 0.12.2
2023-03-31 23:02:19,301:INFO:             mlxtend: 0.21.0
2023-03-31 23:02:19,301:INFO:       statsforecast: Not installed
2023-03-31 23:02:19,301:INFO:        tune_sklearn: Not installed
2023-03-31 23:02:19,301:INFO:                 ray: Not installed
2023-03-31 23:02:19,301:INFO:            hyperopt: Not installed
2023-03-31 23:02:19,301:INFO:              optuna: 3.1.0
2023-03-31 23:02:19,301:INFO:               skopt: Not installed
2023-03-31 23:02:19,301:INFO:              mlflow: 2.2.2
2023-03-31 23:02:19,301:INFO:              gradio: Not installed
2023-03-31 23:02:19,301:INFO:             fastapi: 0.95.0
2023-03-31 23:02:19,301:INFO:             uvicorn: 0.21.1
2023-03-31 23:02:19,301:INFO:              m2cgen: Not installed
2023-03-31 23:02:19,301:INFO:           evidently: Not installed
2023-03-31 23:02:19,301:INFO:               fugue: Not installed
2023-03-31 23:02:19,301:INFO:           streamlit: 1.20.0
2023-03-31 23:02:19,301:INFO:             prophet: 1.1.2
2023-03-31 23:02:19,301:INFO:None
2023-03-31 23:02:19,301:INFO:Set up data.
2023-03-31 23:02:19,977:INFO:Set up train/test split.
2023-03-31 23:02:20,166:INFO:Set up index.
2023-03-31 23:02:20,166:INFO:Set up folding strategy.
2023-03-31 23:02:20,166:INFO:Assigning column types.
2023-03-31 23:02:20,213:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-31 23:02:20,260:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-31 23:02:20,260:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 23:02:20,291:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 23:02:20,291:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 23:02:20,338:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-31 23:02:20,338:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 23:02:20,354:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 23:02:20,370:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 23:02:20,370:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-31 23:02:20,402:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 23:02:20,433:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 23:02:20,433:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 23:02:20,479:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 23:02:20,503:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 23:02:20,503:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 23:02:20,503:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-03-31 23:02:20,574:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 23:02:20,574:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 23:02:20,645:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 23:02:20,653:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 23:02:20,653:INFO:Preparing preprocessing pipeline...
2023-03-31 23:02:20,661:INFO:Set up simple imputation.
2023-03-31 23:02:20,733:INFO:Set up encoding of ordinal features.
2023-03-31 23:02:20,749:INFO:Set up encoding of categorical features.
2023-03-31 23:02:20,749:INFO:Set up imbalanced handling.
2023-03-31 23:02:20,757:INFO:Set up column name cleaning.
2023-03-31 23:02:33,203:INFO:Finished creating preprocessing pipeline.
2023-03-31 23:02:33,231:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Mujahid\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Days for shipping (real)',
                                             'Days for shipment (scheduled)',
                                             'Benefit per order',
                                             'Sales per customer',
                                             'Late_delivery_risk',
                                             'Category Id', 'Customer Id',
                                             'Department Id', 'Latitude',
                                             'Longitude', 'Order Customer Id',
                                             'Order Id...
                                                                   return_df=True,
                                                                   sigma=None,
                                                                   verbose=0))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-03-31 23:02:33,231:INFO:Creating final display dataframe.
2023-03-31 23:02:44,468:INFO:Setup _display_container:                     Description             Value
0                    Session id              2022
1                        Target   SUSPECTED_FRAUD
2                   Target type            Binary
3           Original data shape      (180519, 51)
4        Transformed data shape     (301196, 106)
5   Transformed train set shape     (247040, 106)
6    Transformed test set shape      (54156, 106)
7              Ordinal features                 1
8              Numeric features                26
9          Categorical features                24
10     Rows with missing values              0.0%
11                   Preprocess              True
12              Imputation type            simple
13           Numeric imputation              mean
14       Categorical imputation              mode
15     Maximum one-hot encoding                25
16              Encoding method              None
17                Fix imbalance              True
18         Fix imbalance method             SMOTE
19               Fold Generator   StratifiedKFold
20                  Fold Number                10
21                     CPU Jobs                -1
22                      Use GPU             False
23               Log Experiment             False
24              Experiment Name  clf-default-name
25                          USI              99a7
2023-03-31 23:02:44,546:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 23:02:44,546:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 23:02:44,608:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 23:02:44,608:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 23:02:44,608:INFO:setup() successfully completed in 26.22s...............
2023-03-31 23:03:06,985:INFO:Initializing compare_models()
2023-03-31 23:03:06,985:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F723592610>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001F723592610>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-03-31 23:03:06,985:INFO:Checking exceptions
2023-03-31 23:03:07,039:INFO:Preparing display monitor
2023-03-31 23:03:07,082:INFO:Initializing Logistic Regression
2023-03-31 23:03:07,082:INFO:Total runtime is 0.0 minutes
2023-03-31 23:03:07,082:INFO:SubProcess create_model() called ==================================
2023-03-31 23:03:07,082:INFO:Initializing create_model()
2023-03-31 23:03:07,082:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F723592610>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F725AF0FD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 23:03:07,082:INFO:Checking exceptions
2023-03-31 23:03:07,082:INFO:Importing libraries
2023-03-31 23:03:07,082:INFO:Copying training dataset
2023-03-31 23:03:07,180:INFO:Defining folds
2023-03-31 23:03:07,180:INFO:Declaring metric variables
2023-03-31 23:03:07,180:INFO:Importing untrained model
2023-03-31 23:03:07,180:INFO:Logistic Regression Imported successfully
2023-03-31 23:03:07,198:INFO:Starting cross validation
2023-03-31 23:03:07,201:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 23:03:08,746:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:09,071:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:09,338:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:12,265:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:12,704:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:12,846:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:14,122:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:14,201:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:14,296:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:14,454:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:14,572:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:14,588:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:14,724:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:15,895:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:16,398:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:16,429:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:16,646:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:16,718:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:16,726:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:16,820:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:16,872:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:19,370:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:20,100:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:20,100:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:20,156:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:20,292:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:20,320:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:20,404:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:20,436:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:24,530:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 2.32s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:25,931:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.01s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:26,277:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:26,285:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:26,412:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:26,412:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:26,567:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:26,696:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:31,535:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:32,229:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:32,898:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.01s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:33,046:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:33,093:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:33,148:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:33,180:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:33,211:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:36,573:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.43s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:37,053:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.28s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:37,427:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:38,401:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.41s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:38,675:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:38,839:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.21s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:38,879:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.21s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:38,942:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.20s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:39,257:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.25s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:39,768:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:46,492:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:47,738:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:49,062:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:49,689:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:49,858:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:50,216:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:50,359:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:51,276:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:51,843:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:51,999:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:03:52,701:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:03:52,874:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:06:58,419:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 23:06:58,810:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 23:06:59,354:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 23:07:00,306:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 23:07:00,891:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 23:07:01,778:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 23:07:02,026:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 23:07:03,162:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:07:03,274:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 23:07:03,506:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:07:03,574:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:07:04,593:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:07:05,233:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:07:05,529:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:07:05,897:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:07:07,184:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:07:08,033:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:07:09,040:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:07:13,288:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:07:14,567:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:07:15,152:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:07:17,112:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:07:17,687:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:07:19,431:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:07:22,202:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:07:24,438:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:07:24,992:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:27,166:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 23:08:28,672:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-03-31 23:08:30,220:INFO:Calculating mean and std
2023-03-31 23:08:30,251:INFO:Creating metrics dataframe
2023-03-31 23:08:31,397:INFO:Uploading results into container
2023-03-31 23:08:31,400:INFO:Uploading model into container now
2023-03-31 23:08:31,400:INFO:_master_model_container: 1
2023-03-31 23:08:31,400:INFO:_display_container: 2
2023-03-31 23:08:31,400:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=2022, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-03-31 23:08:31,400:INFO:create_model() successfully completed......................................
2023-03-31 23:08:32,809:INFO:SubProcess create_model() end ==================================
2023-03-31 23:08:32,809:INFO:Creating metrics dataframe
2023-03-31 23:08:32,825:INFO:Initializing K Neighbors Classifier
2023-03-31 23:08:32,825:INFO:Total runtime is 5.429052750269572 minutes
2023-03-31 23:08:32,840:INFO:SubProcess create_model() called ==================================
2023-03-31 23:08:32,840:INFO:Initializing create_model()
2023-03-31 23:08:32,840:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F723592610>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F725AF0FD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 23:08:32,840:INFO:Checking exceptions
2023-03-31 23:08:32,840:INFO:Importing libraries
2023-03-31 23:08:32,840:INFO:Copying training dataset
2023-03-31 23:08:33,029:INFO:Defining folds
2023-03-31 23:08:33,029:INFO:Declaring metric variables
2023-03-31 23:08:33,044:INFO:Importing untrained model
2023-03-31 23:08:33,044:INFO:K Neighbors Classifier Imported successfully
2023-03-31 23:08:33,060:INFO:Starting cross validation
2023-03-31 23:08:33,076:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 23:08:36,311:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:08:36,420:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:08:36,780:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:08:36,796:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:08:37,204:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.18s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:08:37,309:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:08:37,424:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:08:38,615:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.44s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:08:38,773:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:39,275:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.22s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:39,275:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:39,511:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:39,793:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:40,091:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:40,138:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:40,986:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:45,478:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:45,623:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.46s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:45,870:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.49s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:45,995:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.47s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:46,403:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:46,513:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:46,591:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:46,968:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:51,502:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.43s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:52,114:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.28s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:52,538:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.29s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:52,600:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.35s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:52,820:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.33s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:53,087:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.32s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:53,353:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.46s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:53,573:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:08:53,729:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.33s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:53,886:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:08:54,562:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:08:55,472:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:55,660:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:58,085:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:58,274:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:58,289:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:58,345:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 23:08:58,352:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 23:08:58,368:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:58,446:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:08:58,870:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 23:08:59,451:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 23:08:59,812:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 23:08:59,843:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 23:08:59,937:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 23:08:59,953:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 23:09:02,117:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 23:09:02,196:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 23:09:02,352:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 23:09:02,932:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 23:09:03,309:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 23:09:03,388:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.22s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:09:03,434:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 23:09:03,497:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.25s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:09:03,686:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:09:05,258:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:09:06,861:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.49s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:09:06,902:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 3.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 23:09:07,671:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 3.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 23:09:08,664:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 23:09:08,709:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 23:09:44,162:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py", line 214, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (603, 27792) and data type float64

  warnings.warn(

2023-03-31 23:09:55,526:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 46.49s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:09:56,018:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 47.00s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:09:56,396:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py", line 214, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\extmath.py", line 153, in safe_sparse_dot
    ret = a @ b
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (603, 27792) and data type float64

  warnings.warn(

2023-03-31 23:09:56,507:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py", line 214, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\extmath.py", line 153, in safe_sparse_dot
    ret = a @ b
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (603, 27792) and data type float64

  warnings.warn(

2023-03-31 23:09:56,761:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 53.23s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:09:56,966:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py", line 214, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1434, in _parallel_pairwise
    ret = np.empty((X.shape[0], Y.shape[0]), dtype=dtype, order="F")
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1023. MiB for an array with shape (603, 222336) and data type float64

  warnings.warn(

2023-03-31 23:10:19,134:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py", line 214, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (603, 27792) and data type float64

  warnings.warn(

2023-03-31 23:16:21,488:WARNING:create_model() for knn raised an exception or returned all 0.0, trying without fit_kwargs:
2023-03-31 23:16:21,531:WARNING:joblib.externals.loky.process_executor._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\externals\loky\backend\queues.py", line 125, in _feed
    obj_ = dumps(obj, reducers=reducers)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\externals\loky\backend\reduction.py", line 211, in dumps
    dump(obj, buf, reducers=reducers, protocol=protocol)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\externals\loky\backend\reduction.py", line 204, in dump
    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\externals\cloudpickle\cloudpickle_fast.py", line 632, in dump
    return Pickler.dump(self, obj)
MemoryError
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1523, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1117, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 267, in cross_validate
    results = parallel(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 567, in wrap_future_result
    return future.result(timeout=timeout)
  File "C:\Users\Mujahid\anaconda3\lib\concurrent\futures\_base.py", line 439, in result
    return self.__get_result()
  File "C:\Users\Mujahid\anaconda3\lib\concurrent\futures\_base.py", line 391, in __get_result
    raise self._exception
_pickle.PicklingError: Could not pickle the task to send it to the workers.

2023-03-31 23:16:21,559:INFO:Initializing create_model()
2023-03-31 23:16:21,559:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F723592610>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F725AF0FD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 23:16:21,559:INFO:Checking exceptions
2023-03-31 23:16:21,559:INFO:Importing libraries
2023-03-31 23:16:21,571:INFO:Copying training dataset
2023-03-31 23:16:22,007:INFO:Defining folds
2023-03-31 23:16:22,007:INFO:Declaring metric variables
2023-03-31 23:16:22,062:INFO:Importing untrained model
2023-03-31 23:16:22,064:INFO:K Neighbors Classifier Imported successfully
2023-03-31 23:16:22,079:INFO:Starting cross validation
2023-03-31 23:16:22,103:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-31 23:16:34,882:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:16:34,898:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:16:41,779:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:16:42,298:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:17:41,610:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py", line 214, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1434, in _parallel_pairwise
    ret = np.empty((X.shape[0], Y.shape[0]), dtype=dtype, order="F")
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1023. MiB for an array with shape (603, 222336) and data type float64

  warnings.warn(

2023-03-31 23:17:41,751:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py", line 214, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1434, in _parallel_pairwise
    ret = np.empty((X.shape[0], Y.shape[0]), dtype=dtype, order="F")
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1023. MiB for an array with shape (603, 222336) and data type float64

  warnings.warn(

2023-03-31 23:17:41,877:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py", line 214, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (603, 27792) and data type float64

  warnings.warn(

2023-03-31 23:17:53,855:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.46s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:17:55,068:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.16s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-31 23:18:01,223:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 4.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:18:01,740:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.34s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:18:07,374:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:18:08,097:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:18:18,130:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 4.01s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:18:18,421:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 4.36s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:18:25,472:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:18:25,730:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-31 23:18:26,869:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 23:18:27,646:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-31 23:18:29,303:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 23:18:32,678:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 23:18:32,946:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-31 23:18:33,743:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.00s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:18:33,917:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-31 23:18:34,319:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py", line 214, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\extmath.py", line 153, in safe_sparse_dot
    ret = a @ b
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (603, 27792) and data type float64

  warnings.warn(

2023-03-31 23:18:34,492:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py", line 214, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\extmath.py", line 153, in safe_sparse_dot
    ret = a @ b
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (603, 27792) and data type float64

  warnings.warn(

2023-03-31 23:18:34,651:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py", line 214, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 752, in kneighbors
    chunked_results = list(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1717, in pairwise_distances_chunked
    D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1889, in pairwise_distances
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1435, in _parallel_pairwise
    Parallel(backend="threading", n_jobs=n_jobs)(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 1098, in __call__
    self.retrieve()
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 975, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 771, in get
    raise self._value
  File "C:\Users\Mujahid\anaconda3\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\_parallel_backends.py", line 620, in __call__
    return self.func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in __call__
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\joblib\parallel.py", line 288, in <listcomp>
    return [func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\fixes.py", line 216, in __call__
    return self.function(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 1418, in _dist_wrapper
    dist_matrix[:, slice_] = dist_func(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 330, in euclidean_distances
    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\pairwise.py", line 371, in _euclidean_distances
    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 128. MiB for an array with shape (603, 27792) and data type float64

  warnings.warn(

2023-03-31 23:58:33,427:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 23:58:33,427:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 23:58:33,427:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 23:58:33,427:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-31 23:58:34,920:INFO:Soft dependency imported: prophet: 1.1.2
2023-03-31 23:58:55,314:INFO:PyCaret ClassificationExperiment
2023-03-31 23:58:55,314:INFO:Logging name: clf-default-name
2023-03-31 23:58:55,314:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-31 23:58:55,314:INFO:version 3.0.0
2023-03-31 23:58:55,314:INFO:Initializing setup()
2023-03-31 23:58:55,314:INFO:self.USI: ee80
2023-03-31 23:58:55,314:INFO:self._variable_keys: {'log_plots_param', 'pipeline', 'fold_generator', 'X', 'data', 'idx', 'X_test', 'n_jobs_param', 'fold_shuffle_param', 'X_train', 'y_train', 'fix_imbalance', 'exp_id', 'seed', '_ml_usecase', 'html_param', 'gpu_n_jobs_param', 'gpu_param', 'fold_groups_param', 'exp_name_log', 'y', 'USI', 'y_test', '_available_plots', 'is_multiclass', 'logging_param', 'target_param', 'memory'}
2023-03-31 23:58:55,314:INFO:Checking environment
2023-03-31 23:58:55,314:INFO:python_version: 3.9.12
2023-03-31 23:58:55,314:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-31 23:58:55,314:INFO:machine: AMD64
2023-03-31 23:58:55,314:INFO:platform: Windows-10-10.0.22624-SP0
2023-03-31 23:58:55,314:INFO:Memory: svmem(total=8378363904, available=672202752, percent=92.0, used=7706161152, free=672202752)
2023-03-31 23:58:55,314:INFO:Physical Core: 4
2023-03-31 23:58:55,314:INFO:Logical Core: 8
2023-03-31 23:58:55,314:INFO:Checking libraries
2023-03-31 23:58:55,314:INFO:System:
2023-03-31 23:58:55,314:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-31 23:58:55,314:INFO:executable: C:\Users\Mujahid\anaconda3\python.exe
2023-03-31 23:58:55,314:INFO:   machine: Windows-10-10.0.22624-SP0
2023-03-31 23:58:55,314:INFO:PyCaret required dependencies:
2023-03-31 23:58:55,314:INFO:                 pip: 23.0.1
2023-03-31 23:58:55,314:INFO:          setuptools: 61.2.0
2023-03-31 23:58:55,314:INFO:             pycaret: 3.0.0
2023-03-31 23:58:55,314:INFO:             IPython: 8.2.0
2023-03-31 23:58:55,314:INFO:          ipywidgets: 7.6.5
2023-03-31 23:58:55,314:INFO:                tqdm: 4.64.0
2023-03-31 23:58:55,314:INFO:               numpy: 1.21.5
2023-03-31 23:58:55,314:INFO:              pandas: 1.4.2
2023-03-31 23:58:55,314:INFO:              jinja2: 3.1.2
2023-03-31 23:58:55,314:INFO:               scipy: 1.7.3
2023-03-31 23:58:55,314:INFO:              joblib: 1.2.0
2023-03-31 23:58:55,314:INFO:             sklearn: 1.0.2
2023-03-31 23:58:55,314:INFO:                pyod: 1.0.8
2023-03-31 23:58:55,314:INFO:            imblearn: 0.10.1
2023-03-31 23:58:55,314:INFO:   category_encoders: 2.6.0
2023-03-31 23:58:55,314:INFO:            lightgbm: 3.3.5
2023-03-31 23:58:55,314:INFO:               numba: 0.55.1
2023-03-31 23:58:55,314:INFO:            requests: 2.27.1
2023-03-31 23:58:55,314:INFO:          matplotlib: 3.5.1
2023-03-31 23:58:55,314:INFO:          scikitplot: 0.3.7
2023-03-31 23:58:55,314:INFO:         yellowbrick: 1.5
2023-03-31 23:58:55,314:INFO:              plotly: 5.6.0
2023-03-31 23:58:55,314:INFO:             kaleido: 0.2.1
2023-03-31 23:58:55,314:INFO:         statsmodels: 0.13.2
2023-03-31 23:58:55,314:INFO:              sktime: 0.16.1
2023-03-31 23:58:55,314:INFO:               tbats: 1.1.2
2023-03-31 23:58:55,314:INFO:            pmdarima: 2.0.3
2023-03-31 23:58:55,314:INFO:              psutil: 5.9.4
2023-03-31 23:58:55,314:INFO:PyCaret optional dependencies:
2023-03-31 23:58:55,503:INFO:                shap: 0.41.0
2023-03-31 23:58:55,503:INFO:           interpret: Not installed
2023-03-31 23:58:55,503:INFO:                umap: 0.5.3
2023-03-31 23:58:55,503:INFO:    pandas_profiling: 4.0.0
2023-03-31 23:58:55,503:INFO:  explainerdashboard: Not installed
2023-03-31 23:58:55,503:INFO:             autoviz: Not installed
2023-03-31 23:58:55,503:INFO:           fairlearn: Not installed
2023-03-31 23:58:55,503:INFO:             xgboost: 1.7.4
2023-03-31 23:58:55,503:INFO:            catboost: 1.1.1
2023-03-31 23:58:55,503:INFO:              kmodes: 0.12.2
2023-03-31 23:58:55,503:INFO:             mlxtend: 0.21.0
2023-03-31 23:58:55,503:INFO:       statsforecast: Not installed
2023-03-31 23:58:55,503:INFO:        tune_sklearn: Not installed
2023-03-31 23:58:55,503:INFO:                 ray: Not installed
2023-03-31 23:58:55,503:INFO:            hyperopt: Not installed
2023-03-31 23:58:55,503:INFO:              optuna: 3.1.0
2023-03-31 23:58:55,503:INFO:               skopt: Not installed
2023-03-31 23:58:55,503:INFO:              mlflow: 2.2.2
2023-03-31 23:58:55,503:INFO:              gradio: Not installed
2023-03-31 23:58:55,503:INFO:             fastapi: 0.95.0
2023-03-31 23:58:55,503:INFO:             uvicorn: 0.21.1
2023-03-31 23:58:55,504:INFO:              m2cgen: Not installed
2023-03-31 23:58:55,504:INFO:           evidently: Not installed
2023-03-31 23:58:55,504:INFO:               fugue: Not installed
2023-03-31 23:58:55,504:INFO:           streamlit: 1.20.0
2023-03-31 23:58:55,504:INFO:             prophet: 1.1.2
2023-03-31 23:58:55,504:INFO:None
2023-03-31 23:58:55,504:INFO:Set up data.
2023-03-31 23:58:55,700:INFO:Set up train/test split.
2023-03-31 23:58:55,856:INFO:Set up index.
2023-03-31 23:58:55,859:INFO:Set up folding strategy.
2023-03-31 23:58:55,859:INFO:Assigning column types.
2023-03-31 23:58:55,954:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-31 23:58:55,996:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-31 23:58:55,998:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 23:58:56,022:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 23:58:56,132:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 23:58:56,288:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-31 23:58:56,288:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 23:58:56,335:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 23:58:56,335:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 23:58:56,335:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-31 23:58:56,418:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 23:58:56,461:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 23:58:56,465:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 23:58:56,537:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-03-31 23:58:56,580:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 23:58:56,587:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 23:58:56,588:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-03-31 23:58:56,703:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 23:58:56,709:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 23:58:56,818:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 23:58:56,818:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 23:58:56,818:INFO:Preparing preprocessing pipeline...
2023-03-31 23:58:56,833:INFO:Set up simple imputation.
2023-03-31 23:58:56,833:INFO:Set up imbalanced handling.
2023-03-31 23:58:56,849:INFO:Set up column name cleaning.
2023-03-31 23:58:59,285:INFO:Finished creating preprocessing pipeline.
2023-03-31 23:58:59,285:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Mujahid\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Type', 'Benefit per order',
                                             'Sales per customer',
                                             'Delivery Status',
                                             'Late_delivery_risk',
                                             'Category Name', 'Customer City',
                                             'Customer Country', 'Customer Id',
                                             'Customer Segment',
                                             'Customer State',
                                             'Department Name', 'Latitu...
                                                              strategy='most_frequent',
                                                              verbose=0))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-03-31 23:58:59,285:INFO:Creating final display dataframe.
2023-03-31 23:59:01,219:INFO:Setup _display_container:                     Description             Value
0                    Session id              7894
1                        Target   SUSPECTED_FRAUD
2                   Target type            Binary
3           Original data shape      (180519, 43)
4        Transformed data shape      (301196, 43)
5   Transformed train set shape      (247040, 43)
6    Transformed test set shape       (54156, 43)
7              Numeric features                42
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                Fix imbalance              True
13         Fix imbalance method             SMOTE
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              ee80
2023-03-31 23:59:01,356:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 23:59:01,356:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 23:59:01,482:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-31 23:59:01,482:INFO:Soft dependency imported: catboost: 1.1.1
2023-03-31 23:59:01,482:INFO:setup() successfully completed in 8.04s...............
2023-03-31 23:59:01,502:INFO:Initializing compare_models()
2023-03-31 23:59:01,502:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244D1137640>, include=None, fold=5, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000244D1137640>, 'include': None, 'exclude': None, 'fold': 5, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-03-31 23:59:01,502:INFO:Checking exceptions
2023-03-31 23:59:01,566:INFO:Preparing display monitor
2023-03-31 23:59:01,610:INFO:Initializing Logistic Regression
2023-03-31 23:59:01,610:INFO:Total runtime is 0.0 minutes
2023-03-31 23:59:01,613:INFO:SubProcess create_model() called ==================================
2023-03-31 23:59:01,614:INFO:Initializing create_model()
2023-03-31 23:59:01,614:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244D1137640>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000244C1F687F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-31 23:59:01,614:INFO:Checking exceptions
2023-03-31 23:59:01,614:INFO:Importing libraries
2023-03-31 23:59:01,614:INFO:Copying training dataset
2023-03-31 23:59:01,723:INFO:Defining folds
2023-03-31 23:59:01,723:INFO:Declaring metric variables
2023-03-31 23:59:01,723:INFO:Importing untrained model
2023-03-31 23:59:01,723:INFO:Logistic Regression Imported successfully
2023-03-31 23:59:01,738:INFO:Starting cross validation
2023-03-31 23:59:01,738:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 00:00:01,538:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-01 00:00:02,503:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-01 00:00:02,725:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-01 00:00:02,852:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-01 00:00:03,154:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-01 00:00:07,768:INFO:Calculating mean and std
2023-04-01 00:00:07,768:INFO:Creating metrics dataframe
2023-04-01 00:00:09,419:INFO:Uploading results into container
2023-04-01 00:00:09,419:INFO:Uploading model into container now
2023-04-01 00:00:09,419:INFO:_master_model_container: 1
2023-04-01 00:00:09,419:INFO:_display_container: 2
2023-04-01 00:00:09,419:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=7894, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-01 00:00:09,419:INFO:create_model() successfully completed......................................
2023-04-01 00:00:15,620:INFO:SubProcess create_model() end ==================================
2023-04-01 00:00:15,620:INFO:Creating metrics dataframe
2023-04-01 00:00:15,636:INFO:Initializing K Neighbors Classifier
2023-04-01 00:00:15,636:INFO:Total runtime is 1.2337606549263 minutes
2023-04-01 00:00:15,645:INFO:SubProcess create_model() called ==================================
2023-04-01 00:00:15,646:INFO:Initializing create_model()
2023-04-01 00:00:15,646:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244D1137640>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000244C1F687F0>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 00:00:15,646:INFO:Checking exceptions
2023-04-01 00:00:15,646:INFO:Importing libraries
2023-04-01 00:00:15,646:INFO:Copying training dataset
2023-04-01 00:00:15,762:INFO:Defining folds
2023-04-01 00:00:15,762:INFO:Declaring metric variables
2023-04-01 00:00:15,766:INFO:Importing untrained model
2023-04-01 00:00:15,770:INFO:K Neighbors Classifier Imported successfully
2023-04-01 00:00:15,770:INFO:Starting cross validation
2023-04-01 00:00:15,780:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 00:00:41,803:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 00:00:41,928:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 00:51:01,851:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-01 00:51:01,853:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-01 00:51:01,853:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-01 00:51:01,853:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-01 00:51:02,928:INFO:Soft dependency imported: prophet: 1.1.2
2023-04-01 00:51:07,832:INFO:PyCaret ClassificationExperiment
2023-04-01 00:51:07,832:INFO:Logging name: clf-default-name
2023-04-01 00:51:07,832:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-04-01 00:51:07,832:INFO:version 3.0.0
2023-04-01 00:51:07,832:INFO:Initializing setup()
2023-04-01 00:51:07,832:INFO:self.USI: 609b
2023-04-01 00:51:07,833:INFO:self._variable_keys: {'fix_imbalance', 'log_plots_param', 'X', 'is_multiclass', 'n_jobs_param', 'y', '_available_plots', 'logging_param', 'pipeline', 'gpu_n_jobs_param', 'html_param', 'memory', 'X_train', 'y_test', 'fold_shuffle_param', 'fold_groups_param', '_ml_usecase', 'data', 'idx', 'X_test', 'fold_generator', 'y_train', 'seed', 'USI', 'target_param', 'gpu_param', 'exp_name_log', 'exp_id'}
2023-04-01 00:51:07,833:INFO:Checking environment
2023-04-01 00:51:07,833:INFO:python_version: 3.9.12
2023-04-01 00:51:07,833:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-04-01 00:51:07,833:INFO:machine: AMD64
2023-04-01 00:51:07,833:INFO:platform: Windows-10-10.0.22624-SP0
2023-04-01 00:51:07,836:INFO:Memory: svmem(total=8378363904, available=198131712, percent=97.6, used=8180232192, free=198131712)
2023-04-01 00:51:07,836:INFO:Physical Core: 4
2023-04-01 00:51:07,836:INFO:Logical Core: 8
2023-04-01 00:51:07,836:INFO:Checking libraries
2023-04-01 00:51:07,836:INFO:System:
2023-04-01 00:51:07,836:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-04-01 00:51:07,836:INFO:executable: C:\Users\Mujahid\anaconda3\python.exe
2023-04-01 00:51:07,836:INFO:   machine: Windows-10-10.0.22624-SP0
2023-04-01 00:51:07,836:INFO:PyCaret required dependencies:
2023-04-01 00:51:07,837:INFO:                 pip: 23.0.1
2023-04-01 00:51:07,837:INFO:          setuptools: 61.2.0
2023-04-01 00:51:07,837:INFO:             pycaret: 3.0.0
2023-04-01 00:51:07,837:INFO:             IPython: 8.2.0
2023-04-01 00:51:07,837:INFO:          ipywidgets: 7.6.5
2023-04-01 00:51:07,837:INFO:                tqdm: 4.64.0
2023-04-01 00:51:07,837:INFO:               numpy: 1.21.5
2023-04-01 00:51:07,837:INFO:              pandas: 1.4.2
2023-04-01 00:51:07,837:INFO:              jinja2: 3.1.2
2023-04-01 00:51:07,837:INFO:               scipy: 1.7.3
2023-04-01 00:51:07,837:INFO:              joblib: 1.2.0
2023-04-01 00:51:07,837:INFO:             sklearn: 1.0.2
2023-04-01 00:51:07,837:INFO:                pyod: 1.0.8
2023-04-01 00:51:07,837:INFO:            imblearn: 0.10.1
2023-04-01 00:51:07,837:INFO:   category_encoders: 2.6.0
2023-04-01 00:51:07,837:INFO:            lightgbm: 3.3.5
2023-04-01 00:51:07,838:INFO:               numba: 0.55.1
2023-04-01 00:51:07,838:INFO:            requests: 2.27.1
2023-04-01 00:51:07,838:INFO:          matplotlib: 3.5.1
2023-04-01 00:51:07,838:INFO:          scikitplot: 0.3.7
2023-04-01 00:51:07,838:INFO:         yellowbrick: 1.5
2023-04-01 00:51:07,838:INFO:              plotly: 5.6.0
2023-04-01 00:51:07,838:INFO:             kaleido: 0.2.1
2023-04-01 00:51:07,838:INFO:         statsmodels: 0.13.2
2023-04-01 00:51:07,838:INFO:              sktime: 0.16.1
2023-04-01 00:51:07,838:INFO:               tbats: 1.1.2
2023-04-01 00:51:07,838:INFO:            pmdarima: 2.0.3
2023-04-01 00:51:07,838:INFO:              psutil: 5.9.4
2023-04-01 00:51:07,838:INFO:PyCaret optional dependencies:
2023-04-01 00:51:08,085:INFO:                shap: 0.41.0
2023-04-01 00:51:08,085:INFO:           interpret: Not installed
2023-04-01 00:51:08,086:INFO:                umap: 0.5.3
2023-04-01 00:51:08,086:INFO:    pandas_profiling: 4.0.0
2023-04-01 00:51:08,086:INFO:  explainerdashboard: Not installed
2023-04-01 00:51:08,086:INFO:             autoviz: Not installed
2023-04-01 00:51:08,086:INFO:           fairlearn: Not installed
2023-04-01 00:51:08,086:INFO:             xgboost: 1.7.4
2023-04-01 00:51:08,086:INFO:            catboost: 1.1.1
2023-04-01 00:51:08,086:INFO:              kmodes: 0.12.2
2023-04-01 00:51:08,086:INFO:             mlxtend: 0.21.0
2023-04-01 00:51:08,086:INFO:       statsforecast: Not installed
2023-04-01 00:51:08,086:INFO:        tune_sklearn: Not installed
2023-04-01 00:51:08,086:INFO:                 ray: Not installed
2023-04-01 00:51:08,086:INFO:            hyperopt: Not installed
2023-04-01 00:51:08,086:INFO:              optuna: 3.1.0
2023-04-01 00:51:08,086:INFO:               skopt: Not installed
2023-04-01 00:51:08,086:INFO:              mlflow: 2.2.2
2023-04-01 00:51:08,086:INFO:              gradio: Not installed
2023-04-01 00:51:08,086:INFO:             fastapi: 0.95.0
2023-04-01 00:51:08,086:INFO:             uvicorn: 0.21.1
2023-04-01 00:51:08,086:INFO:              m2cgen: Not installed
2023-04-01 00:51:08,087:INFO:           evidently: Not installed
2023-04-01 00:51:08,087:INFO:               fugue: Not installed
2023-04-01 00:51:08,087:INFO:           streamlit: 1.20.0
2023-04-01 00:51:08,087:INFO:             prophet: 1.1.2
2023-04-01 00:51:08,087:INFO:None
2023-04-01 00:51:08,087:INFO:Set up data.
2023-04-01 00:51:08,428:INFO:Set up train/test split.
2023-04-01 00:51:08,877:INFO:Set up index.
2023-04-01 00:51:08,884:INFO:Set up folding strategy.
2023-04-01 00:51:08,886:INFO:Assigning column types.
2023-04-01 00:51:09,148:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-04-01 00:51:09,288:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-01 00:51:09,300:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-01 00:51:09,397:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-01 00:51:09,585:INFO:Soft dependency imported: catboost: 1.1.1
2023-04-01 00:51:09,838:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-01 00:51:09,841:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-01 00:51:09,909:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-01 00:51:09,915:INFO:Soft dependency imported: catboost: 1.1.1
2023-04-01 00:51:09,917:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-04-01 00:51:10,031:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-01 00:51:10,104:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-01 00:51:10,112:INFO:Soft dependency imported: catboost: 1.1.1
2023-04-01 00:51:10,227:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-01 00:51:10,297:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-01 00:51:10,303:INFO:Soft dependency imported: catboost: 1.1.1
2023-04-01 00:51:10,304:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-04-01 00:51:10,471:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-01 00:51:10,478:INFO:Soft dependency imported: catboost: 1.1.1
2023-04-01 00:51:10,663:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-01 00:51:10,670:INFO:Soft dependency imported: catboost: 1.1.1
2023-04-01 00:51:10,678:INFO:Preparing preprocessing pipeline...
2023-04-01 00:51:10,710:INFO:Set up simple imputation.
2023-04-01 00:51:10,711:INFO:Set up imbalanced handling.
2023-04-01 00:51:10,742:INFO:Set up column name cleaning.
2023-04-01 00:51:13,059:INFO:Finished creating preprocessing pipeline.
2023-04-01 00:51:13,069:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Mujahid\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Type', 'Benefit per order',
                                             'Sales per customer',
                                             'Delivery Status',
                                             'Late_delivery_risk',
                                             'Category Name', 'Customer City',
                                             'Customer Country', 'Customer Id',
                                             'Customer Segment',
                                             'Customer State',
                                             'Department Name', 'Latitu...
                                                              strategy='most_frequent',
                                                              verbose=0))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-04-01 00:51:13,069:INFO:Creating final display dataframe.
2023-04-01 00:51:16,640:INFO:Setup _display_container:                     Description             Value
0                    Session id              8386
1                        Target   SUSPECTED_FRAUD
2                   Target type            Binary
3           Original data shape      (180519, 43)
4        Transformed data shape      (301196, 43)
5   Transformed train set shape      (247040, 43)
6    Transformed test set shape       (54156, 43)
7              Numeric features                42
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                Fix imbalance              True
13         Fix imbalance method             SMOTE
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              609b
2023-04-01 00:51:16,766:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-01 00:51:16,771:INFO:Soft dependency imported: catboost: 1.1.1
2023-04-01 00:51:16,904:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-01 00:51:16,910:INFO:Soft dependency imported: catboost: 1.1.1
2023-04-01 00:51:16,911:INFO:setup() successfully completed in 12.05s...............
2023-04-01 00:51:16,930:INFO:Initializing compare_models()
2023-04-01 00:51:16,930:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, include=None, fold=5, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, 'include': None, 'exclude': None, 'fold': 5, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-04-01 00:51:16,930:INFO:Checking exceptions
2023-04-01 00:51:17,042:INFO:Preparing display monitor
2023-04-01 00:51:17,095:INFO:Initializing Logistic Regression
2023-04-01 00:51:17,095:INFO:Total runtime is 0.0 minutes
2023-04-01 00:51:17,102:INFO:SubProcess create_model() called ==================================
2023-04-01 00:51:17,103:INFO:Initializing create_model()
2023-04-01 00:51:17,103:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 00:51:17,103:INFO:Checking exceptions
2023-04-01 00:51:17,103:INFO:Importing libraries
2023-04-01 00:51:17,103:INFO:Copying training dataset
2023-04-01 00:51:17,457:INFO:Defining folds
2023-04-01 00:51:17,458:INFO:Declaring metric variables
2023-04-01 00:51:17,487:INFO:Importing untrained model
2023-04-01 00:51:17,495:INFO:Logistic Regression Imported successfully
2023-04-01 00:51:17,509:INFO:Starting cross validation
2023-04-01 00:51:17,511:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 00:52:29,624:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-01 00:52:30,018:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-01 00:52:30,269:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-01 00:52:30,454:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-01 00:52:30,595:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-01 00:52:36,083:INFO:Calculating mean and std
2023-04-01 00:52:36,083:INFO:Creating metrics dataframe
2023-04-01 00:52:37,394:INFO:Uploading results into container
2023-04-01 00:52:37,394:INFO:Uploading model into container now
2023-04-01 00:52:37,403:INFO:_master_model_container: 1
2023-04-01 00:52:37,403:INFO:_display_container: 2
2023-04-01 00:52:37,403:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=8386, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-01 00:52:37,404:INFO:create_model() successfully completed......................................
2023-04-01 00:52:44,255:INFO:SubProcess create_model() end ==================================
2023-04-01 00:52:44,255:INFO:Creating metrics dataframe
2023-04-01 00:52:44,255:INFO:Initializing K Neighbors Classifier
2023-04-01 00:52:44,255:INFO:Total runtime is 1.4526525537172954 minutes
2023-04-01 00:52:44,270:INFO:SubProcess create_model() called ==================================
2023-04-01 00:52:44,270:INFO:Initializing create_model()
2023-04-01 00:52:44,270:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 00:52:44,270:INFO:Checking exceptions
2023-04-01 00:52:44,270:INFO:Importing libraries
2023-04-01 00:52:44,270:INFO:Copying training dataset
2023-04-01 00:52:44,432:INFO:Defining folds
2023-04-01 00:52:44,432:INFO:Declaring metric variables
2023-04-01 00:52:44,439:INFO:Importing untrained model
2023-04-01 00:52:44,446:INFO:K Neighbors Classifier Imported successfully
2023-04-01 00:52:44,456:INFO:Starting cross validation
2023-04-01 00:52:44,459:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 00:52:46,753:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 01:43:03,855:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-01 01:43:07,198:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:319: UserWarning: Persisting input arguments took 2.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 01:45:56,833:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:319: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 01:46:05,473:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.16s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-01 01:46:06,840:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-01 01:46:10,607:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:319: UserWarning: Persisting input arguments took 3.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 01:47:12,031:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-01 01:47:13,600:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:319: UserWarning: Persisting input arguments took 1.21s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 02:32:36,109:INFO:Calculating mean and std
2023-04-01 02:32:36,171:INFO:Creating metrics dataframe
2023-04-01 02:32:38,562:INFO:Uploading results into container
2023-04-01 02:32:38,577:INFO:Uploading model into container now
2023-04-01 02:32:38,577:INFO:_master_model_container: 2
2023-04-01 02:32:38,577:INFO:_display_container: 2
2023-04-01 02:32:38,593:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-04-01 02:32:38,593:INFO:create_model() successfully completed......................................
2023-04-01 02:32:40,389:INFO:SubProcess create_model() end ==================================
2023-04-01 02:32:40,389:INFO:Creating metrics dataframe
2023-04-01 02:32:40,419:INFO:Initializing Naive Bayes
2023-04-01 02:32:40,419:INFO:Total runtime is 101.38871889511745 minutes
2023-04-01 02:32:40,426:INFO:SubProcess create_model() called ==================================
2023-04-01 02:32:40,427:INFO:Initializing create_model()
2023-04-01 02:32:40,427:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 02:32:40,427:INFO:Checking exceptions
2023-04-01 02:32:40,427:INFO:Importing libraries
2023-04-01 02:32:40,428:INFO:Copying training dataset
2023-04-01 02:32:40,729:INFO:Defining folds
2023-04-01 02:32:40,729:INFO:Declaring metric variables
2023-04-01 02:32:40,734:INFO:Importing untrained model
2023-04-01 02:32:40,739:INFO:Naive Bayes Imported successfully
2023-04-01 02:32:40,749:INFO:Starting cross validation
2023-04-01 02:32:40,758:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 02:33:07,256:INFO:Calculating mean and std
2023-04-01 02:33:07,256:INFO:Creating metrics dataframe
2023-04-01 02:33:08,850:INFO:Uploading results into container
2023-04-01 02:33:08,850:INFO:Uploading model into container now
2023-04-01 02:33:08,850:INFO:_master_model_container: 3
2023-04-01 02:33:08,850:INFO:_display_container: 2
2023-04-01 02:33:08,850:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-01 02:33:08,850:INFO:create_model() successfully completed......................................
2023-04-01 02:33:09,162:INFO:SubProcess create_model() end ==================================
2023-04-01 02:33:09,162:INFO:Creating metrics dataframe
2023-04-01 02:33:09,188:INFO:Initializing Decision Tree Classifier
2023-04-01 02:33:09,188:INFO:Total runtime is 101.86821740865709 minutes
2023-04-01 02:33:09,188:INFO:SubProcess create_model() called ==================================
2023-04-01 02:33:09,188:INFO:Initializing create_model()
2023-04-01 02:33:09,188:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 02:33:09,188:INFO:Checking exceptions
2023-04-01 02:33:09,188:INFO:Importing libraries
2023-04-01 02:33:09,188:INFO:Copying training dataset
2023-04-01 02:33:09,353:INFO:Defining folds
2023-04-01 02:33:09,353:INFO:Declaring metric variables
2023-04-01 02:33:09,358:INFO:Importing untrained model
2023-04-01 02:33:09,363:INFO:Decision Tree Classifier Imported successfully
2023-04-01 02:33:09,373:INFO:Starting cross validation
2023-04-01 02:33:09,376:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 02:33:25,940:INFO:Calculating mean and std
2023-04-01 02:33:25,940:INFO:Creating metrics dataframe
2023-04-01 02:33:27,500:INFO:Uploading results into container
2023-04-01 02:33:27,500:INFO:Uploading model into container now
2023-04-01 02:33:27,500:INFO:_master_model_container: 4
2023-04-01 02:33:27,500:INFO:_display_container: 2
2023-04-01 02:33:27,500:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best')
2023-04-01 02:33:27,500:INFO:create_model() successfully completed......................................
2023-04-01 02:33:27,796:INFO:SubProcess create_model() end ==================================
2023-04-01 02:33:27,796:INFO:Creating metrics dataframe
2023-04-01 02:33:27,818:INFO:Initializing SVM - Linear Kernel
2023-04-01 02:33:27,818:INFO:Total runtime is 102.17870905399324 minutes
2023-04-01 02:33:27,823:INFO:SubProcess create_model() called ==================================
2023-04-01 02:33:27,823:INFO:Initializing create_model()
2023-04-01 02:33:27,824:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 02:33:27,824:INFO:Checking exceptions
2023-04-01 02:33:27,824:INFO:Importing libraries
2023-04-01 02:33:27,824:INFO:Copying training dataset
2023-04-01 02:33:27,988:INFO:Defining folds
2023-04-01 02:33:27,989:INFO:Declaring metric variables
2023-04-01 02:33:27,994:INFO:Importing untrained model
2023-04-01 02:33:28,000:INFO:SVM - Linear Kernel Imported successfully
2023-04-01 02:33:28,007:INFO:Starting cross validation
2023-04-01 02:33:28,007:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 02:34:42,532:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-04-01 02:34:50,430:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-04-01 02:34:53,860:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-04-01 02:34:58,373:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-04-01 02:34:59,956:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\utils\metaestimators.py", line 109, in __get__
    if not self.check(obj):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1199, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-04-01 02:35:00,566:INFO:Calculating mean and std
2023-04-01 02:35:00,566:INFO:Creating metrics dataframe
2023-04-01 02:35:02,902:INFO:Uploading results into container
2023-04-01 02:35:02,902:INFO:Uploading model into container now
2023-04-01 02:35:02,902:INFO:_master_model_container: 5
2023-04-01 02:35:02,902:INFO:_display_container: 2
2023-04-01 02:35:02,902:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=8386, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-04-01 02:35:02,902:INFO:create_model() successfully completed......................................
2023-04-01 02:35:03,262:INFO:SubProcess create_model() end ==================================
2023-04-01 02:35:03,262:INFO:Creating metrics dataframe
2023-04-01 02:35:03,277:INFO:Initializing Ridge Classifier
2023-04-01 02:35:03,280:INFO:Total runtime is 103.76974167823793 minutes
2023-04-01 02:35:03,285:INFO:SubProcess create_model() called ==================================
2023-04-01 02:35:03,286:INFO:Initializing create_model()
2023-04-01 02:35:03,286:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 02:35:03,286:INFO:Checking exceptions
2023-04-01 02:35:03,286:INFO:Importing libraries
2023-04-01 02:35:03,286:INFO:Copying training dataset
2023-04-01 02:35:03,462:INFO:Defining folds
2023-04-01 02:35:03,462:INFO:Declaring metric variables
2023-04-01 02:35:03,476:INFO:Importing untrained model
2023-04-01 02:35:03,483:INFO:Ridge Classifier Imported successfully
2023-04-01 02:35:03,488:INFO:Starting cross validation
2023-04-01 02:35:03,488:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 02:35:04,863:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-04-01 02:35:05,004:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-04-01 02:35:05,131:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-04-01 02:35:05,210:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-04-01 02:35:05,638:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 298, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-04-01 02:35:14,496:INFO:Calculating mean and std
2023-04-01 02:35:14,496:INFO:Creating metrics dataframe
2023-04-01 02:35:16,338:INFO:Uploading results into container
2023-04-01 02:35:16,338:INFO:Uploading model into container now
2023-04-01 02:35:16,338:INFO:_master_model_container: 6
2023-04-01 02:35:16,338:INFO:_display_container: 2
2023-04-01 02:35:16,338:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=8386, solver='auto', tol=0.001)
2023-04-01 02:35:16,338:INFO:create_model() successfully completed......................................
2023-04-01 02:35:16,698:INFO:SubProcess create_model() end ==================================
2023-04-01 02:35:16,698:INFO:Creating metrics dataframe
2023-04-01 02:35:16,713:INFO:Initializing Random Forest Classifier
2023-04-01 02:35:16,713:INFO:Total runtime is 103.99363285700483 minutes
2023-04-01 02:35:16,725:INFO:SubProcess create_model() called ==================================
2023-04-01 02:35:16,725:INFO:Initializing create_model()
2023-04-01 02:35:16,725:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 02:35:16,725:INFO:Checking exceptions
2023-04-01 02:35:16,725:INFO:Importing libraries
2023-04-01 02:35:16,725:INFO:Copying training dataset
2023-04-01 02:35:16,921:INFO:Defining folds
2023-04-01 02:35:16,922:INFO:Declaring metric variables
2023-04-01 02:35:16,928:INFO:Importing untrained model
2023-04-01 02:35:16,934:INFO:Random Forest Classifier Imported successfully
2023-04-01 02:35:16,938:INFO:Starting cross validation
2023-04-01 02:35:16,938:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 02:36:39,685:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-01 02:36:39,857:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-01 02:36:39,904:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-01 02:36:41,036:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 02:36:41,572:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-01 02:36:42,220:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-01 02:36:49,367:INFO:Calculating mean and std
2023-04-01 02:36:49,367:INFO:Creating metrics dataframe
2023-04-01 02:36:50,893:INFO:Uploading results into container
2023-04-01 02:36:50,893:INFO:Uploading model into container now
2023-04-01 02:36:50,893:INFO:_master_model_container: 7
2023-04-01 02:36:50,893:INFO:_display_container: 2
2023-04-01 02:36:50,893:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=8386, verbose=0, warm_start=False)
2023-04-01 02:36:50,893:INFO:create_model() successfully completed......................................
2023-04-01 02:36:51,190:INFO:SubProcess create_model() end ==================================
2023-04-01 02:36:51,190:INFO:Creating metrics dataframe
2023-04-01 02:36:51,213:INFO:Initializing Quadratic Discriminant Analysis
2023-04-01 02:36:51,213:INFO:Total runtime is 105.56862816015882 minutes
2023-04-01 02:36:51,214:INFO:SubProcess create_model() called ==================================
2023-04-01 02:36:51,214:INFO:Initializing create_model()
2023-04-01 02:36:51,214:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 02:36:51,214:INFO:Checking exceptions
2023-04-01 02:36:51,214:INFO:Importing libraries
2023-04-01 02:36:51,214:INFO:Copying training dataset
2023-04-01 02:36:51,376:INFO:Defining folds
2023-04-01 02:36:51,376:INFO:Declaring metric variables
2023-04-01 02:36:51,381:INFO:Importing untrained model
2023-04-01 02:36:51,388:INFO:Quadratic Discriminant Analysis Imported successfully
2023-04-01 02:36:51,397:INFO:Starting cross validation
2023-04-01 02:36:51,399:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 02:36:52,793:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-01 02:36:52,918:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-01 02:36:52,996:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-01 02:36:53,090:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-01 02:36:53,105:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-01 02:37:02,420:INFO:Calculating mean and std
2023-04-01 02:37:02,420:INFO:Creating metrics dataframe
2023-04-01 02:37:03,924:INFO:Uploading results into container
2023-04-01 02:37:03,924:INFO:Uploading model into container now
2023-04-01 02:37:03,924:INFO:_master_model_container: 8
2023-04-01 02:37:03,924:INFO:_display_container: 2
2023-04-01 02:37:03,924:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-04-01 02:37:03,924:INFO:create_model() successfully completed......................................
2023-04-01 02:37:04,221:INFO:SubProcess create_model() end ==================================
2023-04-01 02:37:04,221:INFO:Creating metrics dataframe
2023-04-01 02:37:04,245:INFO:Initializing Ada Boost Classifier
2023-04-01 02:37:04,245:INFO:Total runtime is 105.78583218653999 minutes
2023-04-01 02:37:04,257:INFO:SubProcess create_model() called ==================================
2023-04-01 02:37:04,258:INFO:Initializing create_model()
2023-04-01 02:37:04,258:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 02:37:04,258:INFO:Checking exceptions
2023-04-01 02:37:04,258:INFO:Importing libraries
2023-04-01 02:37:04,258:INFO:Copying training dataset
2023-04-01 02:37:04,413:INFO:Defining folds
2023-04-01 02:37:04,414:INFO:Declaring metric variables
2023-04-01 02:37:04,419:INFO:Importing untrained model
2023-04-01 02:37:04,424:INFO:Ada Boost Classifier Imported successfully
2023-04-01 02:37:04,435:INFO:Starting cross validation
2023-04-01 02:37:04,435:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 02:37:15,280:INFO:Calculating mean and std
2023-04-01 02:37:15,280:INFO:Creating metrics dataframe
2023-04-01 02:37:16,804:INFO:Uploading results into container
2023-04-01 02:37:16,804:INFO:Uploading model into container now
2023-04-01 02:37:16,804:INFO:_master_model_container: 9
2023-04-01 02:37:16,804:INFO:_display_container: 2
2023-04-01 02:37:16,804:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=8386)
2023-04-01 02:37:16,804:INFO:create_model() successfully completed......................................
2023-04-01 02:37:17,117:INFO:SubProcess create_model() end ==================================
2023-04-01 02:37:17,117:INFO:Creating metrics dataframe
2023-04-01 02:37:17,137:INFO:Initializing Gradient Boosting Classifier
2023-04-01 02:37:17,137:INFO:Total runtime is 106.00069292783739 minutes
2023-04-01 02:37:17,142:INFO:SubProcess create_model() called ==================================
2023-04-01 02:37:17,142:INFO:Initializing create_model()
2023-04-01 02:37:17,142:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 02:37:17,142:INFO:Checking exceptions
2023-04-01 02:37:17,142:INFO:Importing libraries
2023-04-01 02:37:17,142:INFO:Copying training dataset
2023-04-01 02:37:17,303:INFO:Defining folds
2023-04-01 02:37:17,303:INFO:Declaring metric variables
2023-04-01 02:37:17,309:INFO:Importing untrained model
2023-04-01 02:37:17,315:INFO:Gradient Boosting Classifier Imported successfully
2023-04-01 02:37:17,327:INFO:Starting cross validation
2023-04-01 02:37:17,329:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 02:41:28,260:INFO:Calculating mean and std
2023-04-01 02:41:28,260:INFO:Creating metrics dataframe
2023-04-01 02:41:29,789:INFO:Uploading results into container
2023-04-01 02:41:29,789:INFO:Uploading model into container now
2023-04-01 02:41:29,789:INFO:_master_model_container: 10
2023-04-01 02:41:29,789:INFO:_display_container: 2
2023-04-01 02:41:29,789:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='deviance', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8386, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-04-01 02:41:29,789:INFO:create_model() successfully completed......................................
2023-04-01 02:41:30,096:INFO:SubProcess create_model() end ==================================
2023-04-01 02:41:30,096:INFO:Creating metrics dataframe
2023-04-01 02:41:30,110:INFO:Initializing Linear Discriminant Analysis
2023-04-01 02:41:30,110:INFO:Total runtime is 110.2169048468272 minutes
2023-04-01 02:41:30,110:INFO:SubProcess create_model() called ==================================
2023-04-01 02:41:30,110:INFO:Initializing create_model()
2023-04-01 02:41:30,110:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 02:41:30,110:INFO:Checking exceptions
2023-04-01 02:41:30,110:INFO:Importing libraries
2023-04-01 02:41:30,110:INFO:Copying training dataset
2023-04-01 02:41:30,287:INFO:Defining folds
2023-04-01 02:41:30,287:INFO:Declaring metric variables
2023-04-01 02:41:30,291:INFO:Importing untrained model
2023-04-01 02:41:30,295:INFO:Linear Discriminant Analysis Imported successfully
2023-04-01 02:41:30,305:INFO:Starting cross validation
2023-04-01 02:41:30,308:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 02:41:43,748:INFO:Calculating mean and std
2023-04-01 02:41:43,748:INFO:Creating metrics dataframe
2023-04-01 02:41:45,238:INFO:Uploading results into container
2023-04-01 02:41:45,238:INFO:Uploading model into container now
2023-04-01 02:41:45,238:INFO:_master_model_container: 11
2023-04-01 02:41:45,238:INFO:_display_container: 2
2023-04-01 02:41:45,238:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-04-01 02:41:45,238:INFO:create_model() successfully completed......................................
2023-04-01 02:41:45,551:INFO:SubProcess create_model() end ==================================
2023-04-01 02:41:45,551:INFO:Creating metrics dataframe
2023-04-01 02:41:45,571:INFO:Initializing Extra Trees Classifier
2023-04-01 02:41:45,571:INFO:Total runtime is 110.4745861887932 minutes
2023-04-01 02:41:45,575:INFO:SubProcess create_model() called ==================================
2023-04-01 02:41:45,575:INFO:Initializing create_model()
2023-04-01 02:41:45,575:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 02:41:45,575:INFO:Checking exceptions
2023-04-01 02:41:45,575:INFO:Importing libraries
2023-04-01 02:41:45,575:INFO:Copying training dataset
2023-04-01 02:41:45,725:INFO:Defining folds
2023-04-01 02:41:45,725:INFO:Declaring metric variables
2023-04-01 02:41:45,730:INFO:Importing untrained model
2023-04-01 02:41:45,736:INFO:Extra Trees Classifier Imported successfully
2023-04-01 02:41:45,748:INFO:Starting cross validation
2023-04-01 02:41:45,751:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 02:42:19,510:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-01 02:42:20,123:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-01 02:42:20,186:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-01 02:42:20,232:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-01 02:42:21,407:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 02:42:21,627:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 02:42:22,146:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:319: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 02:42:29,379:INFO:Calculating mean and std
2023-04-01 02:42:29,379:INFO:Creating metrics dataframe
2023-04-01 02:42:31,136:INFO:Uploading results into container
2023-04-01 02:42:31,138:INFO:Uploading model into container now
2023-04-01 02:42:31,138:INFO:_master_model_container: 12
2023-04-01 02:42:31,138:INFO:_display_container: 2
2023-04-01 02:42:31,139:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=8386, verbose=0, warm_start=False)
2023-04-01 02:42:31,139:INFO:create_model() successfully completed......................................
2023-04-01 02:42:31,491:INFO:SubProcess create_model() end ==================================
2023-04-01 02:42:31,491:INFO:Creating metrics dataframe
2023-04-01 02:42:31,515:INFO:Initializing Extreme Gradient Boosting
2023-04-01 02:42:31,515:INFO:Total runtime is 111.24032021363578 minutes
2023-04-01 02:42:31,530:INFO:SubProcess create_model() called ==================================
2023-04-01 02:42:31,530:INFO:Initializing create_model()
2023-04-01 02:42:31,530:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 02:42:31,530:INFO:Checking exceptions
2023-04-01 02:42:31,530:INFO:Importing libraries
2023-04-01 02:42:31,530:INFO:Copying training dataset
2023-04-01 02:42:31,730:INFO:Defining folds
2023-04-01 02:42:31,730:INFO:Declaring metric variables
2023-04-01 02:42:31,730:INFO:Importing untrained model
2023-04-01 02:42:31,745:INFO:Extreme Gradient Boosting Imported successfully
2023-04-01 02:42:31,756:INFO:Starting cross validation
2023-04-01 02:42:31,759:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 02:43:51,617:INFO:Calculating mean and std
2023-04-01 02:43:51,617:INFO:Creating metrics dataframe
2023-04-01 02:43:53,133:INFO:Uploading results into container
2023-04-01 02:43:53,133:INFO:Uploading model into container now
2023-04-01 02:43:53,133:INFO:_master_model_container: 13
2023-04-01 02:43:53,133:INFO:_display_container: 2
2023-04-01 02:43:53,133:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-04-01 02:43:53,133:INFO:create_model() successfully completed......................................
2023-04-01 02:43:53,447:INFO:SubProcess create_model() end ==================================
2023-04-01 02:43:53,447:INFO:Creating metrics dataframe
2023-04-01 02:43:53,471:INFO:Initializing Light Gradient Boosting Machine
2023-04-01 02:43:53,471:INFO:Total runtime is 112.6062532345454 minutes
2023-04-01 02:43:53,485:INFO:SubProcess create_model() called ==================================
2023-04-01 02:43:53,486:INFO:Initializing create_model()
2023-04-01 02:43:53,486:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 02:43:53,486:INFO:Checking exceptions
2023-04-01 02:43:53,486:INFO:Importing libraries
2023-04-01 02:43:53,486:INFO:Copying training dataset
2023-04-01 02:43:53,644:INFO:Defining folds
2023-04-01 02:43:53,645:INFO:Declaring metric variables
2023-04-01 02:43:53,650:INFO:Importing untrained model
2023-04-01 02:43:53,656:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-01 02:43:53,664:INFO:Starting cross validation
2023-04-01 02:43:53,668:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 02:44:17,033:INFO:Calculating mean and std
2023-04-01 02:44:17,033:INFO:Creating metrics dataframe
2023-04-01 02:44:18,531:INFO:Uploading results into container
2023-04-01 02:44:18,531:INFO:Uploading model into container now
2023-04-01 02:44:18,531:INFO:_master_model_container: 14
2023-04-01 02:44:18,531:INFO:_display_container: 2
2023-04-01 02:44:18,531:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8386, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-04-01 02:44:18,531:INFO:create_model() successfully completed......................................
2023-04-01 02:44:18,828:INFO:SubProcess create_model() end ==================================
2023-04-01 02:44:18,828:INFO:Creating metrics dataframe
2023-04-01 02:44:18,859:INFO:Initializing CatBoost Classifier
2023-04-01 02:44:18,859:INFO:Total runtime is 113.02938974698387 minutes
2023-04-01 02:44:18,865:INFO:SubProcess create_model() called ==================================
2023-04-01 02:44:18,865:INFO:Initializing create_model()
2023-04-01 02:44:18,865:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=catboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 02:44:18,865:INFO:Checking exceptions
2023-04-01 02:44:18,865:INFO:Importing libraries
2023-04-01 02:44:18,865:INFO:Copying training dataset
2023-04-01 02:44:19,015:INFO:Defining folds
2023-04-01 02:44:19,015:INFO:Declaring metric variables
2023-04-01 02:44:19,020:INFO:Importing untrained model
2023-04-01 02:44:19,033:INFO:CatBoost Classifier Imported successfully
2023-04-01 02:44:19,041:INFO:Starting cross validation
2023-04-01 02:44:19,043:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 02:49:06,615:INFO:Calculating mean and std
2023-04-01 02:49:06,615:INFO:Creating metrics dataframe
2023-04-01 02:49:08,173:INFO:Uploading results into container
2023-04-01 02:49:08,173:INFO:Uploading model into container now
2023-04-01 02:49:08,189:INFO:_master_model_container: 15
2023-04-01 02:49:08,189:INFO:_display_container: 2
2023-04-01 02:49:08,189:INFO:<catboost.core.CatBoostClassifier object at 0x000001A0BAF22B20>
2023-04-01 02:49:08,189:INFO:create_model() successfully completed......................................
2023-04-01 02:49:08,470:INFO:SubProcess create_model() end ==================================
2023-04-01 02:49:08,470:INFO:Creating metrics dataframe
2023-04-01 02:49:08,497:INFO:Initializing Dummy Classifier
2023-04-01 02:49:08,497:INFO:Total runtime is 117.85669087568921 minutes
2023-04-01 02:49:08,497:INFO:SubProcess create_model() called ==================================
2023-04-01 02:49:08,497:INFO:Initializing create_model()
2023-04-01 02:49:08,497:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={})
2023-04-01 02:49:08,497:INFO:Checking exceptions
2023-04-01 02:49:08,497:INFO:Importing libraries
2023-04-01 02:49:08,497:INFO:Copying training dataset
2023-04-01 02:49:08,667:INFO:Defining folds
2023-04-01 02:49:08,667:INFO:Declaring metric variables
2023-04-01 02:49:08,672:INFO:Importing untrained model
2023-04-01 02:49:08,676:INFO:Dummy Classifier Imported successfully
2023-04-01 02:49:08,682:INFO:Starting cross validation
2023-04-01 02:49:08,682:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 02:49:09,358:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-01 02:49:09,421:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-01 02:49:09,421:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-01 02:49:09,437:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-01 02:49:09,499:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-01 02:49:16,782:INFO:Calculating mean and std
2023-04-01 02:49:16,782:INFO:Creating metrics dataframe
2023-04-01 02:49:18,309:INFO:Uploading results into container
2023-04-01 02:49:18,309:INFO:Uploading model into container now
2023-04-01 02:49:18,309:INFO:_master_model_container: 16
2023-04-01 02:49:18,309:INFO:_display_container: 2
2023-04-01 02:49:18,309:INFO:DummyClassifier(constant=None, random_state=8386, strategy='prior')
2023-04-01 02:49:18,309:INFO:create_model() successfully completed......................................
2023-04-01 02:49:18,606:INFO:SubProcess create_model() end ==================================
2023-04-01 02:49:18,606:INFO:Creating metrics dataframe
2023-04-01 02:49:18,645:INFO:Initializing create_model()
2023-04-01 02:49:18,645:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best'), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-01 02:49:18,646:INFO:Checking exceptions
2023-04-01 02:49:18,655:INFO:Importing libraries
2023-04-01 02:49:18,655:INFO:Copying training dataset
2023-04-01 02:49:18,811:INFO:Defining folds
2023-04-01 02:49:18,811:INFO:Declaring metric variables
2023-04-01 02:49:18,811:INFO:Importing untrained model
2023-04-01 02:49:18,812:INFO:Declaring custom model
2023-04-01 02:49:18,812:INFO:Decision Tree Classifier Imported successfully
2023-04-01 02:49:18,812:INFO:Cross validation set to False
2023-04-01 02:49:18,812:INFO:Fitting Model
2023-04-01 02:49:21,689:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best')
2023-04-01 02:49:21,689:INFO:create_model() successfully completed......................................
2023-04-01 02:49:22,013:INFO:_master_model_container: 16
2023-04-01 02:49:22,013:INFO:_display_container: 2
2023-04-01 02:49:22,013:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best')
2023-04-01 02:49:22,013:INFO:compare_models() successfully completed......................................
2023-04-01 02:49:22,039:INFO:Initializing tune_model()
2023-04-01 02:49:22,039:INFO:tune_model(estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>)
2023-04-01 02:49:22,039:INFO:Checking exceptions
2023-04-01 02:49:22,127:INFO:Copying training dataset
2023-04-01 02:49:22,223:INFO:Checking base model
2023-04-01 02:49:22,223:INFO:Base model : Decision Tree Classifier
2023-04-01 02:49:22,228:INFO:Declaring metric variables
2023-04-01 02:49:22,232:INFO:Defining Hyperparameters
2023-04-01 02:49:22,538:INFO:Tuning with n_jobs=-1
2023-04-01 02:49:22,538:INFO:Initializing RandomizedSearchCV
2023-04-01 02:49:24,572:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-04-01 02:49:24,743:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-04-01 02:49:25,790:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-04-01 02:49:29,605:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:29,667:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:29,823:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:30,058:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:30,401:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:30,807:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:30,839:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-04-01 02:49:31,167:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:32,292:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-04-01 02:49:32,448:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:32,479:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-04-01 02:49:32,542:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:32,729:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:32,948:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:33,089:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:33,494:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:34,181:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:43,138:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:44,077:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:44,546:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:44,593:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-04-01 02:49:44,671:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:45,467:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:45,499:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:45,655:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:45,983:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:49:46,891:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-01 02:50:32,986:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-01 02:50:34,961:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 02:53:03,370:INFO:best_params: {'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0002, 'actual_estimator__max_features': 'log2', 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'entropy'}
2023-04-01 02:53:03,386:INFO:Hyperparameter search completed
2023-04-01 02:53:03,386:INFO:SubProcess create_model() called ==================================
2023-04-01 02:53:03,386:INFO:Initializing create_model()
2023-04-01 02:53:03,386:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A01279DC70>, model_only=True, return_train_score=False, kwargs={'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0002, 'max_features': 'log2', 'max_depth': 15, 'criterion': 'entropy'})
2023-04-01 02:53:03,386:INFO:Checking exceptions
2023-04-01 02:53:03,386:INFO:Importing libraries
2023-04-01 02:53:03,386:INFO:Copying training dataset
2023-04-01 02:53:03,558:INFO:Defining folds
2023-04-01 02:53:03,558:INFO:Declaring metric variables
2023-04-01 02:53:03,585:INFO:Importing untrained model
2023-04-01 02:53:03,585:INFO:Declaring custom model
2023-04-01 02:53:03,585:INFO:Decision Tree Classifier Imported successfully
2023-04-01 02:53:03,604:INFO:Starting cross validation
2023-04-01 02:53:03,604:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 02:53:04,860:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-01 02:53:06,908:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-01 02:53:07,064:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.25s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-04-01 02:53:07,236:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 02:53:07,486:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-01 02:53:07,970:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 02:53:08,157:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-01 02:53:08,267:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 02:53:08,423:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 02:53:08,501:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 02:53:08,704:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 02:53:27,413:INFO:Calculating mean and std
2023-04-01 02:53:27,413:INFO:Creating metrics dataframe
2023-04-01 02:53:27,429:INFO:Finalizing model
2023-04-01 02:53:29,873:INFO:Uploading results into container
2023-04-01 02:53:29,874:INFO:Uploading model into container now
2023-04-01 02:53:29,878:INFO:_master_model_container: 17
2023-04-01 02:53:29,878:INFO:_display_container: 3
2023-04-01 02:53:29,878:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',
                       max_depth=15, max_features='log2', max_leaf_nodes=None,
                       min_impurity_decrease=0.0002, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best')
2023-04-01 02:53:29,878:INFO:create_model() successfully completed......................................
2023-04-01 02:53:31,092:INFO:SubProcess create_model() end ==================================
2023-04-01 02:53:31,092:INFO:choose_better activated
2023-04-01 02:53:31,092:INFO:SubProcess create_model() called ==================================
2023-04-01 02:53:31,092:INFO:Initializing create_model()
2023-04-01 02:53:31,092:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-01 02:53:31,092:INFO:Checking exceptions
2023-04-01 02:53:31,107:INFO:Importing libraries
2023-04-01 02:53:31,107:INFO:Copying training dataset
2023-04-01 02:53:31,259:INFO:Defining folds
2023-04-01 02:53:31,259:INFO:Declaring metric variables
2023-04-01 02:53:31,259:INFO:Importing untrained model
2023-04-01 02:53:31,259:INFO:Declaring custom model
2023-04-01 02:53:31,259:INFO:Decision Tree Classifier Imported successfully
2023-04-01 02:53:31,259:INFO:Starting cross validation
2023-04-01 02:53:31,259:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-01 02:53:39,466:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:319: UserWarning: Persisting input arguments took 1.38s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-01 02:53:57,040:INFO:Calculating mean and std
2023-04-01 02:53:57,040:INFO:Creating metrics dataframe
2023-04-01 02:53:57,040:INFO:Finalizing model
2023-04-01 02:53:58,996:INFO:Uploading results into container
2023-04-01 02:53:58,996:INFO:Uploading model into container now
2023-04-01 02:53:58,996:INFO:_master_model_container: 18
2023-04-01 02:53:58,996:INFO:_display_container: 4
2023-04-01 02:53:58,996:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best')
2023-04-01 02:53:58,996:INFO:create_model() successfully completed......................................
2023-04-01 02:53:59,298:INFO:SubProcess create_model() end ==================================
2023-04-01 02:53:59,298:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best') result for Accuracy is 1.0
2023-04-01 02:53:59,298:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',
                       max_depth=15, max_features='log2', max_leaf_nodes=None,
                       min_impurity_decrease=0.0002, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best') result for Accuracy is 1.0
2023-04-01 02:53:59,298:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best') is best model
2023-04-01 02:53:59,298:INFO:choose_better completed
2023-04-01 02:53:59,298:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-04-01 02:53:59,321:INFO:_master_model_container: 18
2023-04-01 02:53:59,322:INFO:_display_container: 3
2023-04-01 02:53:59,322:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best')
2023-04-01 02:53:59,322:INFO:tune_model() successfully completed......................................
2023-04-01 02:54:00,815:INFO:Initializing plot_model()
2023-04-01 02:54:00,815:INFO:plot_model(plot=confusion_matrix, fold=None, use_train_data=False, verbose=True, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, system=True)
2023-04-01 02:54:00,815:INFO:Checking exceptions
2023-04-01 02:54:00,875:INFO:Preloading libraries
2023-04-01 02:54:00,875:INFO:Copying training dataset
2023-04-01 02:54:00,875:INFO:Plot type: confusion_matrix
2023-04-01 02:54:01,592:INFO:Fitting Model
2023-04-01 02:54:01,592:WARNING:C:\Users\Mujahid\anaconda3\lib\site-packages\sklearn\base.py:450: UserWarning:

X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names


2023-04-01 02:54:01,592:INFO:Scoring test/hold-out set
2023-04-01 02:54:01,834:INFO:Visual Rendered Successfully
2023-04-01 02:54:02,113:INFO:plot_model() successfully completed......................................
2023-04-01 04:15:48,923:INFO:Initializing interpret_model()
2023-04-01 04:15:48,923:INFO:interpret_model(estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best'), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>)
2023-04-01 04:15:48,923:INFO:Checking exceptions
2023-04-01 04:15:48,924:INFO:Soft dependency imported: shap: 0.41.0
2023-04-01 04:15:49,018:INFO:plot type: summary
2023-04-01 04:15:49,019:INFO:Creating TreeExplainer
2023-04-01 04:15:49,019:INFO:Compiling shap values
2023-04-01 04:15:54,730:INFO:Visual Rendered Successfully
2023-04-01 04:15:54,730:INFO:interpret_model() successfully completed......................................
2023-04-01 04:24:35,749:INFO:Initializing interpret_model()
2023-04-01 04:24:35,749:INFO:interpret_model(estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best'), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>)
2023-04-01 04:24:35,749:INFO:Checking exceptions
2023-04-01 04:24:35,749:INFO:Soft dependency imported: shap: 0.41.0
2023-04-01 04:24:35,853:INFO:plot type: summary
2023-04-01 04:24:35,853:INFO:Creating TreeExplainer
2023-04-01 04:24:35,853:INFO:Compiling shap values
2023-04-01 04:24:41,560:INFO:Visual Rendered Successfully
2023-04-01 04:24:41,575:INFO:interpret_model() successfully completed......................................
2023-04-01 04:31:34,147:INFO:Initializing plot_model()
2023-04-01 04:31:34,147:INFO:plot_model(plot=feature, fold=None, use_train_data=False, verbose=True, display=None, display_format=None, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>, system=True)
2023-04-01 04:31:34,147:INFO:Checking exceptions
2023-04-01 04:31:34,211:INFO:Preloading libraries
2023-04-01 04:31:34,211:INFO:Copying training dataset
2023-04-01 04:31:34,211:INFO:Plot type: feature
2023-04-01 04:31:34,211:WARNING:No coef_ found. Trying feature_importances_
2023-04-01 04:31:36,494:INFO:Visual Rendered Successfully
2023-04-01 04:31:36,934:INFO:plot_model() successfully completed......................................
2023-04-01 04:32:27,469:INFO:Initializing interpret_model()
2023-04-01 04:32:27,477:INFO:interpret_model(estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best'), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>)
2023-04-01 04:32:27,477:INFO:Checking exceptions
2023-04-01 04:32:27,477:INFO:Soft dependency imported: shap: 0.41.0
2023-04-01 04:32:27,565:INFO:plot type: summary
2023-04-01 04:32:27,565:INFO:Creating TreeExplainer
2023-04-01 04:32:27,573:INFO:Compiling shap values
2023-04-01 04:32:33,351:INFO:Visual Rendered Successfully
2023-04-01 04:32:33,351:INFO:interpret_model() successfully completed......................................
2023-04-01 04:40:05,608:INFO:Initializing interpret_model()
2023-04-01 04:40:05,616:INFO:interpret_model(estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8386, splitter='best'), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A014AF53D0>)
2023-04-01 04:40:05,616:INFO:Checking exceptions
2023-04-01 04:40:05,616:INFO:Soft dependency imported: shap: 0.41.0
2023-04-01 04:40:05,704:INFO:plot type: summary
2023-04-01 04:40:05,704:INFO:Creating TreeExplainer
2023-04-01 04:40:05,704:INFO:Compiling shap values
2023-04-01 04:40:11,515:INFO:Visual Rendered Successfully
2023-04-01 04:40:11,515:INFO:interpret_model() successfully completed......................................
